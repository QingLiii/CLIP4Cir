# Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features  

ALBERTO BALDRATI, UniversitÃ  degli Studi di Firenze - MICC, Italy and UniversitÃ  di Pisa, Italy   
MARCO BERTINI, UniversitÃ  degli Studi di Firenze - MICC, Italy   
TIBERIO URICCHIO, UniversitÃ  degli Studi di Macerata, Italy   
ALBERTO DEL BIMBO, UniversitÃ  degli Studi di Firenze - MICC, Italy  

Given a query composed of a reference image and a relative caption, the Composed Image Retrieval goal is to retrieve images visually similar to the reference one that integrates the modifications expressed by the caption. Given that recent research has demonstrated the efficacy of large-scale vision and language pre-trained (VLP) models in various tasks, we rely on features from the OpenAI CLIP model to tackle the considered task. We initially perform a task-oriented fine-tuning of both CLIP encoders using the element-wise sum of visual and textual features. Then, in the second stage, we train a Combiner network that learns to combine the image-text features integrating the bimodal information and providing combined features used to perform the retrieval. We use contrastive learning in both stages of training. Starting from the bare CLIP features as a baseline, experimental results show that the task-oriented fine-tuning and the carefully crafted Combiner network are highly effective and outperform more complex state-of-the-art approaches on FashionIQ and CIRR, two popular and challenging datasets for composed image retrieval. Code and pre-trained models are available at https://github.com/ABaldrati/CLIP4Cir  

CCS Concepts: $\bullet$ Computing methodologies $\rightarrow$ Image representations; Visual content-based indexing and retrieval.  

Additional Key Words and Phrases: multimodal retrieval, combiner networks, vision language model  

# ACM Reference Format:  

Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Alberto Del Bimbo. 2023. Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features. ACM Trans. Multimedia Comput. Commun. Appl. 1, 1 (August 2023), 23 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn  

# 1 INTRODUCTION  

Content-Based Image Retrieval (CBIR) is a fundamental task in multimedia and computer vision which has undergone a continuous evolution since its early years [46], moving from the use of engineered features like SIFT to CNNs [33, 56]. It has been applied to many different specialized domains like artworks and cultural heritage [4, 12], commerce [17, 54], surveillance [2], nature [24, 25]. In the basic form, the query is composed of only an image, of which features are computed and compared with the ones extracted by a database of images.  

We can extend CBIR systems to improve their effectiveness by adding additional information to the query image. For example, interactive image retrieval systems extend CBIR systems by adding some form of user feedback, e.g. to provide some measure of relevance [5]. In composed image retrieval, the visual query is extended to an image-language pair [37] where a short textual description, typically expressed in natural language, may request constraints and desired changes or add specifications on some attributes of the retrieved results [26]. Figure 1 illustrates two examples of this task. In both queries, a user selects a reference image and then provides additional requests in the form of text, e.g. asking to change details, texture, color, or shape features of the reference image. Composed image retrieval systems find applications in various domains such as web search, e-commerce, and surveillance. However, developing solutions for this task can be challenging due to the need for incorporating feedback and user intent while addressing the semantic gap between image and text content.  

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/1fa5779a2b881daed69777b3b89739fd264cd1d10a840eb93be435af547a68ee.jpg)  
Fig. 1. The left portion of the illustration depicts a specific case of composed image retrieval in the fashion domain, where the user imposes constraints on the character attribute of a t-shirt. Meanwhile, the right part showcases an example where the user asks to alter objects and their cardinality within a real-life image.  

Very recently, researchers proved that deep neural networks combining visual and language modalities like CLIP [41], ALIGN [28], and the more recent method proposed in [9], trained using an image-caption alignment objective on large-scale internet data, can obtain impressive zero-shot transfer on a myriad of downstream tasks like image classification, text-based image retrieval, and object detection.  

In this work, we show that features obtained from vision and language pretrained (VLP) models â€“ we employed CLIP-based features â€“ can be effectively used to implement a composed image retrieval system where user feedback is provided as natural language input to provide additional (or contrasting) requirements concerning those embedded in the visual features of the image used to query the system. Firstly, we apply the system to the fashion domain, performing experiments on the challenging FashionIQ dataset [51]. Then, to study the generalization capabilities to a broader image domain, we perform experiments on the newly introduced CIRR dataset [37]. Experiments show that the proposed approach obtains state-of-the-art results on both datasets.  

To summarize, we highlight our main contributions as follows:  

â€¢ We propose a novel task-oriented fine-tuning scheme for adapting vision-language models to the composed image retrieval task. The aim of such a task-oriented adaptation scheme is to reduce the mismatch between the large-scale pre-training and the downstream task.   
â€¢ We propose a novel two-stage approach that combines task-oriented fine-tuning with the training of a Combiner network which can perform a fine-grained merging of the multimodal features. This two-stage approach achieves state-of-the-art results on two standard and challenging datasets: FashionIQ and CIRR.   
â€¢ We address the issue of using the CLIP model with images having a high aspect ratio since the CLIP visual encoder can input only square pictures. We propose a novel preprocess pipeline suited for image retrieval tasks that helps to reduce content information loss compared to the standard CLIP preprocess pipeline.   
â€¢ To provide further insight into the workings of our proposed system, we perform several qualitative experiments. The first experiment aims to demonstrate how our approach affects the feature distribution in the embedding spaces and the impact of pairwise feature distances on retrieval performance. Additionally, we report visualization experiments utilizing the gradCAM technique [44] to gain a deeper understanding of the image portions that are most significant during retrieval.  

# 2 RELATED WORKS  

Traditional CBIR does not use user feedback or its intent to refine results. However, within interactive and composed CBIR, much work has been done to improve retrieval performance by incorporating userâ€™s feedback in terms of relevance to the query [42] or by considering relative [30] and absolute attributes [20, 55]. The limiting expressiveness of attributes was successively addressed in [19, 48] by considering purely textual feedback, allowing richer expressiveness. Nonetheless, the performance of the textual model can limit the system in understanding and reacting appropriately.  

## Visual and language pre-training  

Models like GPT-2, BERT [15] and GPT-3 [6] have shown that large amounts of text combined with recent improvements in attention mechanisms enable learning of powerful features that integrate vast knowledge. Adding images to the learning process, CLIP [41] has very recently shown that it is feasible to perform multimodal zero-shot learning, obtaining remarkable feature generalization of both images and text. CLIP is a deep neural network trained to predict the association between text snippets and paired images. Unlike standard vision models trained on specific datasets that are typically good at only one task, this new class of models learns associations between images and natural language supervision that are widely available on the internet. They are not directly optimized for a benchmark and yet can perform consistently well on different tasks. CLIP effectiveness is still subject of study [1], with first applications to art [13], image generation [11] and zero-shot video retrieval [18], event classification [32], visual commonsense reasoning [49]. Our work builds upon CLIP and further explores its potential in the composed image retrieval task, applying the proposed approach to a specific domain, i.e. fashion, and also to general images. ALIGN [28] uses a dual-encoder architecture to learn the alignment of visual and language representations of image and text pairs using a contrastive loss in a noisy dataset. The extremely large scale of such a dataset, composed of 1 billion pairs, twice the size of the CLIP training dataset, makes up for its noise and leads to state-of-the-art representations even using such a simple learning scheme. Differently from CLIP and ALIGN, the authors in [9] propose a data-efficient contrastive distillation method that learns from a training dataset that is $133\times$ smaller than the one used by CLIP (400 million pairs), using a ResNet50 image encoder and DeCLUTR text encoder.  

## Composed image retrieval  

In the growing area of image retrieval with user feedback that combines images and text, our work relates to two recently introduced datasets that address the composed image retrieval task: i) FashionIQ, a fashion image retrieval with text [51], and with ii) the very recent composed image retrieval of generic images introduced in [37]. In [8], a transformer that can be seamlessly plugged into a CNN to selectively preserve and transform the visual features conditioned on language semantics is presented. Text Image Residual Gating (TIRG) [48] combines image and text features using gating and residual features. The authors of [45] leverage skip connections by combining them with graph neural networks, resulting in improved performance. The authors of [31] employ two different neural network modules to address image style and content. In [29], the authors present a Correction Network which explicitly models the difference between the reference and target image in the embedding space. In [37], a new dataset (CIRR) for composed image retrieval on real-world images is proposed, along with a novel transformer-based model that uses rich pre-trained vision-and-language knowledge, called CIRPLANT, to modify visual features conditioned on natural language. CIRPLANT leverages visual-and-language pre-trained models in composed image retrieval: the OSCAR model [34] is carefully adapted to the task with promising results. In [16], the authors proposed the Modality-Agnostic Attention Fusion (MAAF) model to tackle the composed image retrieval task. The model treats the convolutional spatial image features and learned text embeddings as modality-agnostic tokens and passes them to a Transformer for further processing. In [36], the authors propose a Multi-Grained Fusion (MGF) module which fuses features at different stages. ComposeAE [3] is an autoencoder-based model that learns the composition of image and text features for retrieving images by adopting a deep metric learning (DML) approach instead of fusing them by passing through a few fully connected layers. CurlingNet, proposed in [52], measures the semantic differential relationships between images concerning a conditioning query text. The main components are two networks: the first one, called the Delivery filter, delivers the source image to the candidate cluster according to a given query in embedding space, while the second one, called the Sweeping filter, checks the attributes highlighted in the query and learns the path from the center of valid target candidates to the target image. In [53], the composed image retrieval task is extended to a multi-turn conversation. The authors proposed a system that utilizes ComposeAE [3] to combine image and text at each turn. The combined representation is then fed into a recurrent network, following the turn order, for further processing. In [26], the authors present the SAC (Semantic Attention Composition) framework, which consists of two modules: the Semantic Feature Attention (SFA) module finds the salient regions of the image w.r.t. the text, and then the Semantic Feature Modification (SFM) module determines how to change the relevant parts of the image compositing coarse and fine salient image features computed by SFA with text embeddings.  

The proposed method starts with the hypothesis of having a unified embedding space for images and text achieved through the Vision-Language model CLIP. In the first stage, we fine-tune both CLIP encoders to adapt them to the composed image retrieval task. Next, using the task-adapted embedding spaces, we train a Combiner network to merge the multimodal features. In contrast to fashion-oriented approaches like [8, 31], our method does not rely on spatial features. Instead, we argue that when considering images of a broader domain, the semantics hold greater significance than local visual aspects.  

## 3 THE PROPOSED METHOD  

The proposed approach addresses the multimodal task of composed image retrieval. The input query consists of a reference image $I_{q}$ (e.g., an image of a black shirt with a cartoon lion) and a relative caption $T_{q}$ that includes a descriptive request from the user about the image (e.g., "has dog print and is dark grey color"). The goal is to retrieve target images that satisfy similarity constraints imposed by both the input components (e.g., an image of a dark grey shirt with a dog print, as shown in Fig. 3). For a successful retrieval, the system should understand the semantics of the image and the meaning of the text, integrate the multi-domain information, and then use the fused representation to retrieve the relevant images.  
æ‰€æå‡ºçš„æ–¹æ³•è§£å†³äº†ç»„åˆå›¾åƒæ£€ç´¢çš„å¤šæ¨¡æ€ä»»åŠ¡ã€‚è¾“å…¥æŸ¥è¯¢ç”±å‚è€ƒå›¾åƒ$I_{q}$ï¼ˆä¾‹å¦‚ï¼Œä¸€ä»¶å¸¦æœ‰å¡é€šç‹®å­çš„é»‘è‰²è¡¬è¡«çš„å›¾åƒï¼‰å’ŒåŒ…å«ç”¨æˆ·å…³äºå›¾åƒçš„æè¿°æ€§è¯·æ±‚çš„ç›¸å¯¹è¯´æ˜$T_{q}$ï¼ˆä¾‹å¦‚ï¼Œ"æœ‰ç‹—å›¾æ¡ˆä¸”æ˜¯æ·±ç°è‰²"ï¼‰ç»„æˆã€‚ç›®æ ‡æ˜¯æ£€ç´¢æ»¡è¶³ä¸¤ä¸ªè¾“å…¥ç»„ä»¶æ‰€æ–½åŠ çš„ç›¸ä¼¼æ€§çº¦æŸçš„ç›®æ ‡å›¾åƒï¼ˆä¾‹å¦‚ï¼Œä¸€ä»¶å¸¦æœ‰ç‹—å›¾æ¡ˆçš„æ·±ç°è‰²è¡¬è¡«ï¼Œå¦‚å›¾3æ‰€ç¤ºï¼‰ã€‚ä¸ºäº†æˆåŠŸæ£€ç´¢ï¼Œç³»ç»Ÿåº”ç†è§£å›¾åƒçš„è¯­ä¹‰å’Œæ–‡æœ¬çš„å«ä¹‰ï¼Œæ•´åˆå¤šé¢†åŸŸä¿¡æ¯ï¼Œç„¶åä½¿ç”¨èåˆè¡¨ç¤ºæ¥æ£€ç´¢ç›¸å…³å›¾åƒã€‚

In contrast to previous works like [8, 29, 31, 45] that build from different image and textual models, we start from the hypothesis of having a unified embedding of images and text, obtained through using the CLIP model [41]. CLIP is a vision-language model trained to align images and their corresponding text captions in a unified embedding space. It consists of an image encoder $\psi_{I}$ and a text encoder $\psi_{T}$ . Given an image $I_{i}$ , the image encoder extracts a feature representation $\psi_{I}(I)\in\mathbb{R}^{d}$ , where $d$ is the size of the CLIP embedding space. Similarly, for a given text caption $T$ , the text encoder extracts a feature representation $\psi_{T}(T)\in\mathbb{R}^{d}$ . CLIP learns to map similar concepts expressed in images and text to similar feature representations. For instance, given the image of a cat $I_{c}$ and the text $T_{c}$ â€œa photo of a cat", the way CLIP is trained should guarantee that $\psi_{I}(I_{c})\approx\psi_{T}(T_{c})$ .  
ä¸åƒ[8, 29, 31, 45]è¿™æ ·ä»ä¸åŒå›¾åƒå’Œæ–‡æœ¬æ¨¡å‹æ„å»ºçš„å…ˆå‰å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬ä»å‡è®¾æ‹¥æœ‰é€šè¿‡ä½¿ç”¨CLIPæ¨¡å‹[41]è·å¾—çš„å›¾åƒå’Œæ–‡æœ¬çš„ç»Ÿä¸€åµŒå…¥å¼€å§‹ã€‚CLIPæ˜¯ä¸€ç§è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç»è¿‡è®­ç»ƒå¯ä»¥åœ¨ç»Ÿä¸€çš„åµŒå…¥ç©ºé—´ä¸­å¯¹é½å›¾åƒåŠå…¶ç›¸åº”çš„æ–‡æœ¬æ ‡é¢˜ã€‚å®ƒç”±å›¾åƒç¼–ç å™¨$\psi_{I}$å’Œæ–‡æœ¬ç¼–ç å™¨$\psi_{T}$ç»„æˆã€‚ç»™å®šå›¾åƒ$I_{i}$ï¼Œå›¾åƒç¼–ç å™¨æå–ç‰¹å¾è¡¨ç¤º$\psi_{I}(I)\in\mathbb{R}^{d}$ï¼Œå…¶ä¸­$d$æ˜¯CLIPåµŒå…¥ç©ºé—´çš„å¤§å°ã€‚ç±»ä¼¼åœ°ï¼Œå¯¹äºç»™å®šçš„æ–‡æœ¬æ ‡é¢˜$T$ï¼Œæ–‡æœ¬ç¼–ç å™¨æå–ç‰¹å¾è¡¨ç¤º$\psi_{T}(T)\in\mathbb{R}^{d}$ã€‚CLIPå­¦ä¹ å°†å›¾åƒå’Œæ–‡æœ¬ä¸­è¡¨è¾¾çš„ç›¸ä¼¼æ¦‚å¿µæ˜ å°„åˆ°ç›¸ä¼¼çš„ç‰¹å¾è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œç»™å®šçŒ«çš„å›¾åƒ$I_{c}$å’Œæ–‡æœ¬$T_{c}$"çŒ«çš„ç…§ç‰‡"ï¼ŒCLIPçš„è®­ç»ƒæ–¹å¼åº”è¯¥ä¿è¯$\psi_{I}(I_{c})\approx\psi_{T}(T_{c})$ã€‚

We argue that, even though having a unified embedding space is a good starting point, it is not exactly what we need in the task we are considering. In composed image retrieval, the goal is to move from the reference to the target image point in the image embedding space with the aid of textual information. Hence, instead of utilizing a unified image-text embedding space, our approach involves creating two separate embedding spaces that can be combined through a sum operation. Formally, given an image of a black dress $I_{x}$ and the corresponding text $T_{y}$ ("is blue"). Let $I_{z}$ represent the image of a blue dress. Our aim is to shape the embedding spaces such that $\psi_{I}(I_{x})+\psi_{T}(T_{y})\approx\psi_{I}(I_{z})$ . When this equation is satisfied, we can affirm that the textual embedding space exhibits strong "additivity properties" in relation to the image space, or equivalently, the embedding spaces are additive.  
æˆ‘ä»¬è®¤ä¸ºï¼Œå°½ç®¡æ‹¥æœ‰ç»Ÿä¸€çš„åµŒå…¥ç©ºé—´æ˜¯ä¸€ä¸ªè‰¯å¥½çš„èµ·ç‚¹ï¼Œä½†è¿™å¹¶ä¸å®Œå…¨ç¬¦åˆæˆ‘ä»¬æ‰€è€ƒè™‘ä»»åŠ¡çš„éœ€æ±‚ã€‚åœ¨ç»„åˆå›¾åƒæ£€ç´¢ä¸­ï¼Œç›®æ ‡æ˜¯åœ¨æ–‡æœ¬ä¿¡æ¯çš„å¸®åŠ©ä¸‹ï¼Œåœ¨å›¾åƒåµŒå…¥ç©ºé—´ä¸­ä»å‚è€ƒå›¾åƒç‚¹ç§»åŠ¨åˆ°ç›®æ ‡å›¾åƒç‚¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸æ˜¯åˆ©ç”¨ç»Ÿä¸€çš„å›¾åƒ-æ–‡æœ¬åµŒå…¥ç©ºé—´ï¼Œè€Œæ˜¯åˆ›å»ºä¸¤ä¸ªå¯ä»¥é€šè¿‡æ±‚å’Œæ“ä½œç»„åˆçš„ç‹¬ç«‹åµŒå…¥ç©ºé—´ã€‚å½¢å¼ä¸Šï¼Œç»™å®šä¸€å¼ é»‘è‰²è¿è¡£è£™çš„å›¾åƒ$I_{x}$å’Œç›¸åº”çš„æ–‡æœ¬$T_{y}$ï¼ˆ"æ˜¯è“è‰²çš„"ï¼‰ã€‚è®©$I_{z}$è¡¨ç¤ºè“è‰²è¿è¡£è£™çš„å›¾åƒã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¡‘é€ åµŒå…¥ç©ºé—´ï¼Œä½¿å¾—$\psi_{I}(I_{x})+\psi_{T}(T_{y})\approx\psi_{I}(I_{z})$ã€‚å½“è¿™ä¸ªç­‰å¼æˆç«‹æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®è®¤æ–‡æœ¬åµŒå…¥ç©ºé—´ç›¸å¯¹äºå›¾åƒç©ºé—´è¡¨ç°å‡ºå¼ºçƒˆçš„"åŠ æ€§å±æ€§"ï¼Œæˆ–è€…ç­‰æ•ˆåœ°è¯´ï¼ŒåµŒå…¥ç©ºé—´æ˜¯å¯åŠ çš„ã€‚

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/46dca4d6af1dea2eef6c5c64f6fb139f75a8787e5e1988525686d61ff3c03c53.jpg)  

Fig. 2. First stage of training. In this stage, we perform a task-oriented fine-tuning of CLIP encoders to reduce the mismatch between the large-scale pre-training and the downstream task. We start by extracting the image-text query features and combining them through an element-wise sum. We then employ a contrastive loss to minimize the distance between combined features and target image features in the same triplet and maximize the distance from the other images in the batch. We update the weights of both CLIP encoders.  
å›¾2. è®­ç»ƒçš„ç¬¬ä¸€é˜¶æ®µã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬å¯¹CLIPç¼–ç å™¨è¿›è¡Œé¢å‘ä»»åŠ¡çš„å¾®è°ƒï¼Œä»¥å‡å°‘å¤§è§„æ¨¡é¢„è®­ç»ƒä¸ä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´çš„ä¸åŒ¹é…ã€‚æˆ‘ä»¬é¦–å…ˆæå–å›¾åƒ-æ–‡æœ¬æŸ¥è¯¢ç‰¹å¾ï¼Œå¹¶é€šè¿‡é€å…ƒç´ æ±‚å’Œå°†å®ƒä»¬ç»„åˆèµ·æ¥ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨å¯¹æ¯”æŸå¤±æ¥æœ€å°åŒ–ç»„åˆç‰¹å¾ä¸åŒä¸€ä¸‰å…ƒç»„ä¸­ç›®æ ‡å›¾åƒç‰¹å¾ä¹‹é—´çš„è·ç¦»ï¼Œå¹¶æœ€å¤§åŒ–ä¸æ‰¹æ¬¡ä¸­å…¶ä»–å›¾åƒçš„è·ç¦»ã€‚æˆ‘ä»¬æ›´æ–°ä¸¤ä¸ªCLIPç¼–ç å™¨çš„æƒé‡ã€‚

Ideally, the embeddings of the relative caption should correspond to the displacement vector from the query image to the target image features, i.e. $\psi_{T}(T_{y})\approx\psi_{I}(I_{z})-\psi_{I}(I_{x})$ .  

We propose a two-stage approach to address the task of composed image retrieval by taking full advantage of the capabilities of CLIPâ€™s features. In the first stage, we tackle the objective mismatch between the large-scale pretraining of CLIP and the downstream task: we propose a novel fine-tuning scheme tailored to improving the additivity properties of the embedding spaces. In the second stage, starting from the task-oriented features, we train from scratch a Combiner neural network that learns to perform a fine-grained combination of image-text features. Although we train the Combiner network from scratch, we design its structure to take full advantage of the first stage of training (see Section 3.2 for more details). During both stages the training is performed using triplets $(I_{q},T_{q},I_{t})$ , where $q=(I_{q},T_{q})$ is the query and $I_{t}$ is the target image that we aim to retrieve given $q$ .  
ç†æƒ³æƒ…å†µä¸‹ï¼Œç›¸å¯¹è¯´æ˜çš„åµŒå…¥åº”è¯¥å¯¹åº”äºä»æŸ¥è¯¢å›¾åƒåˆ°ç›®æ ‡å›¾åƒç‰¹å¾çš„ä½ç§»å‘é‡ï¼Œå³$\psi_{T}(T_{y})\approx\psi_{I}(I_{z})-\psi_{I}(I_{x})$ã€‚

æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ–¹æ³•æ¥è§£å†³ç»„åˆå›¾åƒæ£€ç´¢ä»»åŠ¡ï¼Œå……åˆ†åˆ©ç”¨CLIPç‰¹å¾çš„èƒ½åŠ›ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬è§£å†³CLIPå¤§è§„æ¨¡é¢„è®­ç»ƒä¸ä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´çš„ç›®æ ‡ä¸åŒ¹é…é—®é¢˜ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¾®è°ƒæ–¹æ¡ˆï¼Œæ—¨åœ¨æ”¹å–„åµŒå…¥ç©ºé—´çš„åŠ æ€§å±æ€§ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œä»é¢å‘ä»»åŠ¡çš„ç‰¹å¾å¼€å§‹ï¼Œæˆ‘ä»¬ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªCombinerç¥ç»ç½‘ç»œï¼Œå­¦ä¹ æ‰§è¡Œå›¾åƒ-æ–‡æœ¬ç‰¹å¾çš„ç²¾ç»†ç»„åˆã€‚å°½ç®¡æˆ‘ä»¬ä»å¤´å¼€å§‹è®­ç»ƒCombinerç½‘ç»œï¼Œä½†æˆ‘ä»¬è®¾è®¡å…¶ç»“æ„ä»¥å……åˆ†åˆ©ç”¨ç¬¬ä¸€é˜¶æ®µçš„è®­ç»ƒï¼ˆè¯¦è§3.2èŠ‚ï¼‰ã€‚åœ¨ä¸¤ä¸ªé˜¶æ®µä¸­ï¼Œè®­ç»ƒéƒ½ä½¿ç”¨ä¸‰å…ƒç»„$(I_{q},T_{q},I_{t})$è¿›è¡Œï¼Œå…¶ä¸­$q=(I_{q},T_{q})$æ˜¯æŸ¥è¯¢ï¼Œ$I_{t}$æ˜¯ç»™å®š$q$æˆ‘ä»¬å¸Œæœ›æ£€ç´¢çš„ç›®æ ‡å›¾åƒã€‚

At inference time, given a query $(I_{q},T_{q})$ , we utilize the fine-tuned CLIP encoders and the trained Combiner network to generate the combined features. Subsequently, following the standard image-to-image retrieval approach, we compute the cosine distances between the combined features and the database of index image features. The results are then sorted based on their similarity.  
åœ¨æ¨ç†é˜¶æ®µï¼Œç»™å®šæŸ¥è¯¢$(I_{q},T_{q})$ï¼Œæˆ‘ä»¬åˆ©ç”¨å¾®è°ƒåçš„CLIPç¼–ç å™¨å’Œè®­ç»ƒå¥½çš„Combinerç½‘ç»œç”Ÿæˆç»„åˆç‰¹å¾ã€‚éšåï¼ŒæŒ‰ç…§æ ‡å‡†çš„å›¾åƒåˆ°å›¾åƒæ£€ç´¢æ–¹æ³•ï¼Œæˆ‘ä»¬è®¡ç®—ç»„åˆç‰¹å¾ä¸ç´¢å¼•å›¾åƒç‰¹å¾æ•°æ®åº“ä¹‹é—´çš„ä½™å¼¦è·ç¦»ã€‚ç»“æœç„¶åæ ¹æ®ç›¸ä¼¼åº¦è¿›è¡Œæ’åºã€‚

## 3.1 Task-oriented fine-tuning  

In this stage, we adapt both CLIPâ€™s encoders to composed image retrieval reducing the mismatch between the large-scale pre-training and the downstream task. Given a query consisting of a reference image $I_{q}$ and a relative caption $T_{q}$ , we extract their feature representations using the CLIP image encoder $\psi_{I}$ and text encoder $\psi_{T}$ respectively. This results in $\psi_{I}(I_{q})\in\mathbb{R}^{d}$ and $\psi_{T}(T_{q})\in\mathbb R^{d}$ , where $d$ denotes the size of the CLIP embedding space. To combine the query features, we perform an element-wise sum, resulting in $\phi_{q}=\psi_{I}(I_{q})+\psi_{T}(T_{q})$ .  
åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬å°†ä¸¤ä¸ªCLIPç¼–ç å™¨é€‚é…åˆ°ç»„åˆå›¾åƒæ£€ç´¢ä»»åŠ¡ï¼Œå‡å°‘å¤§è§„æ¨¡é¢„è®­ç»ƒä¸ä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´çš„ä¸åŒ¹é…ã€‚ç»™å®šä¸€ä¸ªç”±å‚è€ƒå›¾åƒ$I_{q}$å’Œç›¸å¯¹è¯´æ˜$T_{q}$ç»„æˆçš„æŸ¥è¯¢ï¼Œæˆ‘ä»¬åˆ†åˆ«ä½¿ç”¨CLIPå›¾åƒç¼–ç å™¨$\psi_{I}$å’Œæ–‡æœ¬ç¼–ç å™¨$\psi_{T}$æå–å®ƒä»¬çš„ç‰¹å¾è¡¨ç¤ºã€‚è¿™å°†å¾—åˆ°$\psi_{I}(I_{q})\in\mathbb{R}^{d}$å’Œ$\psi_{T}(T_{q})\in\mathbb R^{d}$ï¼Œå…¶ä¸­$d$è¡¨ç¤ºCLIPåµŒå…¥ç©ºé—´çš„å¤§å°ã€‚ä¸ºäº†ç»„åˆæŸ¥è¯¢ç‰¹å¾ï¼Œæˆ‘ä»¬æ‰§è¡Œé€å…ƒç´ æ±‚å’Œï¼Œå¾—åˆ°$\phi_{q}=\psi_{I}(I_{q})+\psi_{T}(T_{q})$ã€‚

Our objective is to minimize the distance between the query combined features $\phi_{q}$ and the target image features $\phi_{t}=\psi_{I}(I_{t})$ belonging to the same triplet and, at the same time, maximize the distance from the other target images in the same batch. To this end, following [31, 45, 48], we employ a batch-based contrastive loss:  
æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æœ€å°åŒ–æŸ¥è¯¢ç»„åˆç‰¹å¾$\phi_{q}$ä¸å±äºåŒä¸€ä¸‰å…ƒç»„çš„ç›®æ ‡å›¾åƒç‰¹å¾$\phi_{t}=\psi_{I}(I_{t})$ä¹‹é—´çš„è·ç¦»ï¼ŒåŒæ—¶æœ€å¤§åŒ–ä¸åŒä¸€æ‰¹æ¬¡ä¸­å…¶ä»–ç›®æ ‡å›¾åƒçš„è·ç¦»ã€‚ä¸ºæ­¤ï¼Œå‚è€ƒ[31, 45, 48]ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºæ‰¹æ¬¡çš„å¯¹æ¯”æŸå¤±ï¼š

$$
\mathcal{L}_{c o n t r}=\frac{1}{B}\sum_{i=1}^{B}-\log\frac{\exp\{\tau*\kappa(\phi_{q}^{i},\phi_{t}^{i})\}}{\sum_{j=1}^{B}\exp\{\tau*\kappa(\phi_{q}^{i},\phi_{t}^{j})\}}
$$  

Here, $\kappa(\cdot)$ denotes the cosine similarity, $\tau$ is a temperature parameter that controls the range of the logits, and $B$ is the number of images in a batch. We update the weights of both CLIP encoders. We use this loss because, being a batch-wise contrastive loss, it does not require the definition of a sampling strategy: it considers all negative samples in a mini-batch. Figure 2 shows an overview of the task-oriented fine-tuning stage.  
è¿™é‡Œï¼Œ$\kappa(\cdot)$è¡¨ç¤ºä½™å¼¦ç›¸ä¼¼åº¦ï¼Œ$\tau$æ˜¯æ§åˆ¶logitsèŒƒå›´çš„æ¸©åº¦å‚æ•°ï¼Œ$B$æ˜¯æ‰¹æ¬¡ä¸­çš„å›¾åƒæ•°é‡ã€‚æˆ‘ä»¬æ›´æ–°ä¸¤ä¸ªCLIPç¼–ç å™¨çš„æƒé‡ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ç§æŸå¤±å‡½æ•°æ˜¯å› ä¸ºï¼Œä½œä¸ºä¸€ç§åŸºäºæ‰¹æ¬¡çš„å¯¹æ¯”æŸå¤±ï¼Œå®ƒä¸éœ€è¦å®šä¹‰é‡‡æ ·ç­–ç•¥ï¼šå®ƒè€ƒè™‘äº†å°æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰è´Ÿæ ·æœ¬ã€‚å›¾2å±•ç¤ºäº†é¢å‘ä»»åŠ¡çš„å¾®è°ƒé˜¶æ®µçš„æ¦‚è§ˆã€‚

Using the element-wise sum as the combination of query features goes in the direction of making CLIPâ€™s embedding spaces more additive. Consequently, similar concepts expressed in text and images no longer share similar features. Instead, the textual features serve as displacement vectors from the query to the target in the image space. From a high-level perspective, we notice that, in composed image retrieval, the image and the text do not play the same role. The task is not symmetric with respect to the input: we start from an image, and we would like to retrieve another image using textual guidance. For this reason, the break up of the unified embedding space is not an undesirable side-effect.  
ä½¿ç”¨é€å…ƒç´ æ±‚å’Œä½œä¸ºæŸ¥è¯¢ç‰¹å¾çš„ç»„åˆæ–¹å¼æœ‰åŠ©äºä½¿CLIPçš„åµŒå…¥ç©ºé—´æ›´å…·åŠ æ€§ã€‚å› æ­¤ï¼Œæ–‡æœ¬å’Œå›¾åƒä¸­è¡¨è¾¾çš„ç›¸ä¼¼æ¦‚å¿µä¸å†å…±äº«ç›¸ä¼¼ç‰¹å¾ã€‚ç›¸åï¼Œæ–‡æœ¬ç‰¹å¾ä½œä¸ºå›¾åƒç©ºé—´ä¸­ä»æŸ¥è¯¢åˆ°ç›®æ ‡çš„ä½ç§»å‘é‡ã€‚ä»é«˜å±‚æ¬¡çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°ï¼Œåœ¨ç»„åˆå›¾åƒæ£€ç´¢ä¸­ï¼Œå›¾åƒå’Œæ–‡æœ¬ä¸æ‰®æ¼”ç›¸åŒçš„è§’è‰²ã€‚è¯¥ä»»åŠ¡ç›¸å¯¹äºè¾“å…¥ä¸æ˜¯å¯¹ç§°çš„ï¼šæˆ‘ä»¬ä»ä¸€ä¸ªå›¾åƒå¼€å§‹ï¼Œå¸Œæœ›ä½¿ç”¨æ–‡æœ¬æŒ‡å¯¼æ£€ç´¢å¦ä¸€ä¸ªå›¾åƒã€‚å› æ­¤ï¼Œç»Ÿä¸€åµŒå…¥ç©ºé—´çš„åˆ†è§£å¹¶ä¸æ˜¯ä¸€ä¸ªä¸è‰¯çš„å‰¯ä½œç”¨ã€‚

æˆ‘ä»¬å°†å¾®è°ƒåçš„å›¾åƒç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨åˆ†åˆ«è¡¨ç¤ºä¸º $\overline{{\psi_{I}}}$å’Œ$\overline{{\psi_{T}}}$ã€‚
We will denote the fine-tuned image encoder and text encoder as $\overline{{\psi_{I}}}$ and $\overline{{\psi_{T}}}$ , respectively.  

## 3.2 Combiner training  

During the training of the Combiner network, we follow the same general framework as in the previous stage. However, this time we train from scratch the Combiner network instead of updating the weights of the CLIP encoders. In contrast to the first stage, we use the Combiner network $C_{\theta}$ to combine the query features. Specifically, the combined features are obtained as $\overline{{\phi_{q}}}=C_{\theta}(\overline{{\psi_{I}}}(I_{q}),\overline{{\psi_{T}}}(T_{q}))$ . We optimize the Combiner network by utilizing the $\mathcal{L}_{c o n t r}$ loss described in Eq. (1) with $\overline{{\phi_{q}}}$ and $\overline{{\phi_{t}}}=\overline{{\psi_{T}}}(I_{t})$ as inputs. Figure 3 depicts a visual overview of the Combiner network training stage. By employing the contrastive loss, we train the Combiner $C_{\theta}$ to produce features as close as possible to the target features and as far away as possible from all other image features.  
åœ¨Combinerç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éµå¾ªä¸å‰ä¸€é˜¶æ®µç›¸åŒçš„æ€»ä½“æ¡†æ¶ã€‚ç„¶è€Œï¼Œè¿™æ¬¡æˆ‘ä»¬ä»å¤´å¼€å§‹è®­ç»ƒCombinerç½‘ç»œï¼Œè€Œä¸æ˜¯æ›´æ–°CLIPç¼–ç å™¨çš„æƒé‡ã€‚ä¸ç¬¬ä¸€é˜¶æ®µä¸åŒï¼Œæˆ‘ä»¬ä½¿ç”¨Combinerç½‘ç»œ$C_{\theta}$æ¥ç»„åˆæŸ¥è¯¢ç‰¹å¾ã€‚å…·ä½“æ¥è¯´ï¼Œç»„åˆç‰¹å¾é€šè¿‡$\overline{{\phi_{q}}}=C_{\theta}(\overline{{\psi_{I}}}(I_{q}),\overline{{\psi_{T}}}(T_{q}))$è·å¾—ã€‚æˆ‘ä»¬åˆ©ç”¨å…¬å¼(1)ä¸­æè¿°çš„$\mathcal{L}_{contr}$æŸå¤±æ¥ä¼˜åŒ–Combinerç½‘ç»œï¼Œå°†$\overline{{\phi_{q}}}$å’Œ$\overline{{\phi_{t}}}=\overline{{\psi_{T}}}(I_{t})$ä½œä¸ºè¾“å…¥ã€‚å›¾3å±•ç¤ºäº†Combinerç½‘ç»œè®­ç»ƒé˜¶æ®µçš„æ•´ä½“è§†å›¾ã€‚é€šè¿‡é‡‡ç”¨å¯¹æ¯”æŸå¤±ï¼Œæˆ‘ä»¬è®­ç»ƒCombiner $C_{\theta}$ç”Ÿæˆå°½å¯èƒ½æ¥è¿‘ç›®æ ‡ç‰¹å¾ã€åŒæ—¶è¿œç¦»å…¶ä»–å›¾åƒç‰¹å¾çš„ç‰¹å¾è¡¨ç¤ºã€‚

The Combiner network, depicted in Fig. 4, is designed to take full advantage of the first stage of training and the increased additivity properties of the adapted embedding spaces. The idea is to learn the residual of a convex combination of the image-text query features. We begin by projecting the text and image features through a linear transformation followed by a ReLU function. The resulting projected features are then concatenated and passed to two separate branches. The first branch, labeled as (1) in Fig. 4, is responsible for computing the coefficients of a convex combination between the image and text features. To compute these coefficients, we feed the concatenated features into a linear layer, followed by the ReLU function, another linear layer, and the sigmoid function. The sigmoid output provides the coefficients needed for the query image-text convex combination. The second branch, labeled as (2), outputs the mixture contribution of the image and text features. The structure of this branch is the same as the first branch, except it does not include the final sigmoid function. Finally, we sum the convex combination of the query features and the learned image-text mixture. To reduce overfitting, we apply dropout to each layer.  
å¦‚å›¾4æ‰€ç¤ºçš„Combinerç½‘ç»œï¼Œå…¶è®¾è®¡æ—¨åœ¨å……åˆ†åˆ©ç”¨ç¬¬ä¸€é˜¶æ®µçš„è®­ç»ƒæˆæœå’Œé€‚é…ååµŒå…¥ç©ºé—´å¢å¼ºçš„åŠ æ€§å±æ€§ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯å­¦ä¹ å›¾åƒ-æ–‡æœ¬æŸ¥è¯¢ç‰¹å¾å‡¸ç»„åˆçš„æ®‹å·®ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡çº¿æ€§å˜æ¢å’ŒReLUå‡½æ•°å¯¹æ–‡æœ¬å’Œå›¾åƒç‰¹å¾è¿›è¡ŒæŠ•å½±ã€‚å°†æŠ•å½±åçš„ç‰¹å¾æ‹¼æ¥åï¼Œä¼ é€’è‡³ä¸¤ä¸ªç‹¬ç«‹çš„åˆ†æ”¯ã€‚å›¾4ä¸­æ ‡è®°ä¸º(1)çš„ç¬¬ä¸€ä¸ªåˆ†æ”¯è´Ÿè´£è®¡ç®—å›¾åƒä¸æ–‡æœ¬ç‰¹å¾ä¹‹é—´å‡¸ç»„åˆçš„ç³»æ•°ã€‚ä¸ºäº†è®¡ç®—è¿™äº›ç³»æ•°ï¼Œæˆ‘ä»¬å°†æ‹¼æ¥ç‰¹å¾è¾“å…¥çº¿æ€§å±‚ï¼Œéšåç»è¿‡ReLUå‡½æ•°ã€å¦ä¸€ä¸ªçº¿æ€§å±‚å’Œsigmoidå‡½æ•°ã€‚sigmoidè¾“å‡ºæä¾›æŸ¥è¯¢å›¾åƒ-æ–‡æœ¬å‡¸ç»„åˆæ‰€éœ€çš„ç³»æ•°ã€‚æ ‡è®°ä¸º(2)çš„ç¬¬äºŒä¸ªåˆ†æ”¯è¾“å‡ºå›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„æ··åˆè´¡çŒ®ã€‚è¯¥åˆ†æ”¯ç»“æ„ä¸ç¬¬ä¸€ä¸ªåˆ†æ”¯ç›¸åŒï¼Œä½†ä¸åŒ…å«æœ€ç»ˆçš„sigmoidå‡½æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹æŸ¥è¯¢ç‰¹å¾çš„å‡¸ç»„åˆä¸å­¦ä¹ åˆ°çš„å›¾åƒ-æ–‡æœ¬æ··åˆç»“æœè¿›è¡Œæ±‚å’Œã€‚ä¸ºå‡å°‘è¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªå±‚åº”ç”¨dropoutã€‚

By denoting the outputs of the first branch (1) as $\lambda$ and $1-\lambda$ , and the output of the second branch (2) as $v$ , we can express the combined features as $\overline{{{\phi_{q}}}}=\left(1-\lambda\right)*\overline{{{\psi_{I}}}}(I_{q})+\lambda*\overline{{{\psi_{T}}}}(T_{q})+v$ . Notably, the convex combination $\left(1-\lambda\right)*\overline{{\psi_{I}}}(I_{q})+\lambda*\overline{{\psi_{T}}}(T_{q})$ is a generalization of the element-wise sum of the query features. Consequently, as the embedding spaces exhibit stronger additivity properties, the Combinerâ€™s effectiveness in its task is enhanced. We intentionally design the Combiner to capitalize on the task adaptation achieved during the first stage of training.  
é€šè¿‡å°†ç¬¬ä¸€ä¸ªåˆ†æ”¯(1)çš„è¾“å‡ºè¡¨ç¤ºä¸º$\lambda$å’Œ$1-\lambda$ï¼Œç¬¬äºŒä¸ªåˆ†æ”¯(2)çš„è¾“å‡ºè¡¨ç¤ºä¸º$v$ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç»„åˆç‰¹å¾è¡¨è¾¾ä¸º$\overline{{{\phi_{q}}}}=\left(1-\lambda\right)*\overline{{{\psi_{I}}}}(I_{q})+\lambda*\overline{{{\psi_{T}}}}(T_{q})+v$ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå‡¸ç»„åˆ$\left(1-\lambda\right)*\overline{{\psi_{I}}}(I_{q})+\lambda*\overline{{\psi_{T}}}(T_{q})$æ˜¯æŸ¥è¯¢ç‰¹å¾é€å…ƒç´ æ±‚å’Œçš„ä¸€èˆ¬åŒ–å½¢å¼ã€‚å› æ­¤ï¼Œéšç€åµŒå…¥ç©ºé—´å±•ç°å‡ºæ›´å¼ºçš„åŠ æ€§å±æ€§ï¼ŒCombineråœ¨ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ä¹Ÿéšä¹‹å¢å¼ºã€‚æˆ‘ä»¬ç‰¹æ„è®¾è®¡Combinerç½‘ç»œä»¥å……åˆ†åˆ©ç”¨ç¬¬ä¸€é˜¶æ®µè®­ç»ƒä¸­è·å¾—çš„ä»»åŠ¡é€‚é…èƒ½åŠ›ã€‚

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/ccbff7eb3d754da86de3e4bd9e07ac9394da7b6f06ced2c0872998476772acdd.jpg)  
Fig. 3. Second stage of training. In this stage, we train from scratch a Combiner network that learns to fuse the multimodal features extracted with CLIP encoders. We start by extracting the image-text query features using the fine-tuned encoders, and we combine them using the Combiner network. We then employ a contrastive loss to minimize the distance between combined features and target image features in the same triplet and maximize the distance from the other images in the batch. We keep both CLIP encoders frozen while we only update the weights of the Combiner network. At inference time the fine-tuned encoders and the trained Combiner are used to produce an effective representation used to query the database.  
è®­ç»ƒçš„ç¬¬äºŒé˜¶æ®µã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬ä»å¤´å¼€å§‹è®­ç»ƒCombinerç½‘ç»œï¼Œå­¦ä¹ èåˆç”±CLIPç¼–ç å™¨æå–çš„å¤šæ¨¡æ€ç‰¹å¾ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨å¾®è°ƒåçš„ç¼–ç å™¨æå–å›¾åƒ-æ–‡æœ¬æŸ¥è¯¢ç‰¹å¾ï¼Œå¹¶é€šè¿‡Combinerç½‘ç»œè¿›è¡Œç»„åˆã€‚ç„¶åé‡‡ç”¨å¯¹æ¯”æŸå¤±æ¥æœ€å°åŒ–ç»„åˆç‰¹å¾ä¸åŒä¸€ä¸‰å…ƒç»„ä¸­ç›®æ ‡å›¾åƒç‰¹å¾çš„è·ç¦»ï¼ŒåŒæ—¶æœ€å¤§åŒ–ä¸æ‰¹æ¬¡ä¸­å…¶ä»–å›¾åƒçš„è·ç¦»ã€‚æˆ‘ä»¬ä¿æŒä¸¤ä¸ªCLIPç¼–ç å™¨çš„å‚æ•°å†»ç»“ï¼Œä»…æ›´æ–°Combinerç½‘ç»œçš„æƒé‡ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œå¾®è°ƒåçš„ç¼–ç å™¨ä¸è®­ç»ƒå¥½çš„Combinerç½‘ç»œå…±åŒç”Ÿæˆç”¨äºæ•°æ®åº“æŸ¥è¯¢çš„æœ‰æ•ˆç‰¹å¾è¡¨ç¤ºã€‚

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/bd805296f3cc380c6d9485a28658955c6a8af8bf3b3ca10eec6abaacafc8f0d2.jpg)  
Fig. 4. Architecture of the Combiner network $C_{\theta}$ . It takes as input the multimodal query features and outputs a unified representation. $\sigma$ represents the sigmoid function. We denote the outputs of the first branch (1) as $\lambda$ and $1-\lambda$ , while the output of the second branch (2) as ğ‘£. The combined features are $\overline{{{\phi_{q}}}}=\left(1-\lambda\right)*\overline{{{\psi_{I}}}}(I_{q})+\lambda*\overline{{{\psi_{T}}}}(T_{q})+v$  
å›¾4. Combinerç½‘ç»œ$C_{\theta}$çš„æ¶æ„ã€‚è¯¥ç½‘ç»œä»¥å¤šæ¨¡æ€æŸ¥è¯¢ç‰¹å¾ä¸ºè¾“å…¥ï¼Œè¾“å‡ºç»Ÿä¸€çš„ç‰¹å¾è¡¨ç¤ºã€‚$\sigma$è¡¨ç¤ºsigmoidå‡½æ•°ã€‚ç¬¬ä¸€ä¸ªåˆ†æ”¯(1)çš„è¾“å‡ºè®°ä¸º$\lambda$å’Œ$1-\lambda$ï¼Œç¬¬äºŒä¸ªåˆ†æ”¯(2)çš„è¾“å‡ºè®°ä¸ºğ‘£ã€‚ç»„åˆç‰¹å¾ä¸º$\overline{{{\phi_{q}}}}=\left(1-\lambda\right)*\overline{{{\psi_{I}}}}(I_{q})+\lambda*\overline{{{\psi_{T}}}}(T_{q})+v$ã€‚

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/71686ea0c1a3a745747f481ec3e14f0f99f7773428cf16cf0142b61a91fe3a50.jpg)  
Fig. 5. Histogram of image aspect ratios in FashionIQ and CIRR datasets (a) and the three categories of FashionIQ (b). The $\mathbf{x}\cdot\mathbf{\partial}$ -axis represents the aspect ratio defined as ğ‘šğ‘ğ‘¥ (ğ‘¤ğ‘–ğ‘‘ğ‘¡â„, â„ğ‘’ğ‘–ğ‘”â„ğ‘¡)/ğ‘šğ‘–ğ‘›(ğ‘¤ğ‘–ğ‘‘ğ‘¡â„, â„ğ‘’ğ‘–ğ‘”â„ğ‘¡) while the y-axis represents the number of images (in logarithmic scale). The width of each bin is 0.5, and the first bin starts at 1. More than half of the datasetâ€™s images are skewed and have at least a 1.5 aspect ratio. In the FashionIQ dataset, the issue is evident in the Dress category.  
å›¾5. FashionIQå’ŒCIRRæ•°æ®é›†(a)åŠFashionIQä¸‰ä¸ªå­ç±»(b)çš„å›¾åƒçºµæ¨ªæ¯”åˆ†å¸ƒç›´æ–¹å›¾ã€‚$\mathbf{x}\cdot\mathbf{\partial}$è½´è¡¨ç¤ºç”±ğ‘šğ‘ğ‘¥(ğ‘¤ğ‘–ğ‘‘ğ‘¡â„, â„ğ‘’ğ‘–ğ‘”â„ğ‘¡)/ğ‘šğ‘–ğ‘›(ğ‘¤ğ‘–ğ‘‘ğ‘¡â„, â„ğ‘’ğ‘–ğ‘”â„ğ‘¡)å®šä¹‰çš„çºµæ¨ªæ¯”ï¼Œyè½´è¡¨ç¤ºå›¾åƒæ•°é‡ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰ã€‚æ¯ä¸ªæ•°æ®æ¡çš„å®½åº¦ä¸º0.5ï¼Œç¬¬ä¸€ä¸ªæ•°æ®æ¡èµ·å§‹äº1ã€‚æ•°æ®é›†ä¸­è¶…è¿‡åŠæ•°çš„å›¾åƒå­˜åœ¨æ¯”ä¾‹å¤±è¡¡ï¼Œå…¶çºµæ¨ªæ¯”è‡³å°‘ä¸º1.5ã€‚åœ¨FashionIQæ•°æ®é›†ä¸­ï¼Œè¿™ä¸€ç°è±¡åœ¨Dressï¼ˆè¿è¡£è£™ï¼‰ç±»åˆ«ä¸­å°¤ä¸ºæ˜¾è‘—ã€‚

## 3.3 Preprocess Pipelineé¢„å¤„ç†æµç¨‹  

The standard preprocess pipeline of CLIP is mainly composed of two steps: a resize operation where the smaller side of the image matches the CLIP input dimension ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡_ğ‘‘ğ‘–ğ‘š followed by a center crop operation which results in a square patch ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡_ğ‘‘ğ‘–ğ‘š $\times$ ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡_ğ‘‘ğ‘–ğ‘š output. Subsequently, as the ratio between the largest and the smaller side increases, the area of the image lost after the preprocess increases. From now on, we will say that an image has a high aspect ratio when it is far from having a square shape. In Fig. 5 is shown how, in the datasets we consider (detailed in Section 4), the number of images with a high aspect ratio is not negligible. As can be seen, this is especially true for the FashionIQ dress category and the CIRR dataset.  
CLIPçš„æ ‡å‡†é¢„å¤„ç†æµç¨‹ä¸»è¦åŒ…å«ä¸¤ä¸ªæ­¥éª¤ï¼šé¦–å…ˆè¿›è¡Œå°ºå¯¸è°ƒæ•´ï¼ˆä½¿å›¾åƒçš„è¾ƒçŸ­è¾¹åŒ¹é…CLIPè¾“å…¥ç»´åº¦$input\_dim$ï¼‰ï¼Œéšåè¿›è¡Œä¸­å¿ƒè£å‰ªæ“ä½œï¼Œæœ€ç»ˆå¾—åˆ°$input\_dim \times input\_dim$çš„æ­£æ–¹å½¢å›¾åƒå—ã€‚éšç€å›¾åƒé•¿å®½æ¯”ä¾‹çš„å¢åŠ ï¼Œé¢„å¤„ç†è¿‡ç¨‹ä¸­ä¸¢å¤±çš„å›¾åƒåŒºåŸŸä¹Ÿä¼šç›¸åº”å¢åŠ ã€‚å½“å›¾åƒå½¢çŠ¶æ˜æ˜¾åç¦»æ­£æ–¹å½¢æ—¶ï¼Œæˆ‘ä»¬ç§°å…¶å…·æœ‰é«˜çºµæ¨ªæ¯”ã€‚å¦‚å›¾5æ‰€ç¤ºï¼Œåœ¨æˆ‘ä»¬ç ”ç©¶çš„æ•°æ®é›†ï¼ˆè¯¦è§ç¬¬4èŠ‚ï¼‰ä¸­ï¼Œé«˜çºµæ¨ªæ¯”å›¾åƒçš„æ•°é‡ä¸å¯å¿½è§†ï¼Œè¿™ä¸€ç°è±¡åœ¨FashionIQçš„Dressç±»åˆ«å’ŒCIRRæ•°æ®é›†ä¸­å°¤ä¸ºæ˜¾è‘—ã€‚

One way to address the loss of information due to the center crop operation is to apply zero-padding to match the smaller side to the larger side, effectively squaring the image. Although this approach eliminates the loss of content information, it also reduces the resolution of the useful portion of the image since the CLIP image encoder input dimension cannot change. Thus, we develop a preprocessing pipeline that seeks to balance the two approaches discussed above. Specifically, we apply padding to an image only if its aspect ratio exceeds a predefined target ratio. Additionally, instead of squaring the image, we adjust its aspect ratio to match the target ratio when padding is applied. The pseudocode for the proposed preprocess pipeline is shown in Algorithm 1.  
ä¸ºè§£å†³ä¸­å¿ƒè£å‰ªå¯¼è‡´çš„ä¿¡æ¯ä¸¢å¤±é—®é¢˜ï¼Œå¯é‡‡ç”¨é›¶å€¼å¡«å……æ–¹æ³•ä½¿è¾ƒçŸ­è¾¹ä¸è¾ƒé•¿è¾¹ç­‰é•¿ï¼Œä»è€Œå°†å›¾åƒè°ƒæ•´ä¸ºæ­£æ–¹å½¢ã€‚è™½ç„¶è¿™ç§æ–¹æ³•èƒ½ä¿ç•™å®Œæ•´å†…å®¹ä¿¡æ¯ï¼Œä½†ç”±äºCLIPå›¾åƒç¼–ç å™¨çš„è¾“å…¥ç»´åº¦å›ºå®šï¼Œä¼šå¯¼è‡´æœ‰æ•ˆåŒºåŸŸåˆ†è¾¨ç‡é™ä½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¹³è¡¡ä¸¤ç§æ–¹æ³•çš„é¢„å¤„ç†æµç¨‹ï¼šä»…åœ¨å›¾åƒçºµæ¨ªæ¯”è¶…è¿‡é¢„è®¾ç›®æ ‡æ¯”ä¾‹æ—¶æ·»åŠ å¡«å……ï¼Œä¸”å¡«å……æ—¶è°ƒæ•´å›¾åƒçºµæ¨ªæ¯”ä½¿å…¶åŒ¹é…ç›®æ ‡æ¯”ä¾‹è€Œéå¼ºåˆ¶è½¬ä¸ºæ­£æ–¹å½¢ã€‚å…·ä½“é¢„å¤„ç†æµç¨‹çš„ä¼ªä»£ç å¦‚ç®—æ³•1æ‰€ç¤ºã€‚

Figure 6 presents the preprocess pipelines as mentioned earlier. It is evident that when the ratio between the larger and smaller sides deviates significantly from one, the standard CLIP preprocess removes a substantial portion of the image, which considerably hampers the retrieval process. Although the visual disparities between the square pad and the proposed pad (with a target ratio of 1.25) approaches are not substantial, we will demonstrate that the model benefits from having such an increased usable portion in the images during retrieval.  
å›¾6. ä¸åŒé¢„å¤„ç†æµç¨‹çš„å¯¹æ¯”ã€‚å¯ä»¥æ˜æ˜¾çœ‹å‡ºï¼Œå½“å›¾åƒé•¿å®½æ¯”æ˜¾è‘—åç¦»1æ—¶ï¼Œæ ‡å‡†CLIPé¢„å¤„ç†æµç¨‹ä¼šç§»é™¤å›¾åƒçš„å¤§éƒ¨åˆ†åŒºåŸŸï¼Œè¿™å°†æ˜¾è‘—é˜»ç¢æ£€ç´¢è¿‡ç¨‹ã€‚å°½ç®¡æ­£æ–¹å½¢å¡«å……ä¸å»ºè®®å¡«å……ï¼ˆç›®æ ‡æ¯”ä¾‹ä¸º1.25ï¼‰æ–¹æ³•äº§ç”Ÿçš„è§†è§‰å·®å¼‚å¹¶ä¸æ˜¾è‘—ï¼Œä½†æˆ‘ä»¬å°†è¯æ˜æ¨¡å‹åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­ä¼šå› å›¾åƒå¯ç”¨åŒºåŸŸçš„å¢åŠ è€Œå—ç›Šã€‚

# 4 EXPERIMENTAL RESULTS  

## 4.1 Implementation details  

We perform the experiments using two CLIP models of different sizes. The smallest one relies on a modified ResNet50 (RN-50) [22] architecture. It takes input images of $224\times224$ , and the size of its embedding space is $d=1024$ . The biggest one, denoted as $\mathrm{RN}{-}50\mathrm{x}4$ , follows the EfficientNet-style model scaling and uses approximately $4\times$ the computation of RN-50. It takes input images of $288\times288$ , and the size of its embedding space is $d=640$ .  
æˆ‘ä»¬ä½¿ç”¨ä¸¤ç§ä¸åŒè§„æ¨¡çš„CLIPæ¨¡å‹è¿›è¡Œå®éªŒã€‚è¾ƒå°æ¨¡å‹åŸºäºæ”¹è¿›çš„ResNet50ï¼ˆRN-50ï¼‰[22]æ¶æ„ï¼Œæ¥æ”¶$224\times224$å°ºå¯¸çš„è¾“å…¥å›¾åƒï¼Œå…¶åµŒå…¥ç©ºé—´ç»´åº¦ä¸º$d=1024$ã€‚è¾ƒå¤§æ¨¡å‹è®°ä¸º$\mathrm{RN}{-}50\mathrm{x}4$ï¼Œé‡‡ç”¨EfficientNeté£æ ¼çš„æ¨¡å‹ç¼©æ”¾æ–¹æ³•ï¼Œè®¡ç®—é‡çº¦ä¸ºRN-50çš„4å€ã€‚è¯¥æ¨¡å‹æ¥æ”¶$288\times288$å°ºå¯¸çš„è¾“å…¥å›¾åƒï¼Œå…¶åµŒå…¥ç©ºé—´ç»´åº¦ä¸º$d=640$ã€‚

# in_image : input image to be preprocessed   
# target_ratio : target aspect ratio   
# dim: CLIP image encoder input dimension   
# in_image : å¾…é¢„å¤„ç†çš„è¾“å…¥å›¾åƒ
# target_ratio : ç›®æ ‡çºµæ¨ªæ¯” 
# dim: CLIPå›¾åƒç¼–ç å™¨è¾“å…¥ç»´åº¦
def preprocess ( in_image , target_ratio , dim ):
    w, h $=$ in_image.size
    aspect_ratio $=$ max(w, h) / min(w, h)
    # ä»…å½“çºµæ¨ªæ¯”è¶…è¿‡ç›®æ ‡å€¼æ—¶è¿›è¡Œå¡«å……
    if aspect_ratio $<$ target_ratio :
        out_image $=$ in_image
    else :
        # é›¶å€¼å¡«å……ä½¿å›¾åƒçºµæ¨ªæ¯”åŒ¹é…ç›®æ ‡å€¼
        scaled_max_wh $=$ max(w, h) / target_ratio
        hp $=$ max((scaled_max_wh - w) // 2, 0)
        vp $=$ max((scaled_max_wh - h) // 2, 0)
        padding $=$ (hp, vp, hp, vp)
        out_image $=$ pad(in_image, padding, 0)
    # è°ƒæ•´å°ºå¯¸å¹¶è¿›è¡Œä¸­å¿ƒè£å‰ª
    out_image $=$ resize(out_image, dim)
    out_image $=$ center_crop(out_image, dim)
    return out_image

Algorithm 1: Python-style pseudocode of the proposed preprocess pipeline.  
ç®—æ³•1: åŸºäºPythoné£æ ¼çš„é¢„å¤„ç†æµç¨‹ä¼ªä»£ç ã€‚ 

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/182bb20c593f18854c475729ca748efda767763b17879b555bf0119b82e05746.jpg)  
Fig. 6. Comparison among different preprocesses pipelines. The proposed padding method results in images that contain more details than square padding and provides a better overview than the standard CLIP padding.  
å›¾6. ä¸åŒé¢„å¤„ç†æµç¨‹çš„å¯¹æ¯”ã€‚å»ºè®®çš„å¡«å……æ–¹æ³•ç›¸è¾ƒäºæ­£æ–¹å½¢å¡«å……èƒ½ä¿ç•™æ›´å¤šå›¾åƒç»†èŠ‚ï¼Œç›¸æ¯”æ ‡å‡†CLIPå¡«å……æä¾›æ›´ä¼˜çš„å…¨å±€è§†è§’ã€‚

In the Combiner network (Figure 4), the first two linear layers before the concatenation have input-output dimensionality equal to $(d,4d)$ . After the concatenation in both branches, we have two linear layers. In the first branch (1), the first linear layer has input-output dimensionality of $(4d,8d)$ , and the second one (8ğ‘‘, 1). In the other branch, the first linear layer has input-output dimensionality of $(4d,8d)$ while the second one $(8d,d)$ . Following the standard practice, we set the dropout rate to 0.5. During retrieval, we normalize both the combined and index set features to have a unit $L_{2}$ -norm.  
åœ¨Combinerç½‘ç»œï¼ˆå›¾4ï¼‰ä¸­ï¼Œæ‹¼æ¥æ“ä½œå‰çš„ä¸¤ä¸ªçº¿æ€§å±‚å…·æœ‰$(d,4d)$çš„è¾“å…¥-è¾“å‡ºç»´åº¦ã€‚ä¸¤ä¸ªåˆ†æ”¯åœ¨æ‹¼æ¥åå„åŒ…å«ä¸¤ä¸ªçº¿æ€§å±‚ï¼šåœ¨ç¬¬ä¸€ä¸ªåˆ†æ”¯(1)ä¸­ï¼Œç¬¬ä¸€ä¸ªçº¿æ€§å±‚çš„è¾“å…¥-è¾“å‡ºç»´åº¦ä¸º$(4d,8d)$ï¼Œç¬¬äºŒä¸ªå±‚ä¸º$(8d,1)$ï¼›åœ¨å¦ä¸€ä¸ªåˆ†æ”¯ä¸­ï¼Œç¬¬ä¸€ä¸ªçº¿æ€§å±‚ç»´åº¦ä¸º$(4d,8d)$ï¼Œç¬¬äºŒä¸ªå±‚ä¸º$(8d,d)$ã€‚éµå¾ªå¸¸è§„åšæ³•ï¼Œæˆ‘ä»¬å°†ä¸¢å¼ƒç‡è®¾ç½®ä¸º0.5ã€‚åœ¨æ£€ç´¢é˜¶æ®µï¼Œæˆ‘ä»¬å¯¹ç»„åˆç‰¹å¾å’Œç´¢å¼•é›†ç‰¹å¾è¿›è¡Œå•ä½$L_{2}$èŒƒæ•°å½’ä¸€åŒ–ã€‚

Following the original CLIP training strategy, in the fine-tuning stage, we employed AdamW optimizer [38] with a learning rate of $2e-6$ and a weight decay coefficient of $1e-2$ . Due to GPU memory constraints, we set the batch size to 512 for fine-tuning the RN-50-based CLIP model and 192 for fine-tuning the RN-50x4-based model. We kept the batch normalization layer frozen. We fine-tuned the CLIP encoders for a maximum of 150 epochs. During the training of the Combiner network, we keep both fine-tuned CLIP encoders frozen and only train the Combiner function. We set the learning rate to $2e-5$ and train the model for a maximum of 300 epochs. We set the batch size to 4096 when using both backbones. We used the PyTorch library throughout the experiments. We set the target ratio in the preprocessing pipeline to 1.25. Following the approach described in [41], we set the parameter $\tau$ in Eq. (1) to 100. This value ensures that the logits have a sufficient dynamic range. To mitigate overfitting, we adopt an early stopping strategy. We use mixed-precision training [39] to save memory and speed up the training in both stages. We employ gradient checkpointing [7] to further reduce memory usage.  
éµå¾ªåŸå§‹CLIPè®­ç»ƒç­–ç•¥ï¼Œåœ¨å¾®è°ƒé˜¶æ®µæˆ‘ä»¬é‡‡ç”¨AdamWä¼˜åŒ–å™¨[38]ï¼Œå­¦ä¹ ç‡è®¾ç½®ä¸º$2e-6$ï¼Œæƒé‡è¡°å‡ç³»æ•°ä¸º$1e-2$ã€‚å—é™äºGPUæ˜¾å­˜ï¼ŒåŸºäºRN-50çš„CLIPæ¨¡å‹å¾®è°ƒæ‰¹é‡å¤§å°è®¾ä¸º512ï¼ŒRN-50x4æ¨¡å‹è®¾ä¸º192ã€‚æˆ‘ä»¬ä¿æŒæ‰¹é‡å½’ä¸€åŒ–å±‚å†»ç»“çŠ¶æ€ï¼ŒCLIPç¼–ç å™¨æœ€å¤§å¾®è°ƒ150ä¸ªè®­ç»ƒå‘¨æœŸã€‚åœ¨Combinerç½‘ç»œè®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬å†»ç»“ä¸¤ä¸ªå·²å¾®è°ƒçš„CLIPç¼–ç å™¨ï¼Œä»…è®­ç»ƒCombinerå‡½æ•°ã€‚å­¦ä¹ ç‡è®¾ä¸º$2e-5$ï¼Œæœ€å¤§è®­ç»ƒ300ä¸ªå‘¨æœŸã€‚å½“ä½¿ç”¨åŒéª¨å¹²ç½‘ç»œæ—¶ï¼Œæ‰¹é‡å¤§å°è®¾ä¸º4096ã€‚æ•´ä¸ªå®éªŒåŸºäºPyTorchåº“å®ç°ï¼Œé¢„å¤„ç†æµç¨‹ä¸­ç›®æ ‡æ¯”ä¾‹è®¾ä¸º1.25ã€‚æ ¹æ®[41]çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å°†å…¬å¼(1)ä¸­çš„å‚æ•°$\tau$è®¾ä¸º100ï¼Œè¯¥å€¼ç¡®ä¿é€»è¾‘å€¼å…·æœ‰è¶³å¤Ÿçš„åŠ¨æ€èŒƒå›´ã€‚ä¸ºç¼“è§£è¿‡æ‹Ÿåˆï¼Œé‡‡ç”¨æ—©åœç­–ç•¥ã€‚åœ¨åŒé˜¶æ®µè®­ç»ƒä¸­å‡ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ[39]æ¥èŠ‚çœå†…å­˜å¹¶åŠ é€Ÿè®­ç»ƒï¼ŒåŒæ—¶åº”ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯[7]è¿›ä¸€æ­¥é™ä½æ˜¾å­˜æ¶ˆè€—ã€‚

We conduct all experiments on a single NVIDIA Titan RTX (24GB) GPU. The first stage of training requires approximately 4 hours for the RN-50x4 model and 2 hours for the RN-50 model. The training of the Combiner network takes less than an hour for both models.  
æ‰€æœ‰å®éªŒå‡åœ¨å•å—NVIDIA Titan RTXï¼ˆ24GBï¼‰GPUä¸Šè¿›è¡Œã€‚RN-50x4æ¨¡å‹çš„ç¬¬ä¸€é˜¶æ®µè®­ç»ƒè€—æ—¶çº¦4å°æ—¶ï¼ŒRN-50æ¨¡å‹çº¦2å°æ—¶ã€‚Combinerç½‘ç»œè®­ç»ƒå¯¹ä¸¤ç§æ¨¡å‹å‡å¯åœ¨1å°æ—¶å†…å®Œæˆã€‚

## 4.2 Datasets and metrics  

4.2.1 FashionIQ. FashionIQ [51] is composed of 77,684 fashion images crawled from the web and split into the train, validation, and test sets, divided into three different categories: Dress, Toptee and Shirt. Among the 46,609 training images, there are 18,000 training triplets made of a reference image, a pair of relative captions, and a target image. The captions describe properties to modify in the reference image to match the target image. The validation and test sets consist of 15,537 and 15,538 images, respectively, with 6,017 and 6,119 triplets.  
4.2.1 FashionIQ. FashionIQ [51]æ•°æ®é›†ç”±77,684å¼ ç½‘ç»œçˆ¬å–çš„æ—¶å°šå›¾åƒæ„æˆï¼Œåˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼ŒåŒ…å«Dressï¼ˆè¿è¡£è£™ï¼‰ã€Topteeï¼ˆçŸ­è¢–ä¸Šè¡£ï¼‰å’ŒShirtï¼ˆè¡¬è¡«ï¼‰ä¸‰ä¸ªå­ç±»ã€‚å…¶ä¸­46,609å¼ è®­ç»ƒå›¾åƒåŒ…å«18,000ä¸ªè®­ç»ƒä¸‰å…ƒç»„ï¼Œæ¯ä¸ªä¸‰å…ƒç»„ç”±å‚è€ƒå›¾åƒã€ä¸€å¯¹å…³è”æ–‡æœ¬æè¿°å’Œç›®æ ‡å›¾åƒç»„æˆã€‚æ–‡æœ¬æè¿°æŒ‡å¯¼å¦‚ä½•ä¿®æ”¹å‚è€ƒå›¾åƒä»¥åŒ¹é…ç›®æ ‡å›¾åƒã€‚éªŒè¯é›†å’Œæµ‹è¯•é›†åˆ†åˆ«åŒ…å«15,537å’Œ15,538å¼ å›¾åƒï¼Œå¯¹åº”6,017å’Œ6,119ä¸ªä¸‰å…ƒç»„ã€‚

We follow the standard experimental setting as in [29, 31]. We employ the average recall at rank K (Recall $\boldsymbol{@}\mathrm{K})$ as an evaluation metric, namely Recall@10 $(\mathrm{R}@10)$ and Recall $\ @50$ $(\mathrm{R}@50)$ . Note that for each triplet, there is only a positive index image. Hence, each query has ${\mathrm{R@K}}$ zero or one. All results are on the validation set since, at the time of writing, test set ground-truth labels have not been released yet.  
æˆ‘ä»¬éµå¾ª[29,31]çš„æ ‡å‡†å®éªŒè®¾ç½®ï¼Œé‡‡ç”¨å¹³å‡å¬å›ç‡@Kï¼ˆRecall $\boldsymbol{@}\mathrm{K}$ï¼‰ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œå³R@10ï¼ˆRecall@10ï¼‰å’ŒR@50ï¼ˆRecall@50ï¼‰ã€‚éœ€æ³¨æ„æ¯ä¸ªä¸‰å…ƒç»„ä»…åŒ…å«ä¸€ä¸ªæ­£æ ·æœ¬ç´¢å¼•å›¾åƒï¼Œå› æ­¤æ¯ä¸ªæŸ¥è¯¢çš„${\mathrm{R@K}}$å–å€¼ä¸º0æˆ–1ã€‚ç”±äºæµ‹è¯•é›†æ ‡æ³¨å°šæœªå…¬å¼€ï¼Œæ‰€æœ‰ç»“æœå‡åŸºäºéªŒè¯é›†å¾—å‡ºã€‚

4.2.2 CIRR. The authors of [37] designed the CIRR dataset to address two common problems encountered in composed image retrieval datasets, such as FashionIQ. These problems are the lack of sufficient visual complexity caused by the restricted image domain and the numerous false negatives due to the unfeasibility of extensively labeling target images for each (reference, text) pair. As a result, some images in the dataset that correspond to valid matches for a query are not labeled as valid targets. CIRR (Compose Image Retrieval on Real-life images) dataset consists of 21,552 real-life images taken from the popular natural language reasoning $N L V R^{2}$ dataset [47]. It has the same structure as the FashionIQ dataset and contains 36,554 triplets randomly assigned in $80\%$ for training, $10\%$ for validation, and $10\%$ for the test. The dataset images are grouped in multiple subsets of six semantically and visually similar images. To have negative images with high visual similarity the relative captions are collected describing the differences between two images in the same subset.  
4.2.2 CIRR. æ–‡çŒ®[37]æå‡ºçš„CIRRæ•°æ®é›†æ—¨åœ¨è§£å†³ç»„åˆå›¾åƒæ£€ç´¢æ•°æ®é›†ï¼ˆå¦‚FashionIQï¼‰ä¸­å¸¸è§çš„ä¸¤ä¸ªé—®é¢˜ï¼šå› å›¾åƒé¢†åŸŸå—é™å¯¼è‡´çš„è§†è§‰å¤æ‚æ€§ä¸è¶³ï¼Œä»¥åŠéš¾ä»¥å¯¹æ¯ä¸ªï¼ˆå‚è€ƒå›¾åƒï¼Œæ–‡æœ¬ï¼‰å¯¹è¿›è¡Œå¤§è§„æ¨¡ç›®æ ‡å›¾åƒæ ‡æ³¨è€Œäº§ç”Ÿçš„å‡é˜´æ€§æ ·æœ¬é—®é¢˜ã€‚è¯¥æ•°æ®é›†åŒ…å«21,552å¼ æ¥è‡ªè‡ªç„¶è¯­è¨€æ¨ç†æ•°æ®é›†$NLVR^{2}$[47]çš„çœŸå®åœºæ™¯å›¾åƒï¼Œéµå¾ªä¸FashionIQç›¸åŒçš„ä¸‰å…ƒç»„ç»“æ„ï¼Œå…±å«36,554ä¸ªä¸‰å…ƒç»„ï¼ŒæŒ‰80%è®­ç»ƒã€10%éªŒè¯ã€10%æµ‹è¯•çš„æ¯”ä¾‹éšæœºåˆ’åˆ†ã€‚æ•°æ®é›†å›¾åƒè¢«ç»„ç»‡ä¸ºå¤šä¸ªåŒ…å«å…­å¼ è¯­ä¹‰å’Œè§†è§‰ç›¸ä¼¼å›¾åƒçš„å­é›†ï¼Œé€šè¿‡æ”¶é›†æè¿°åŒä¸€å­é›†å†…ä¸¤å›¾åƒå·®å¼‚çš„ç›¸å¯¹æ–‡æœ¬æ¥æ„å»ºé«˜è§†è§‰ç›¸ä¼¼åº¦çš„è´Ÿæ ·æœ¬ã€‚

The standard evaluation protocol proposed by the authors of the dataset is to report the recall at rank K (Recall@K) at four different ranks (1, 5, 10, 50). Moreover, thanks to the unique design of the CIRR dataset, it is also possible to report the RecallSubset metric that considers only the images in the subset of the query. This subset metric has two main benefits: it is not affected by false-negative samples and, thanks to negative samples with high visual similarity, it captures fine-grained image-text modifications. The reference metrics are the $\mathrm{R}@5$ which accounts for possible false negatives in the entire corpus, and the $\mathrm{R}_{\mathrm{Subset}}@1$ , which better illustrates the fine-grained reasoning abilities.  
è¯¥æ•°æ®é›†çš„æ ‡å‡†è¯„ä¼°åè®®è¦æ±‚æŠ¥å‘Šå››ä¸ªä¸åŒæ’åKï¼ˆ1ã€5ã€10ã€50ï¼‰çš„Recall@KæŒ‡æ ‡ã€‚å¾—ç›Šäºç‹¬ç‰¹çš„æ•°æ®é›†è®¾è®¡ï¼Œè¿˜å¯è®¡ç®—ä»…è€ƒè™‘æŸ¥è¯¢å­é›†å›¾åƒçš„RecallSubsetæŒ‡æ ‡ã€‚è¯¥å­é›†æŒ‡æ ‡å…·æœ‰åŒé‡ä¼˜åŠ¿ï¼šä¸å—å‡é˜´æ€§æ ·æœ¬å½±å“ï¼Œä¸”é€šè¿‡é«˜ç›¸ä¼¼åº¦è´Ÿæ ·æœ¬æ•æ‰ç»†ç²’åº¦å›¾æ–‡ä¿®æ”¹ã€‚æ ¸å¿ƒå‚è€ƒæŒ‡æ ‡ä¸ºè€ƒè™‘å…¨åº“å‡é˜´æ€§çš„$\mathrm{R}@5$ï¼Œä»¥åŠæ›´å¥½ä½“ç°ç»†ç²’åº¦æ¨ç†èƒ½åŠ›çš„$\mathrm{R}_{\mathrm{Subset}}@1$ã€‚

### 4.3 Task-oriented fine-tuning effects  

In this section, we present a set of experiments that illustrate how the task-oriented fine-tuning of CLIP encoders and their increased additivity properties contribute to easing the task of the Combiner network and help to improve retrieval performance. For each dataset, we compare the performance varying the combining function and the modality of the CLIP fine-tuning. Throughout all the experiments, we use the RN-50 CLIP model. For each fine-tuning modality, we train from scratch a different Combiner network. We report the results in Table 1 for the FashionIQ dataset and in Table 2 for the CIRR dataset.  
æœ¬èŠ‚é€šè¿‡ç³»åˆ—å®éªŒé˜æ˜CLIPç¼–ç å™¨çš„ä»»åŠ¡å¯¼å‘å¾®è°ƒåŠå…¶å¢å¼ºçš„å¯åŠ æ€§ç‰¹æ€§å¦‚ä½•ç®€åŒ–Combinerç½‘ç»œçš„ä»»åŠ¡ï¼Œå¹¶æœ‰åŠ©äºæå‡æ£€ç´¢æ€§èƒ½ã€‚é’ˆå¯¹å„æ•°æ®é›†ï¼Œæˆ‘ä»¬é€šè¿‡æ”¹å˜ç»„åˆå‡½æ•°å’ŒCLIPå¾®è°ƒæ¨¡å¼æ¥æ¯”è¾ƒæ€§èƒ½è¡¨ç°ã€‚åœ¨å…¨éƒ¨å®éªŒä¸­å‡ä½¿ç”¨RN-50 CLIPæ¨¡å‹ï¼Œé’ˆå¯¹æ¯ç§å¾®è°ƒæ¨¡å¼ä»å¤´å¼€å§‹è®­ç»ƒä¸åŒçš„Combinerç½‘ç»œã€‚åœ¨FashionIQæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè®°å½•äºè¡¨1ï¼ŒCIRRæ•°æ®é›†çš„ç»“æœè§Table 2ã€‚

Notably, the element-wise sum of out-of-the-box CLIP features achieves impressive results without domain or task-specific training on both datasets. This performance is intriguing as it demonstrates that the CLIP imagetext common embedding space exhibits good additivity properties, even though its training objective does not explicitly optimize for this aspect. Fine-tuning only the CLIP image encoder brings an interesting performance boost compared to the out-of-the-box CLIP features. This improvement is expected when employing the elementwise sum as the combining function, given that the out-of-the-box CLIP features lack domain or task-specific training. However, the most promising improvement occurs when utilizing the trained Combiner network. The text encoder fine-tuning achieves slightly better performance than image encoder fine-tuning. We can notice that on the FashionIQ dataset, the improvement over the image encoder fine-tuning remains constant when using either the element-wise sum or the Combiner network as a combining function. However, on the CIRR dataset, the situation differs. When comparing with the performance of the image encoder fine-tuning, using the element-wise sum to combine the query features results in comparable global metrics, but significantly improved fine-grained subset metrics. In contrast, when utilizing the Combiner network, we observe a reduction in the gaps within the subset metrics, while achieving a greater improvement in the global metrics. We achieve the best results on both datasets when we fine-tune both encoders. The element-wise sum of the fine-tuned features outperforms the performance of the out-of-the-box features combined with the trained Combiner network by a significant margin. Moreover, when we combine the query features with the Combiner network, the performances further improve. It is worth highlighting that when utilizing the Combiner as a combining function, the improvement achieved by fine-tuning both encoders over the out-of-the-box CLIP features is the arithmetic sum of the improvements obtained by fine-tuning either the image or the text encoder.  
å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæœªç»é¢†åŸŸæˆ–ä»»åŠ¡ç‰¹å®šè®­ç»ƒçš„CLIPç‰¹å¾å…ƒç´ æ±‚å’Œï¼Œåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå‡å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚è¿™ä¸€è¡¨ç°å¼•äººæ³¨ç›®ï¼Œå› ä¸ºå®ƒè¡¨æ˜CLIPå›¾åƒ-æ–‡æœ¬å…±äº«åµŒå…¥ç©ºé—´å…·æœ‰è‰¯å¥½çš„å¯åŠ æ€§ï¼Œå°½ç®¡å…¶è®­ç»ƒç›®æ ‡å¹¶æœªæ˜ç¡®ä¼˜åŒ–è¿™ä¸€æ–¹é¢ã€‚ä¸æœªè°ƒæ•´çš„CLIPç‰¹å¾ç›¸æ¯”ï¼Œä»…å¾®è°ƒCLIPå›¾åƒç¼–ç å™¨å¸¦æ¥äº†æœ‰è¶£çš„æ€§èƒ½æå‡ã€‚å½“é‡‡ç”¨å…ƒç´ æ±‚å’Œä½œä¸ºç»„åˆå‡½æ•°æ—¶ï¼Œè¿™ä¸€æå‡æ˜¯å¯ä»¥é¢„è§çš„ï¼Œå› ä¸ºæœªç»è°ƒæ•´çš„CLIPç‰¹å¾ç¼ºä¹é¢†åŸŸæˆ–ä»»åŠ¡ç‰¹å®šçš„è®­ç»ƒã€‚ç„¶è€Œï¼Œæœ€æ˜¾è‘—çš„æ”¹è¿›å‡ºç°åœ¨ä½¿ç”¨è®­ç»ƒè¿‡çš„Combinerç½‘ç»œæ—¶ã€‚æ–‡æœ¬ç¼–ç å™¨çš„å¾®è°ƒæ¯”å›¾åƒç¼–ç å™¨çš„å¾®è°ƒç•¥èƒœä¸€ç­¹ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œåœ¨FashionIQæ•°æ®é›†ä¸Šï¼Œæ— è®ºæ˜¯ä½¿ç”¨å…ƒç´ æ±‚å’Œè¿˜æ˜¯Combinerç½‘ç»œä½œä¸ºç»„åˆå‡½æ•°ï¼Œç›¸å¯¹äºå›¾åƒç¼–ç å™¨å¾®è°ƒçš„æ”¹è¿›ä¿æŒç¨³å®šã€‚ä½†åœ¨CIRRæ•°æ®é›†ä¸Šï¼Œæƒ…å†µæœ‰æ‰€ä¸åŒã€‚ä¸å›¾åƒç¼–ç å™¨å¾®è°ƒçš„è¡¨ç°ç›¸æ¯”ï¼Œä½¿ç”¨å…ƒç´ æ±‚å’Œç»„åˆæŸ¥è¯¢ç‰¹å¾æ—¶ï¼Œå…¨å±€æŒ‡æ ‡ç›¸å½“ï¼Œä½†ç»†åˆ†å­é›†æŒ‡æ ‡æ˜¾è‘—æå‡ã€‚ç›¸åï¼Œä½¿ç”¨Combinerç½‘ç»œæ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å­é›†æŒ‡æ ‡é—´çš„å·®è·ç¼©å°ï¼ŒåŒæ—¶å…¨å±€æŒ‡æ ‡è·å¾—æ›´å¤§æå‡ã€‚å½“åŒæ—¶å¾®è°ƒä¸¤ä¸ªç¼–ç å™¨æ—¶ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šéƒ½è¾¾åˆ°äº†æœ€ä½³ç»“æœã€‚å¾®è°ƒåç‰¹å¾çš„å…ƒç´ æ±‚å’Œè¡¨ç°è¿œè¶…æœªè°ƒæ•´ç‰¹å¾ä¸è®­ç»ƒè¿‡çš„Combinerç½‘ç»œç»“åˆçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œå½“ä½¿ç”¨Combinerç½‘ç»œç»„åˆæŸ¥è¯¢ç‰¹å¾æ—¶ï¼Œæ€§èƒ½è¿›ä¸€æ­¥æå‡ã€‚å€¼å¾—å¼ºè°ƒçš„æ˜¯ï¼Œä½¿ç”¨Combinerä½œä¸ºç»„åˆå‡½æ•°æ—¶ï¼ŒåŒæ—¶å¾®è°ƒä¸¤ä¸ªç¼–ç å™¨ç›¸å¯¹äºæœªè°ƒæ•´CLIPç‰¹å¾çš„æ”¹è¿›ï¼Œæ˜¯å•ç‹¬å¾®è°ƒå›¾åƒæˆ–æ–‡æœ¬ç¼–ç å™¨æ‰€è·æ”¹è¿›çš„ç®—æœ¯å’Œã€‚

Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features â€¢ 11   


<html><body><table><tr><td colspan="3"></td><td colspan="2">Shirt</td><td colspan="2">Dress</td><td colspan="2">Toptee</td><td colspan="2">Average</td></tr><tr><td>CF</td><td>IFT</td><td>TFT</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td></tr><tr><td rowspan="4">Sum</td><td>Ã—</td><td>Ã—</td><td>19.53</td><td>35.57</td><td>17.70</td><td>36.29</td><td>21.88</td><td>42.93</td><td>19.70</td><td>38.26</td></tr><tr><td></td><td>Ã—</td><td>30.08</td><td>52.94</td><td>29.10</td><td>52.01</td><td>34.42</td><td>57.62</td><td>31.20</td><td>54.19</td></tr><tr><td>Ã—</td><td></td><td>32.29</td><td>53.73</td><td>27.76</td><td>52.31</td><td>35.14</td><td>60.12</td><td>31.73</td><td>55.39</td></tr><tr><td></td><td>âˆš</td><td>38.67</td><td>59.42</td><td>35.99</td><td>62.22</td><td>43.35</td><td>67.52</td><td>39.34</td><td>63.05</td></tr><tr><td rowspan="4">Combiner</td><td>Ã—</td><td>Ã—</td><td>31.85</td><td>52.50</td><td>27.22</td><td>50.62</td><td>33.81</td><td>57.57</td><td>30.96</td><td>53.56</td></tr><tr><td></td><td></td><td>34.30</td><td>55.79</td><td>32.47</td><td>55.18</td><td>38.45</td><td>62.36</td><td>35.07</td><td>57.78</td></tr><tr><td>Ã—</td><td>â†</td><td>35.87</td><td>57.21</td><td>31.43</td><td>54.98</td><td>38.20</td><td>63.22</td><td>35.16</td><td>58.47</td></tr><tr><td></td><td></td><td>39.87</td><td>60.84</td><td>37.67</td><td>63.16</td><td>44.88</td><td>68.59</td><td>40.80</td><td>64.20</td></tr></table></body></html>  

Table 1. Recall at K on the FashionIQ validation set while varying the combining function and the modality of CLIP finetuning. We denote IFT (image encoder fine-tuning) and TFT (text encoder fine-tuning) to represent whether the image encoder or the text encoder is fine-tuned in the first stage. CF (combining function) indicates the function used to combine the query features. We highlight the best scores in bold and underline the second-best scores.   
è¡¨1. åœ¨FashionIQéªŒè¯é›†ä¸Šï¼Œä¸åŒç»„åˆå‡½æ•°å’ŒCLIPå¾®è°ƒæ¨¡æ€ä¸‹çš„Kå€¼å¬å›ç‡ã€‚æˆ‘ä»¬ä½¿ç”¨IFTï¼ˆå›¾åƒç¼–ç å™¨å¾®è°ƒï¼‰å’ŒTFTï¼ˆæ–‡æœ¬ç¼–ç å™¨å¾®è°ƒï¼‰æ¥è¡¨ç¤ºåœ¨ç¬¬ä¸€é˜¶æ®µæ˜¯å¦å¯¹å›¾åƒç¼–ç å™¨æˆ–æ–‡æœ¬ç¼–ç å™¨è¿›è¡Œäº†å¾®è°ƒã€‚CFï¼ˆç»„åˆå‡½æ•°ï¼‰æŒ‡çš„æ˜¯ç”¨äºç»„åˆæŸ¥è¯¢ç‰¹å¾çš„å‡½æ•°ã€‚æœ€ä½³å¾—åˆ†ä»¥ç²—ä½“æ ‡æ³¨ï¼Œæ¬¡ä½³å¾—åˆ†åˆ™ä»¥ä¸‹åˆ’çº¿æ ‡å‡ºã€‚


<html><body><table><tr><td rowspan="2"></td><td></td><td></td><td colspan="4">Recall@K</td><td colspan="3">Rsubset @K</td></tr><tr><td>IFT</td><td>TFT</td><td>Kï¼š äºŒ</td><td>K = 5</td><td>Kï¼š = 10</td><td>Kï¼š äºŒ 50</td><td>K 1</td><td>K = ï¼š2</td><td>Kï¼š =3</td></tr><tr><td rowspan="4">Sum</td><td></td><td>Ã—</td><td>21.38</td><td>50.85</td><td>64.00</td><td>87.23</td><td>54.48</td><td>76.01</td><td>87.16</td></tr><tr><td></td><td>Ã—</td><td>31.67</td><td>66.08</td><td>79.36</td><td>95.38</td><td>58.12</td><td>78.42</td><td>89.78</td></tr><tr><td></td><td></td><td>32.72</td><td>66.63</td><td>79.22</td><td>94.86</td><td>67.21</td><td>86.00</td><td>93.81</td></tr><tr><td>âˆš</td><td>âˆš</td><td>40.97</td><td>74.70</td><td>85.51</td><td>96.94</td><td>68.81</td><td>86.96</td><td>93.90</td></tr><tr><td rowspan="4">Combiner</td><td>Ã—</td><td></td><td>31.26</td><td>64.79</td><td>77.71</td><td>95.31</td><td>61.56</td><td>81.08</td><td>91.12</td></tr><tr><td></td><td>Ã—</td><td>34.01</td><td>69.07</td><td>81.77</td><td>95.72</td><td>62.78</td><td>81.80</td><td>91.41</td></tr><tr><td>Ã—</td><td></td><td>36.86</td><td>71.32</td><td>82.32</td><td>96.24</td><td>68.28</td><td>86.51</td><td>94.14</td></tr><tr><td><</td><td></td><td>42.05</td><td>76.13</td><td>86.51</td><td>97.49</td><td>70.15</td><td>87.18</td><td>94.40</td></tr></table></body></html>

Table 2. Recall at K on the CIRR validation set while varying the combining function and the modality of CLIP fine-tuning. We denote IFT (image encoder fine-tuning) and TFT (text encoder fine-tuning) to represent whether the image encoder or the text encoder is fine-tuned in the first stage. CF (combining function) indicates the function used to combine the query features. We highlight the best scores in bold and underline the second-best scores.  
è¡¨2å±•ç¤ºäº†åœ¨CIRRéªŒè¯é›†ä¸Šï¼Œé€šè¿‡æ”¹å˜ç»“åˆå‡½æ•°å’ŒCLIPå¾®è°ƒæ¨¡æ€ï¼ŒKå€¼å¬å›ç‡çš„å˜åŒ–æƒ…å†µã€‚æˆ‘ä»¬ç”¨IFTï¼ˆå›¾åƒç¼–ç å™¨å¾®è°ƒï¼‰å’ŒTFTï¼ˆæ–‡æœ¬ç¼–ç å™¨å¾®è°ƒï¼‰æ¥è¡¨ç¤ºç¬¬ä¸€é˜¶æ®µæ˜¯å¦å¯¹å›¾åƒç¼–ç å™¨æˆ–æ–‡æœ¬ç¼–ç å™¨è¿›è¡Œäº†å¾®è°ƒã€‚CFï¼ˆç»“åˆå‡½æ•°ï¼‰æŒ‡çš„æ˜¯ç”¨äºç»„åˆæŸ¥è¯¢ç‰¹å¾çš„åŠŸèƒ½ã€‚æˆ‘ä»¬å°†æœ€ä½³å¾—åˆ†åŠ ç²—æ˜¾ç¤ºï¼Œå¹¶å°†æ¬¡ä½³å¾—åˆ†åŠ ä¸‹åˆ’çº¿ã€‚

Given this last observation and all the other results, we formulate the hypothesis that the fine-tuning of the image and the text encoder learn different and complementary information that improves performances differently. We conjecture that the fine-tuning of the image encoder adapts the image manifold to the domain of the data (e.g., the fashion domain for the FashionIQ dataset). On the contrary, the fine-tuning of the text-encoder adapts the text embedding space to the task of composed image retrieval by transforming textual features into displacement vectors within the image embedding space. In support of this conjecture, we highlight the difference in performances between the global metrics and subset metrics on the CIRR dataset when comparing the image and the text encoder fine-tuning using the element-wise sum as a combining function (second and third row in Table 2). We note that in the global metrics, where the domain of the images is diverse, the performance differences between the two experiments approach zero. Conversely, in the subset metrics, where the visual differences among the images are low, the image fine-tuning is not capable of capturing the fine-grained differences making the textual information more discriminative and thus making the fine-tuning of the text encoder perform better. The experiments described in Section 4.8 provide additional confirmation of our intuition.  
åŸºäºè¿™ä¸€æœ€åè§‚å¯ŸåŠæ‰€æœ‰å…¶ä»–ç»“æœï¼Œæˆ‘ä»¬æå‡ºå‡è®¾ï¼šå›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨çš„å¾®è°ƒå­¦ä¹ åˆ°äº†ä¸åŒä¸”äº’è¡¥çš„ä¿¡æ¯ï¼Œä»è€Œä»¥ä¸åŒçš„æ–¹å¼æå‡äº†æ€§èƒ½ã€‚æˆ‘ä»¬æ¨æµ‹ï¼Œå›¾åƒç¼–ç å™¨çš„å¾®è°ƒä½¿å›¾åƒæµå½¢é€‚åº”æ•°æ®çš„é¢†åŸŸï¼ˆä¾‹å¦‚ï¼ŒFashionIQæ•°æ®é›†çš„æ—¶å°šé¢†åŸŸï¼‰ã€‚ç›¸åï¼Œæ–‡æœ¬ç¼–ç å™¨çš„å¾®è°ƒåˆ™é€šè¿‡å°†æ–‡æœ¬ç‰¹å¾è½¬æ¢ä¸ºå›¾åƒåµŒå…¥ç©ºé—´å†…çš„ä½ç§»å‘é‡ï¼Œä½¿æ–‡æœ¬åµŒå…¥ç©ºé—´é€‚åº”ç»„åˆå›¾åƒæ£€ç´¢çš„ä»»åŠ¡ã€‚ä¸ºæ”¯æŒè¿™ä¸€æ¨æµ‹ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†åœ¨CIRRæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨å…ƒç´ å’Œä½œä¸ºç»“åˆå‡½æ•°æ—¶ï¼Œå›¾åƒä¸æ–‡æœ¬ç¼–ç å™¨å¾®è°ƒåœ¨å…¨çƒæŒ‡æ ‡å’Œå­é›†æŒ‡æ ‡ä¸Šçš„æ€§èƒ½å·®å¼‚ï¼ˆè§è¡¨2ç¬¬äºŒå’Œç¬¬ä¸‰è¡Œï¼‰ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œåœ¨å…¨çƒæŒ‡æ ‡ä¸­ï¼Œå›¾åƒé¢†åŸŸå¤šæ ·åŒ–ï¼Œä¸¤é¡¹å®éªŒé—´çš„æ€§èƒ½å·®å¼‚è¶‹è¿‘äºé›¶ã€‚ç›¸åï¼Œåœ¨å­é›†æŒ‡æ ‡ä¸­ï¼Œå›¾åƒé—´è§†è§‰å·®å¼‚è¾ƒå°ï¼Œå›¾åƒå¾®è°ƒæ— æ³•æ•æ‰åˆ°ç»†ç²’åº¦çš„å·®å¼‚ï¼Œè¿™ä½¿å¾—æ–‡æœ¬ä¿¡æ¯æ›´å…·åŒºåˆ†æ€§ï¼Œä»è€Œè®©æ–‡æœ¬ç¼–ç å™¨çš„å¾®è°ƒè¡¨ç°æ›´ä½³ã€‚ç¬¬4.8èŠ‚æ‰€è¿°çš„å®éªŒè¿›ä¸€æ­¥è¯å®äº†æˆ‘ä»¬çš„ç›´è§‰ã€‚

<html><body><table><tr><td></td><td colspan="2">Shirt</td><td colspan="2">Dress</td><td colspan="2">Toptee</td><td colspan="2">Average</td></tr><tr><td>Model</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td></tr><tr><td>Element-wisesum</td><td>38.67</td><td>59.42</td><td>35.99</td><td>62.22</td><td>43.35</td><td>67.52</td><td>39.34</td><td>63.05</td></tr><tr><td>Convexcombination</td><td>39.45</td><td>60.16</td><td>36.44</td><td>62.57</td><td>44.05</td><td>67.87</td><td>39.98</td><td>63.53</td></tr><tr><td>W/oconvexcombination</td><td>31.40</td><td>55.64</td><td>35.94</td><td>61.03</td><td>40.29</td><td>64.97</td><td>35.87</td><td>60.55</td></tr><tr><td>Static skip</td><td>39.00</td><td>60.54</td><td>36.99</td><td>63.11</td><td>44.26</td><td>68.23</td><td>40.08</td><td>63.96</td></tr><tr><td>Proposed Combiner</td><td>39.87</td><td>60.84</td><td>37.67</td><td>63.16</td><td>44.88</td><td>68.59</td><td>40.80</td><td>64.20</td></tr></table></body></html>

Table 3. Recall at K on the FashionIQ validation set, with variations on the Combiner architecture. We highlight the best scores in bold and underline the second-best scores.  
è¡¨3. åœ¨FashionIQéªŒè¯é›†ä¸Šï¼Œé’ˆå¯¹ä¸åŒCombineræ¶æ„å˜ä½“çš„Kå¬å›ç‡ã€‚æœ€ä½³å¾—åˆ†ä»¥ç²—ä½“æ ‡å‡ºï¼Œæ¬¡ä½³å¾—åˆ†åˆ™åŠ ä¸‹åˆ’çº¿æ˜¾ç¤ºã€‚

<html><body><table><tr><td></td><td colspan="4">Recall@K</td><td colspan="4">Rsubset @K</td></tr><tr><td>Model</td><td>K=1</td><td>K=5</td><td>K =10</td><td>K=50</td><td>K=1</td><td></td><td>K=2</td><td>K=3</td></tr><tr><td>Element-wisesum</td><td>40.97</td><td>74.70</td><td>85.51</td><td>96.94</td><td>68.81</td><td></td><td>86.96</td><td>93.90</td></tr><tr><td>Convexcombination</td><td>41.11</td><td>75.56</td><td>85.55</td><td>97.44</td><td></td><td>70.46</td><td>87.08</td><td>94.33</td></tr><tr><td>W/oconvexcombination</td><td>36.98</td><td>72.06</td><td>82.83</td><td>96.67</td><td></td><td>65.53</td><td>84.74</td><td>93.06</td></tr><tr><td>Static skip</td><td>41.88</td><td>75.87</td><td>86.20</td><td>97.46</td><td></td><td>69.89</td><td>87.35</td><td>94.21</td></tr><tr><td>Proposed Combiner</td><td>42.05</td><td>76.13</td><td>86.51</td><td></td><td>97.49</td><td>70.15</td><td>87.18</td><td>94.40</td></tr></table></body></html>

Table 4. Recall at K on the CIRR validation set, with variations on the Combiner architecture. We highlight the best scores in bold and underline the second-best scores.  
è¡¨4. åœ¨CIRRéªŒè¯é›†ä¸ŠKå€¼å¬å›ç‡ï¼Œå±•ç¤ºäº†Combineræ¶æ„çš„ä¸åŒå˜ä½“ã€‚æˆ‘ä»¬ç”¨ç²—ä½“æ ‡å‡ºæœ€ä½³åˆ†æ•°ï¼Œå¹¶ç”¨ä¸‹åˆ’çº¿æ ‡å‡ºæ¬¡ä½³åˆ†æ•°ã€‚

### 4.4 Combiner ablation study  

In this section, we present a set of experiments with ablations and variations of the proposed Combiner network. We perform all the experiments using the fine-tuned RN-50 CLIP model. We train all the Combiner networks using a batch size of 4096 and a learning rate of $2e-5$ .  
åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ç³»åˆ—é’ˆå¯¹æ‰€æå‡ºçš„Combinerç½‘ç»œè¿›è¡Œçš„æ¶ˆèå®éªŒåŠå˜ä½“ç ”ç©¶ã€‚æ‰€æœ‰å®éªŒå‡é‡‡ç”¨å¾®è°ƒåçš„RN-50 CLIPæ¨¡å‹æ‰§è¡Œã€‚æˆ‘ä»¬è®­ç»ƒæ‰€æœ‰Combinerç½‘ç»œæ—¶ï¼Œä½¿ç”¨çš„æ‰¹é‡å¤§å°ä¸º4096ï¼Œå­¦ä¹ ç‡è®¾å®šä¸º$2e-5$ã€‚

Given the proposed Combiner network illustrated in Fig. 4, we denote the outputs of the first branch (1) as $\lambda$ and $1-\lambda$ , while the output of the second branch (2) as $v$ . The output features of the proposed Combiner are: $\overline{{{\phi_{q}}}}=\left(1-\lambda\right)*\overline{{{\psi_{I}}}}(I_{q})+\lambda*\overline{{{\psi_{T}}}}(T_{q})+v$ .  

To evaluate each component of the proposed design, we tested the following variations:  

â€¢ Element-wise sum: fine-tuned image and text features are summed: $\overline{{\phi_{q}}}=\overline{{\psi_{I}}}(I_{q})+\overline{{\psi_{T}}}(T_{q})$   
â€¢ Convex combination: only convex combination of image and text features, i.e. the model without the mixture contribution of text and image: $\overline{{\phi_{q}}}=(1-\lambda)*\overline{{\psi_{I}}}(I_{q})+\lambda*\overline{{\psi_{T}}}(T_{q})$   
â€¢ W/o convex combination: only the mixture contribution of text and image, i.e the model without the convex combination of text and image features: $\overline{{\phi_{q}}}=v$   
â€¢ Static skip: the convex coefficients are statically set to $0.5\colon\overline{{\phi_{q}}}=0.5*\overline{{\psi_{I}}}(I_{q})+0.5*\overline{{\psi_{T}}}(T_{q})+v$   
â€¢ Proposed Combiner: the Combiner architecture illustrated in Fig. 4.  
æ ¹æ®å›¾4æ‰€ç¤ºçš„Combinerç½‘ç»œè®¾è®¡ï¼Œæˆ‘ä»¬å°†ç¬¬ä¸€ä¸ªåˆ†æ”¯ï¼ˆ1ï¼‰çš„è¾“å‡ºè¡¨ç¤ºä¸º$\lambda$å’Œ$1-\lambda$ï¼Œè€Œç¬¬äºŒä¸ªåˆ†æ”¯ï¼ˆ2ï¼‰çš„è¾“å‡ºè¡¨ç¤ºä¸º$v$ã€‚æ‰€æå‡ºçš„Combinerçš„è¾“å‡ºç‰¹å¾ä¸ºï¼š$\overline{{{\phi_{q}}}}=(1-\lambda)*\overline{{{\psi_{I}}}}(I_{q})+\lambda*\overline{{{\psi_{T}}}}(T_{q})+v$ã€‚

ä¸ºäº†è¯„ä¼°è®¾è®¡ä¸­çš„æ¯ä¸ªç»„æˆéƒ¨åˆ†ï¼Œæˆ‘ä»¬æµ‹è¯•äº†ä»¥ä¸‹å˜ä½“ï¼š

- å…ƒç´ çº§æ±‚å’Œï¼šå¾®è°ƒåçš„å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ç›´æ¥ç›¸åŠ ï¼š$\overline{{\phi_{q}}}=\overline{{\psi_{I}}}(I_{q})+\overline{{\psi_{T}}}(T_{q})$
- å‡¸ç»„åˆï¼šä»…å¯¹å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾è¿›è¡Œå‡¸ç»„åˆï¼Œå³ä¸è€ƒè™‘æ–‡æœ¬å’Œå›¾åƒæ··åˆè´¡çŒ®çš„æ¨¡å‹ï¼š$\overline{{\phi_{q}}}=(1-\lambda)*\overline{{\psi_{I}}}(I_{q})+\lambda*\overline{{\psi_{T}}}(T_{q})$
- æ— å‡¸ç»„åˆï¼šä»…è€ƒè™‘æ–‡æœ¬å’Œå›¾åƒçš„æ··åˆè´¡çŒ®ï¼Œå³ä¸è€ƒè™‘å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾å‡¸ç»„åˆçš„æ¨¡å‹ï¼š$\overline{{\phi_{q}}}=v$
- é™æ€è·³è·ƒï¼šå‡¸ç³»æ•°é™æ€è®¾ç½®ä¸º0.5ï¼š$\overline{{\phi_{q}}}=0.5*\overline{{\psi_{I}}}(I_{q})+0.5*\overline{{\psi_{T}}}(T_{q}})+v$
- æå‡ºçš„Combinerï¼šå¦‚å›¾4æ‰€ç¤ºçš„Combineræ¶æ„ã€‚


<html><body><table><tr><td></td><td></td><td></td><td colspan="2">Shirt</td><td colspan="2">Dress</td><td colspan="2">Toptee</td><td colspan="2">Average</td></tr><tr><td>Approach</td><td>IFT</td><td>TFT</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td></tr><tr><td rowspan="3">End-to-end</td><td></td><td>x</td><td>31.79</td><td>53.14</td><td>30.29</td><td>53.49</td><td>33.55</td><td>59.15</td><td>31.87</td><td>55.26</td></tr><tr><td>Ã—</td><td></td><td>33.02</td><td>54.41</td><td>30.19</td><td>53.64</td><td>35.90</td><td>61.60</td><td>33.03</td><td>56.55</td></tr><tr><td></td><td></td><td>37.29</td><td>59.02</td><td>34.65</td><td>60.83</td><td>41.20</td><td>65.99</td><td>37.71</td><td>61.95</td></tr><tr><td rowspan="3">Two-stages</td><td></td><td>x</td><td>34.30</td><td>55.79</td><td>32.47</td><td>55.18</td><td>38.45</td><td>62.36</td><td>35.07</td><td>57.78</td></tr><tr><td>Ã—</td><td></td><td>35.87</td><td>57.21</td><td>31.43</td><td>54.98</td><td>38.20</td><td>63.22</td><td>35.16</td><td>58.47</td></tr><tr><td></td><td>âˆš</td><td>39.87</td><td>60.84</td><td>37.67</td><td>63.16</td><td>44.88</td><td>68.59</td><td>40.80</td><td>64.20</td></tr></table></body></html>

Table 5. Recall at K on the FashionIQ validation set employing either the two-stage or the end-to-end approach. We denote IFT (image encoder fine-tuning) and TFT (text encoder fine-tuning) to represent whether the image encoder or the text encoder is fine-tuned in the first stage. We highlight the best scores in bold and underline the second-best scores. 
è¡¨5å±•ç¤ºäº†åœ¨FashionIQéªŒè¯é›†ä¸Šé‡‡ç”¨ä¸¤é˜¶æ®µæˆ–ç«¯åˆ°ç«¯æ–¹æ³•æ—¶çš„Kå€¼å¬å›ç‡ã€‚æˆ‘ä»¬ç”¨IFTï¼ˆå›¾åƒç¼–ç å™¨å¾®è°ƒï¼‰å’ŒTFTï¼ˆæ–‡æœ¬ç¼–ç å™¨å¾®è°ƒï¼‰æ¥è¡¨ç¤ºåœ¨ç¬¬ä¸€é˜¶æ®µæ˜¯å¦å¯¹å›¾åƒç¼–ç å™¨æˆ–æ–‡æœ¬ç¼–ç å™¨è¿›è¡Œäº†å¾®è°ƒã€‚æˆ‘ä»¬å°†æœ€ä½³å¾—åˆ†åŠ ç²—æ˜¾ç¤ºï¼Œå¹¶å°†æ¬¡ä¼˜å¾—åˆ†åŠ ä¸‹åˆ’çº¿ã€‚ 

We report the results for each variation in Table 3 for the FashionIQ dataset and in Table 4 for the CIRR dataset. The element-wise sum of the fine-tuned features serves as a solid starting point. As shown in section 4.3, the task-oriented fine-tuning process is highly effective and results in significant improvements over the out-of-thebox features on both datasets. The convex combination baseline, which dynamically computes text and image convex coefficients for greater adaptability to the query, achieves a slight improvement over the element-wise sum of the features. Notably, when we remove the text and image convex combination, we observe a significant drop in performance compared to the proposed Combiner. This emphasizes the importance of the text and image convex combination in achieving good performance. This result demonstrates that allowing the Combiner network to learn the residual from the element-wise sum (or its generalization, the convex combination) leads to a considerable improvement in performance. This outcome is expected because without the contribution of the image-text convex combination, the effectiveness of the first-stage training, which aims to enhance the additivity properties of the embedding spaces, is compromised. It is worth noting that setting the convex coefficients statically to 0.5 leads to a slight decrease in performance, which is attributed to the greater adaptability of the dynamically computed coefficients.  
æˆ‘ä»¬åœ¨è¡¨3ä¸­æŠ¥å‘Šäº†FashionIQæ•°æ®é›†çš„æ¯ç§å˜ä½“ç»“æœï¼Œåœ¨è¡¨4ä¸­æŠ¥å‘Šäº†CIRRæ•°æ®é›†çš„ç»“æœã€‚å¾®è°ƒç‰¹å¾çš„å…ƒç´ å’Œä½œä¸ºä¸€ä¸ªåšå®çš„èµ·ç‚¹ã€‚å¦‚ç¬¬4.3èŠ‚æ‰€ç¤ºï¼Œé¢å‘ä»»åŠ¡çš„å¾®è°ƒè¿‡ç¨‹éå¸¸æœ‰æ•ˆï¼Œå¹¶ä¸”åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šéƒ½æ˜¾è‘—ä¼˜äºå¼€ç®±å³ç”¨çš„ç‰¹å¾ã€‚å‡¸ç»„åˆåŸºçº¿åŠ¨æ€è®¡ç®—æ–‡æœ¬å’Œå›¾åƒçš„å‡¸ç³»æ•°ï¼Œä»¥æé«˜å¯¹æŸ¥è¯¢çš„é€‚åº”æ€§ï¼Œç›¸æ¯”ç‰¹å¾çš„å…ƒç´ å’Œç•¥æœ‰æå‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“æˆ‘ä»¬ç§»é™¤æ–‡æœ¬å’Œå›¾åƒçš„å‡¸ç»„åˆæ—¶ï¼Œä¸æå‡ºçš„Combinerç›¸æ¯”ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚è¿™å¼ºè°ƒäº†æ–‡æœ¬å’Œå›¾åƒå‡¸ç»„åˆåœ¨å®ç°è‰¯å¥½æ€§èƒ½ä¸­çš„é‡è¦æ€§ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼Œå…è®¸Combinerç½‘ç»œä»å…ƒç´ å’Œï¼ˆæˆ–å…¶æ³›åŒ–ï¼Œå‡¸ç»„åˆï¼‰ä¸­å­¦ä¹ æ®‹å·®ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚è¿™ä¸€ç»“æœæ˜¯é¢„æœŸçš„ï¼Œå› ä¸ºå¦‚æœæ²¡æœ‰å›¾åƒ-æ–‡æœ¬å‡¸ç»„åˆçš„è´¡çŒ®ï¼Œæ—¨åœ¨å¢å¼ºåµŒå…¥ç©ºé—´å¯åŠ æ€§çš„ç¬¬ä¸€é˜¶æ®µè®­ç»ƒçš„æœ‰æ•ˆæ€§å°±ä¼šå—åˆ°å½±å“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°†å‡¸ç³»æ•°é™æ€è®¾ç½®ä¸º0.5ä¼šå¯¼è‡´æ€§èƒ½ç•¥æœ‰ä¸‹é™ï¼Œè¿™å½’å› äºåŠ¨æ€è®¡ç®—ç³»æ•°çš„æ›´å¤§é€‚åº”æ€§ã€‚

Our experiments demonstrate the crucial role of the Combiner architecture in effectively exploiting the full potential of the additive embedding spaces constructed during the first stage of training. By enabling the network to learn the residual from the dynamically computed convex combination, we observe significant performance improvements.  
æˆ‘ä»¬çš„å®éªŒè¯æ˜äº†Combineræ¶æ„åœ¨æœ‰æ•ˆåˆ©ç”¨ç¬¬ä¸€é˜¶æ®µè®­ç»ƒä¸­æ„å»ºçš„åŠ æ€§åµŒå…¥ç©ºé—´å…¨éƒ¨æ½œåŠ›ä¸­çš„å…³é”®ä½œç”¨ã€‚é€šè¿‡ä½¿ç½‘ç»œèƒ½å¤Ÿä»åŠ¨æ€è®¡ç®—çš„å‡¸ç»„åˆä¸­å­¦ä¹ æ®‹å·®ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚

### 4.5 Analysis of Two-Stage vs. End-to-End approach  

In order to explain why a two-stage training method, where the CLIP encoder and Combiner are trained separately, in contrast to an end-to-end approach, we perform an experiment where we compare the two settings on both CIRR and FashionIQ datasets. First, we train end-to-end by fine-tuning CLIP encoders while training the Combiner network simultaneously. Then, we followed the proposed two-stage approach. In both settings, we also enable fine-tuning of the textual or image encoders separately and jointly. In all the experiments in this section, we use the RN-50 CLIP model.  
ä¸ºäº†è§£é‡Šä¸ºä½•é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•â€”â€”å³åˆ†åˆ«è®­ç»ƒCLIPç¼–ç å™¨å’Œç»„åˆå™¨ï¼Œè€Œéç«¯åˆ°ç«¯æ–¹å¼ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å®éªŒï¼Œåœ¨CIRRå’ŒFashionIQä¸¤ä¸ªæ•°æ®é›†ä¸Šå¯¹æ¯”äº†è¿™ä¸¤ç§è®¾ç½®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é‡‡ç”¨ç«¯åˆ°ç«¯æ–¹å¼ï¼Œåœ¨å¾®è°ƒCLIPç¼–ç å™¨çš„åŒæ—¶è®­ç»ƒç»„åˆå™¨ç½‘ç»œã€‚éšåï¼Œéµå¾ªæ‰€æå‡ºçš„ä¸¤é˜¶æ®µæ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚åœ¨è¿™ä¸¤ç§è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬è¿˜åˆ†åˆ«åŠè”åˆå¯ç”¨äº†æ–‡æœ¬æˆ–å›¾åƒç¼–ç å™¨çš„å¾®è°ƒã€‚æœ¬èŠ‚æ‰€æœ‰å®éªŒå‡é‡‡ç”¨RN-50 CLIPæ¨¡å‹ã€‚

We present the results in Table 5 for the FashionIQ dataset and in Table 6 for the CIRR dataset. Remarkably, the two-stage approach consistently outperforms the end-to-end one on both datasets. These superior results remain consistent even when varying the fine-tuning modality.  
æˆ‘ä»¬åœ¨è¡¨5ä¸­å±•ç¤ºäº†FashionIQæ•°æ®é›†çš„ç»“æœï¼Œè¡¨6ä¸­åˆ™æ˜¯CIRRæ•°æ®é›†çš„ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸¤é˜¶æ®µæ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå‡æŒç»­ä¼˜äºç«¯åˆ°ç«¯æ–¹æ³•ã€‚å³ä¾¿åœ¨å¾®è°ƒæ¨¡å¼å˜åŒ–çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸€ä¼˜åŠ¿ç»“æœä¾ç„¶ä¿æŒä¸€è‡´ã€‚

The results validate the effectiveness of constructing an embedding space with robust additivity properties before combining the features using a non-linear function. We hypothesize that when training the Combiner network simultaneously with the CLIP encoders, the entire system struggles to effectively learn the additive embedding spaces and the non-linear combining function in a cohesive manner. As a result, this limitation negatively impacts the overall performance, leading to suboptimal outcomes.  
è¿™äº›ç»“æœéªŒè¯äº†åœ¨åˆ©ç”¨éçº¿æ€§å‡½æ•°ç»„åˆç‰¹å¾ä¹‹å‰ï¼Œæ„å»ºå…·æœ‰å¼ºåŠ æ³•æ€§è´¨çš„åµŒå…¥ç©ºé—´çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æ¨æµ‹ï¼Œå½“ç»„åˆå™¨ç½‘ç»œä¸CLIPç¼–ç å™¨åŒæ—¶è®­ç»ƒæ—¶ï¼Œæ•´ä¸ªç³»ç»Ÿéš¾ä»¥æœ‰æ•ˆåœ°ä»¥ä¸€ä½“åŒ–æ–¹å¼å­¦ä¹ åŠ æ³•åµŒå…¥ç©ºé—´å’Œéçº¿æ€§ç»„åˆå‡½æ•°ã€‚å› æ­¤ï¼Œè¿™ç§é™åˆ¶å¯¹æ•´ä½“æ€§èƒ½äº§ç”Ÿäº†è´Ÿé¢å½±å“ï¼Œå¯¼è‡´äº†ä¸å°½äººæ„çš„ç»“æœã€‚

<html><body><table><tr><td rowspan="2"></td><td rowspan="2">TFT</td><td rowspan="2"></td><td colspan="4">Recall@K</td><td colspan="3">Rsubset @K</td></tr><tr><td>K = 1</td><td>K=5</td><td>K = 10</td><td>K = 50</td><td>K=</td><td>Kï¼š =2</td><td>Kï¼š =3</td></tr><tr><td rowspan="3">End-to-end</td><td></td><td>x</td><td>33.19</td><td>67.01</td><td>79.83</td><td>95.52</td><td>58.90</td><td>79.33</td><td>90.28</td></tr><tr><td>Ã—</td><td></td><td>33.58</td><td>67.59</td><td>79.57</td><td>95.28</td><td>67.37</td><td>85.58</td><td>93.44</td></tr><tr><td></td><td>âˆš</td><td>40.03</td><td>74.09</td><td>85.14</td><td>97.12</td><td>68.14</td><td>86.06</td><td>93.64</td></tr><tr><td rowspan="3">Two-stages</td><td></td><td>x</td><td>34.01</td><td>69.07</td><td>81.77</td><td>95.72</td><td>62.78</td><td>81.80</td><td>91.41</td></tr><tr><td></td><td></td><td>36.86</td><td>71.32</td><td>82.32</td><td>96.24</td><td>68.28</td><td>86.51</td><td>94.14</td></tr><tr><td></td><td></td><td>42.05</td><td>76.13</td><td>86.51</td><td>97.49</td><td>70.15</td><td>87.18</td><td>94.40</td></tr></table></body></html>

Table 6. Recall at K on the CIRR validation set employing either the two-stage or the end-to-end approach. We denote IFT (image encoder fine-tuning) and TFT (text encoder fine-tuning) to represent whether the image encoder or the text encoder is fine-tuned in the first stage. We highlight the best scores in bold and underline the second-best scores.  
è¡¨6å±•ç¤ºäº†åœ¨CIRRéªŒè¯é›†ä¸Šé‡‡ç”¨ä¸¤é˜¶æ®µæˆ–ç«¯åˆ°ç«¯æ–¹æ³•æ—¶çš„Kå€¼å¬å›ç‡ã€‚æˆ‘ä»¬ç”¨IFTï¼ˆå›¾åƒç¼–ç å™¨å¾®è°ƒï¼‰å’ŒTFTï¼ˆæ–‡æœ¬ç¼–ç å™¨å¾®è°ƒï¼‰æ¥è¡¨ç¤ºç¬¬ä¸€é˜¶æ®µæ˜¯å¦å¯¹å›¾åƒç¼–ç å™¨æˆ–æ–‡æœ¬ç¼–ç å™¨è¿›è¡Œäº†å¾®è°ƒã€‚æœ€ä½³å¾—åˆ†ä»¥ç²—ä½“æ ‡å‡ºï¼Œæ¬¡ä½³å¾—åˆ†åˆ™ä»¥ä¸‹åˆ’çº¿è¡¨ç¤ºã€‚

### 4.6 Preprocess upshot  

In this section, we show how the proposed preprocess pipeline, described in Section 3.3, contributes to further improving performance. We compare the proposed preprocess with two other methods: the standard CLIP preprocess pipeline, primarily consisting of resize and center crop operations, and the Square preprocess, which involves applying a square zero-pad to the image before resizing and center cropping. The comparison among the different preprocess techniques is presented in Table 7 for the FashionIQ dataset and Table 8 for the CIRR dataset. 
åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç¬¬3.3èŠ‚ä¸­æè¿°çš„æè®®é¢„å¤„ç†æµç¨‹å¦‚ä½•è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚æˆ‘ä»¬å°†æè®®çš„é¢„å¤„ç†ä¸å¦å¤–ä¸¤ç§æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼šæ ‡å‡†çš„CLIPé¢„å¤„ç†æµç¨‹ï¼Œä¸»è¦åŒ…æ‹¬è°ƒæ•´å¤§å°å’Œä¸­å¿ƒè£å‰ªæ“ä½œï¼›ä»¥åŠSquareé¢„å¤„ç†ï¼Œå³åœ¨è°ƒæ•´å¤§å°å’Œä¸­å¿ƒè£å‰ªä¹‹å‰å¯¹å›¾åƒåº”ç”¨æ–¹å½¢é›¶å¡«å……ã€‚ä¸åŒé¢„å¤„ç†æŠ€æœ¯çš„æ¯”è¾ƒç»“æœåœ¨FashionIQæ•°æ®é›†çš„è¡¨7å’ŒCIRRæ•°æ®é›†çš„è¡¨8ä¸­å‘ˆç°ã€‚

On the FashionIQ dataset, the improvement obtained using the proposed preprocess pipeline over the standard one is substantial in the Dress category and noticeable in the Toptee category. Conversely, the square pad preprocess technique achieves comparable performance to the proposed one in the Dress and Toptee categories while suffering a performance deficit in the Shirt category. Overall, we observe a correlation between the difference in performance among the methods and the number of images with a high aspect ratio, as depicted in Figure 5. In other words, when dealing with images with a high aspect ratio, it is preferable to pad them to avoid losing crucial portions of the image during the center crop operation. On the other hand, when images have a low aspect ratio, it is more effective not to reduce the usable portion of the image with padding. The proposed preprocess pipeline achieves the best performance by effectively adapting to the aspect ratio of each image. On the CIRR dataset, we observe that the proposed preprocess significantly improves performance compared to the standard CLIP and the square preprocess. The performance gain is particularly significant in low-rank recall measures, where the importance of every lost detail is crucial.  
åœ¨FashionIQæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨æè®®çš„é¢„å¤„ç†æµç¨‹ç›¸æ¯”æ ‡å‡†æµç¨‹åœ¨Dressç±»åˆ«ä¸­è·å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨Topteeç±»åˆ«ä¸­ä¹Ÿæœ‰æ˜æ˜¾çš„æå‡ã€‚ç›¸åï¼Œæ–¹å½¢å¡«å……é¢„å¤„ç†æŠ€æœ¯åœ¨Dresså’ŒTopteeç±»åˆ«ä¸­å–å¾—äº†ä¸æè®®é¢„å¤„ç†ç›¸å½“çš„æ€§èƒ½ï¼Œä½†åœ¨Shirtç±»åˆ«ä¸­è¡¨ç°ä¸ä½³ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸åŒæ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ä¸é«˜çºµæ¨ªæ¯”å›¾åƒçš„æ•°é‡ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ï¼Œå¦‚å›¾5æ‰€ç¤ºã€‚æ¢å¥è¯è¯´ï¼Œåœ¨å¤„ç†é«˜çºµæ¨ªæ¯”çš„å›¾åƒæ—¶ï¼Œå®œå¯¹å…¶è¿›è¡Œå¡«å……ï¼Œä»¥é¿å…åœ¨ä¸­å¿ƒè£å‰ªæ“ä½œä¸­ä¸¢å¤±å›¾åƒçš„å…³é”®éƒ¨åˆ†ã€‚å¦ä¸€æ–¹é¢ï¼Œå½“å›¾åƒçš„çºµæ¨ªæ¯”è¾ƒä½æ—¶ï¼Œä¸é€šè¿‡å¡«å……å‡å°‘å›¾åƒå¯ç”¨éƒ¨åˆ†æ›´ä¸ºæœ‰æ•ˆã€‚æè®®çš„é¢„å¤„ç†æµç¨‹é€šè¿‡æœ‰æ•ˆé€‚åº”æ¯å¼ å›¾åƒçš„çºµæ¨ªæ¯”ï¼Œå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚åœ¨CIRRæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æè®®çš„é¢„å¤„ç†ç›¸æ¯”æ ‡å‡†CLIPå’Œæ–¹å½¢é¢„å¤„ç†æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚æ€§èƒ½æå‡åœ¨ä½æ’åå¬å›ç‡æŒ‡æ ‡ä¸­å°¤ä¸ºæ˜æ˜¾ï¼Œåœ¨è¿™äº›æŒ‡æ ‡ä¸­ï¼Œæ¯ä¸€ä¸ªä¸¢å¤±çš„ç»†èŠ‚éƒ½è‡³å…³é‡è¦ã€‚

### 4.7 Comparison with SotA  

We compare the proposed method with state-of-the-art approaches on two standard and challenging datasets. To ensure a fair comparison, we follow the standard experimental settings of the two datasets [37, 51]. Unless specifically mentioned, we report the metrics for each method as documented in the official papers, and we refer to those papers for more comprehensive details about the individual approaches.  
æˆ‘ä»¬å°†æè®®çš„æ–¹æ³•ä¸ä¸¤ä¸ªæ ‡å‡†ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„æœ€æ–°æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚ä¸ºäº†ç¡®ä¿å…¬å¹³æ¯”è¾ƒï¼Œæˆ‘ä»¬éµå¾ªè¿™ä¸¤ä¸ªæ•°æ®é›†çš„æ ‡å‡†å®éªŒè®¾ç½®[37, 51]ã€‚é™¤éç‰¹åˆ«è¯´æ˜ï¼Œæˆ‘ä»¬æŠ¥å‘Šçš„æ–¹æ³•æŒ‡æ ‡å‡æ¥è‡ªå®˜æ–¹è®ºæ–‡ä¸­çš„è®°å½•ï¼Œå¹¶è¯·è¯»è€…å‚è€ƒè¿™äº›è®ºæ–‡ä»¥è·å–å„ä¸ªæ–¹æ³•çš„æ›´å…¨é¢ç»†èŠ‚ã€‚

Table 9 reports the comparison between the proposed method and other state-of-the-art approaches. We divide the table into two sections: the upper section includes methods that are not directly comparable to our approach. These approaches either do not utilize a pre-trained textual encoder [8, 29, 31, 45, 50, 52] or, in the case of TRACE [27], they use BERT [15] as a pre-trained textual encoder but do not update its weights. It is important to note that even when a competitor [8, 29, 45] utilizes the GloVe word embedding [40], we do not consider their textual encoder as pre-trained. All the methods in this section rely on a ResNet model pre-trained on the ImageNet dataset [43] and fine-tuned during training. We include the results of these methods to provide a more comprehensive discussion. The lower section of Table 9 reports methods that are directly comparable to ours: they rely on both pre-trained visual and language models updating all the weights of both backbones during training. CIRRPLANT [37] relies on the OSCAR pretrained model as a textual backbone, while [16, 21, 26] rely on the pre-trained BERT model. It is worth mentioning that FashionViL is a fashion-oriented approach that carries out a large-scale pre-training for learning $\mathrm{V+L}$ representation in the fashion domain. For this reason, it is not surprising that it exhibits strong performances in a fashion dataset such as FashionIQ. When considering the RN50-based method, the proposed approach outperforms the competitors by improving up to $9\%$ in average $\mathrm{R}\ @10$ and $7\%$ in average $\mathrm{R}@50$ compared to the best-performing competitor, FashionViL, when using the same visual backbone architecture. Our method demonstrates the highest recall across all categories, with a particularly significant margin observed in the Shirt category. When considering the larger $\mathrm{RN}{-}50\mathrm{x}4$ -based model, we observe an improvement ranging from $2\%$ to $4\%$ in all categories compared to the smaller backbone. This result demonstrates that our approach scales well when using larger and heavier VL models.  
è¡¨9æŠ¥å‘Šäº†æè®®æ–¹æ³•ä¸å…¶ä»–æœ€æ–°æ–¹æ³•çš„æ¯”è¾ƒã€‚æˆ‘ä»¬å°†è¡¨æ ¼åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šä¸ŠåŠéƒ¨åˆ†åŒ…æ‹¬ä¸æˆ‘ä»¬æ–¹æ³•ä¸ç›´æ¥å¯æ¯”çš„æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•è¦ä¹ˆä¸ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨[8, 29, 31, 45, 50, 52]ï¼Œè¦ä¹ˆå¦‚TRACE[27]ä½¿ç”¨BERT[15]ä½œä¸ºé¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨ä½†ä¸æ›´æ–°å…¶æƒé‡ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿æŸä¸ªç«äº‰æ–¹æ³•[8, 29, 45]ä½¿ç”¨äº†GloVeè¯åµŒå…¥[40]ï¼Œæˆ‘ä»¬ä¹Ÿä¸è®¤ä¸ºå…¶æ–‡æœ¬ç¼–ç å™¨æ˜¯é¢„è®­ç»ƒçš„ã€‚æœ¬éƒ¨åˆ†ä¸­çš„æ‰€æœ‰æ–¹æ³•éƒ½ä¾èµ–äºåœ¨ImageNetæ•°æ®é›†[43]ä¸Šé¢„è®­ç»ƒå¹¶åœ¨è®­ç»ƒæœŸé—´è¿›è¡Œå¾®è°ƒçš„ResNetæ¨¡å‹ã€‚æˆ‘ä»¬çº³å…¥è¿™äº›æ–¹æ³•çš„ç»“æœä»¥æä¾›æ›´å…¨é¢çš„è®¨è®ºã€‚è¡¨9çš„ä¸‹åŠéƒ¨åˆ†æŠ¥å‘Šäº†ä¸æˆ‘ä»¬æ–¹æ³•ç›´æ¥å¯æ¯”çš„æ–¹æ³•ï¼šè¿™äº›æ–¹æ³•ä¾èµ–äºé¢„è®­ç»ƒçš„è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨è®­ç»ƒæœŸé—´æ›´æ–°ä¸¤ä¸ªéª¨å¹²ç½‘ç»œçš„æ‰€æœ‰æƒé‡ã€‚CIRRPLANT[37]ä¾èµ–OSCARé¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºæ–‡æœ¬éª¨å¹²ï¼Œè€Œ[16, 21, 26]ä¾èµ–é¢„è®­ç»ƒçš„BERTæ¨¡å‹ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒFashionViLæ˜¯ä¸€ç§é¢å‘æ—¶å°šçš„æ–¹æ³•ï¼Œå®ƒåœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒä¸­å­¦ä¹ æ—¶å°šé¢†åŸŸçš„$\mathrm{V+L}$è¡¨ç¤ºã€‚å› æ­¤ï¼Œå®ƒåœ¨FashionIQè¿™æ ·çš„æ—¶å°šæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²å¹¶ä¸ä»¤äººæ„å¤–ã€‚åœ¨è€ƒè™‘åŸºäºRN50çš„æ–¹æ³•æ—¶ï¼Œä¸ä½¿ç”¨ç›¸åŒè§†è§‰éª¨å¹²æ¶æ„çš„æœ€ä½³ç«äº‰è€…FashionViLç›¸æ¯”ï¼Œæè®®æ–¹æ³•åœ¨å¹³å‡$\mathrm{R}\ @10$ä¸Šæé«˜äº†é«˜è¾¾$9%$ï¼Œåœ¨å¹³å‡$\mathrm{R}@50$ä¸Šæé«˜äº†$7%$ï¼Œè¶…è¿‡äº†ç«äº‰å¯¹æ‰‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰ç±»åˆ«ä¸­æ˜¾ç¤ºå‡ºæœ€é«˜çš„å¬å›ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨Shirtç±»åˆ«ä¸­å·®è·å°¤ä¸ºæ˜¾è‘—ã€‚åœ¨è€ƒè™‘æ›´å¤§çš„$\mathrm{RN}{-}50\mathrm{x}4$æ¨¡å‹æ—¶ï¼Œä¸è¾ƒå°çš„éª¨å¹²ç›¸æ¯”ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ‰€æœ‰ç±»åˆ«çš„æ”¹è¿›èŒƒå›´åœ¨$2%$åˆ°$4%$ä¹‹é—´ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä½¿ç”¨æ›´å¤§ã€æ›´é‡çš„VLæ¨¡å‹æ—¶å…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ã€‚

<html><body><table><tr><td></td><td></td><td colspan="2">Shirt</td><td colspan="2">Dress</td><td colspan="2">Toptee</td><td colspan="2">Average</td></tr><tr><td>CF</td><td>Preprocess</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td></tr><tr><td rowspan="3">Sum</td><td>Standard</td><td>37.64</td><td>59.76</td><td>33.42</td><td>59.84</td><td>40.90</td><td>66.80</td><td>37.32</td><td>62.13</td></tr><tr><td>Square</td><td>37.09</td><td>58.52</td><td>35.94</td><td>62.03</td><td>42.53</td><td>66.29</td><td>38.52</td><td>62.28</td></tr><tr><td>Proposed</td><td>38.67</td><td>59.42</td><td>35.99</td><td>62.22</td><td>43.35</td><td>67.52</td><td>39.34</td><td>63.05</td></tr><tr><td rowspan="3">Combiner</td><td>Standard</td><td>39.40</td><td>61.33</td><td>35.25</td><td>60.44</td><td>43.95</td><td>67.72</td><td>39.53</td><td>63.16</td></tr><tr><td>Square</td><td>38.71</td><td>60.21</td><td>37.97</td><td>62.86</td><td>44.12</td><td>68.03</td><td>40.26</td><td>63.70</td></tr><tr><td>Proposed</td><td>39.87</td><td>60.84</td><td>37.67</td><td>63.16</td><td>44.88</td><td>68.59</td><td>40.80</td><td>64.20</td></tr></table></body></html>

Table 7. Recall at K on FashionIQ validation set varying the combining function and the preprocessing pipeline used. CF (combining function) indicates the function used to combine the query features. We highlight the best scores in bold and underline the second-best scores.  
è¡¨7. åœ¨FashionIQéªŒè¯é›†ä¸Šï¼Œæ ¹æ®ä½¿ç”¨çš„ç»„åˆå‡½æ•°å’Œé¢„å¤„ç†æµç¨‹å˜åŒ–çš„Kå€¼å¬å›ç‡ã€‚CFï¼ˆç»„åˆå‡½æ•°ï¼‰è¡¨ç¤ºç”¨äºç»„åˆæŸ¥è¯¢ç‰¹å¾çš„å‡½æ•°ã€‚æˆ‘ä»¬ä»¥ç²—ä½“æ ‡å‡ºæœ€ä½³å¾—åˆ†ï¼Œå¹¶ç”¨ä¸‹åˆ’çº¿æ ‡å‡ºæ¬¡ä½³å¾—åˆ†ã€‚

<html><body><table><tr><td rowspan="2">CF</td><td rowspan="2"></td><td colspan="4">Recall@K</td><td colspan="3">Rsubset @K</td></tr><tr><td>Preprocess K =1</td><td>K=5</td><td>K = 10</td><td>K =50</td><td>K= ï¼š1</td><td>K=2</td><td>K=3</td></tr><tr><td rowspan="3">Sum</td><td>Standard</td><td>39.51</td><td>74.00</td><td>84.72</td><td>97.20</td><td>68.36</td><td>86.15</td><td>94.26</td></tr><tr><td>Square</td><td>41.26</td><td>74.34</td><td>85.00</td><td>96.84</td><td>69.15</td><td>85.89</td><td>93.90</td></tr><tr><td>Proposed</td><td>40.97</td><td>74.70</td><td>85.51</td><td>96.94</td><td>68.81</td><td>86.96</td><td>93.90</td></tr><tr><td rowspan="3">Combiner</td><td>Standard</td><td>40.08</td><td>74.15</td><td>84.67</td><td>97.20</td><td>69.53</td><td>86.27</td><td>94.45</td></tr><tr><td>Square</td><td>41.95</td><td>74.96</td><td>85.24</td><td>96.58</td><td>70.65</td><td>86.67</td><td>94.24</td></tr><tr><td>Proposed</td><td>42.05</td><td>76.13</td><td>86.51</td><td>97.49</td><td>70.15</td><td>87.18</td><td>94.40</td></tr></table></body></html>

Table 8. Recall at K on CIRR validation set varying the combining function and the preprocessing pipeline used. CF (combining function) indicates the function used to combine the query features. We highlight the best scores in bold and underline the second-best scores.  
è¡¨8. åœ¨CIRRéªŒè¯é›†ä¸Šï¼Œæ ¹æ®ä½¿ç”¨çš„ç»„åˆå‡½æ•°å’Œé¢„å¤„ç†æµç¨‹å˜åŒ–çš„Kå€¼å¬å›ç‡ã€‚CFï¼ˆç»„åˆå‡½æ•°ï¼‰è¡¨ç¤ºç”¨äºç»„åˆæŸ¥è¯¢ç‰¹å¾çš„å‡½æ•°ã€‚æˆ‘ä»¬ä»¥ç²—ä½“æ ‡å‡ºæœ€ä½³å¾—åˆ†ï¼Œå¹¶ç”¨ä¸‹åˆ’çº¿æ ‡å‡ºæ¬¡ä½³å¾—åˆ†ã€‚

In Table 10, we report a comparison between the proposed method and other state-of-the-art approaches. As for FashionIQ, the upper section of the table reports methods that are not directly comparable with the proposed one: they do not utilize a pre-trained textual encoder. As the visual backbone, they employ a ResNet-based model, which is pre-trained on ImageNet and fine-tuned during training. The lower section of the table includes directly comparable methods, such as MAAF, which utilizes BERT as a text encoder, and CIRPLANT, which relies on the pre-trained Vision-Language model OSCAR. The results presented in Table 10 are obtained through the official evaluation server. Our approach consistently outperforms the competitors by a significant margin, particularly in low-rank recall measures, where we notice an improvement of approximately $20\%$ in $\mathrm{R}(\varpi1$ when using the RN50 visual backbone. When considering the larger RN- $\it{\cdot50x4}$ model, we observe improvements ranging from $3\%$ in low-rank recall metrics to $1\%$ as the recall rank increases.  
åœ¨è¡¨10ä¸­ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†æè®®æ–¹æ³•ä¸å…¶ä»–æœ€å…ˆè¿›æ–¹æ³•çš„æ¯”è¾ƒã€‚ä¸FashionIQç±»ä¼¼ï¼Œè¡¨æ ¼çš„ä¸ŠåŠéƒ¨åˆ†æŠ¥å‘Šäº†ä¸æè®®æ–¹æ³•ä¸ç›´æ¥å¯æ¯”çš„æ–¹æ³•ï¼šè¿™äº›æ–¹æ³•æœªä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨ã€‚ä½œä¸ºè§†è§‰éª¨å¹²ï¼Œå®ƒä»¬é‡‡ç”¨äº†åŸºäºResNetçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ImageNetä¸Šé¢„è®­ç»ƒå¹¶åœ¨è®­ç»ƒæœŸé—´è¿›è¡Œå¾®è°ƒã€‚è¡¨æ ¼çš„ä¸‹åŠéƒ¨åˆ†åŒ…æ‹¬ç›´æ¥å¯æ¯”çš„æ–¹æ³•ï¼Œä¾‹å¦‚ä½¿ç”¨BERTä½œä¸ºæ–‡æœ¬ç¼–ç å™¨çš„MAAFï¼Œä»¥åŠä¾èµ–é¢„è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹OSCARçš„CIRPLANTã€‚è¡¨10ä¸­å‘ˆç°çš„ç»“æœæ˜¯é€šè¿‡å®˜æ–¹è¯„ä¼°æœåŠ¡å™¨è·å¾—çš„ã€‚æˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä»¥æ˜¾è‘—ä¼˜åŠ¿è¶…è¶Šç«äº‰å¯¹æ‰‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½æ’åå¬å›ç‡æŒ‡æ ‡ä¸­ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨RN50è§†è§‰éª¨å¹²æ—¶æ³¨æ„åˆ°$\mathrm{R}(\varpi1$æé«˜äº†å¤§çº¦$20\%$ã€‚åœ¨è€ƒè™‘æ›´å¤§çš„RN-$\it{\cdot50x4}$æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä½æ’åå¬å›ç‡æŒ‡æ ‡çš„æ”¹è¿›å¹…åº¦ä¸º$3\%$ï¼Œéšç€å¬å›æ’åçš„å¢åŠ ï¼Œæ”¹è¿›å¹…åº¦é€æ¸å‡å°è‡³$1\%$ã€‚

<html><body><table><tr><td rowspan="2">Method</td><td colspan="2">Encoder</td><td colspan="2">Shirt</td><td colspan="2">Dress</td><td colspan="2">Toptee</td><td colspan="2">Average</td></tr><tr><td>Visual</td><td>Textual</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td></tr><tr><td>TRACE [27]</td><td>RN-50</td><td>BERT [15]</td><td>20.80</td><td>40.80</td><td>22.70</td><td>44.91</td><td>24.22</td><td>49.80</td><td>22.57</td><td>46.19</td></tr><tr><td>VAL [8]</td><td>RN-50</td><td>LSTM(GloVe) [23]</td><td>22.38</td><td>44.15</td><td>22.53</td><td>44.00</td><td>27.53</td><td>51.68</td><td>24.15</td><td>46.61</td></tr><tr><td>CurlingNet [52]</td><td>RN-152</td><td>biGRU [10]</td><td>21.45</td><td>44.56</td><td>26.15</td><td>53.24</td><td>30.12</td><td>55.23</td><td>25.90</td><td>51.01</td></tr><tr><td>RTIC-GCN[45]</td><td>RN-50</td><td>LSTM(GloVe)</td><td>23.79</td><td>47.25</td><td>29.15</td><td>54.04</td><td>31.61</td><td>57.98</td><td>28.18</td><td>53.09</td></tr><tr><td>CoSMo [31]</td><td>RN-50</td><td>LSTM</td><td>24.90</td><td>49.18</td><td>25.64</td><td>50.30</td><td>29.21</td><td>57.46</td><td>26.58</td><td>52.31</td></tr><tr><td>DCNet [29]</td><td>RN-50</td><td>Conv1D(GloVe)</td><td>23.95</td><td>47.30</td><td>28.95</td><td>56.07</td><td>30.44</td><td>58.29</td><td>27.78</td><td>53.89</td></tr><tr><td>CLVC-Net [50]</td><td>RN-50</td><td>LSTM</td><td>28.75</td><td>54.76</td><td>29.85</td><td>56.47</td><td>33.50</td><td>64.00</td><td>30.70</td><td>58.41</td></tr><tr><td>CIRPLANT [37]</td><td>RN-152</td><td>OSCAR [34]</td><td>17.53</td><td>38.81</td><td>17.45</td><td>40.41</td><td>21.64</td><td>45.38</td><td>18.87</td><td>41.53</td></tr><tr><td>MAAF [16]</td><td>RN-50</td><td>BERT</td><td>18.55</td><td>37.63</td><td>18.59</td><td>39.66</td><td>23.05</td><td>45.95</td><td>20.06</td><td>41.08</td></tr><tr><td>SAC [26]</td><td>RN-50</td><td>BERT</td><td>28.02</td><td>51.86</td><td>26.52</td><td>51.01</td><td>32.70</td><td>61.23</td><td>29.08</td><td>54.70</td></tr><tr><td>FashionViL[21]</td><td>RN-50</td><td>BERT</td><td>25.17</td><td>50.39</td><td>33.47</td><td>59.94</td><td>34.98</td><td>60.79</td><td>31.20</td><td>57.04</td></tr><tr><td>Ours</td><td>RN-50</td><td>Transformer</td><td>39.87</td><td>60.84</td><td>37.67</td><td>63.16</td><td>44.88</td><td>68.59</td><td>40.80</td><td>64.20</td></tr><tr><td>Ours</td><td>RN-50x4</td><td>Transformer</td><td>44.41</td><td>65.26</td><td>39.46</td><td>64.55</td><td>47.48</td><td>70.98</td><td>43.78</td><td>66.93</td></tr></table></body></html>

Table 9. Comparison between our method and current state-of-the-art models on the Fashion-IQ validation set. We highlight the best scores in bold and underline the second-best scores. The upper section of the table presents methods that are not directly comparable to our proposed approach, as they either do not utilize a pre-trained textual encoder or do not update its weights. "RN" stands for ResNet.  
è¡¨9. æˆ‘ä»¬çš„æ–¹æ³•ä¸Fashion-IQéªŒè¯é›†ä¸Šå½“å‰æœ€å…ˆè¿›æ¨¡å‹çš„æ¯”è¾ƒã€‚æˆ‘ä»¬ä»¥ç²—ä½“æ ‡å‡ºæœ€ä½³å¾—åˆ†ï¼Œå¹¶ç”¨ä¸‹åˆ’çº¿æ ‡å‡ºæ¬¡ä½³å¾—åˆ†ã€‚è¡¨æ ¼çš„ä¸ŠåŠéƒ¨åˆ†å±•ç¤ºäº†ä¸æˆ‘ä»¬æè®®æ–¹æ³•ä¸ç›´æ¥å¯æ¯”çš„æ–¹æ³•ï¼Œå› ä¸ºè¿™äº›æ–¹æ³•è¦ä¹ˆæœªä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨ï¼Œè¦ä¹ˆæœªæ›´æ–°å…¶æƒé‡ã€‚"RN"ä»£è¡¨ResNetã€‚

<html><body><table><tr><td></td><td colspan="2">Encoder</td><td colspan="4">Recall@K</td><td colspan="3">Rsubset @K</td></tr><tr><td>Method</td><td>Visual</td><td>Textual</td><td>K=1</td><td>K=5</td><td>K =10</td><td>K =50</td><td>K=1</td><td>Kï¼š =2</td><td>K=3</td></tr><tr><td>TIRG [48]</td><td>RN-18</td><td>LSTM</td><td>14.61</td><td>48.37</td><td>64.08</td><td>90.03</td><td>22.67</td><td>44.97</td><td>65.14</td></tr><tr><td>TIRG+LastConv+ [48]</td><td>RN-18</td><td>LSTM</td><td>11.04</td><td>35.68</td><td>51.27</td><td>83.29</td><td>23.82</td><td>45.65</td><td>64.55</td></tr><tr><td>MAAFt [16]</td><td>RN-50</td><td>LSTM</td><td>10.31</td><td>33.03</td><td>48.30</td><td>80.06</td><td>21.05</td><td>41.81</td><td>61.60</td></tr><tr><td>MAAF-IT [16]</td><td>RN-50</td><td>LSTM</td><td>9.90</td><td>32.86</td><td>48.83</td><td>80.27</td><td>21.17</td><td>42.04</td><td>60.91</td></tr><tr><td>MAAF-RP+ [16]</td><td>RN-50</td><td>LSTM</td><td>10.22</td><td>33.32</td><td>48.68</td><td>81.84</td><td>21.41</td><td>42.17</td><td>61.60</td></tr><tr><td>ARTEMIS [14]</td><td>RN-152</td><td>biGRU</td><td>16.96</td><td>46.10</td><td>61.31</td><td>87.73</td><td>39.99</td><td>62.20</td><td>75.67</td></tr><tr><td>MAAFt [16]</td><td>RN-50</td><td>BERT</td><td>10.12</td><td>33.10</td><td>48.01</td><td>80.57</td><td>22.04</td><td>42.41</td><td>62.14</td></tr><tr><td>CIRPLANTt [37]</td><td>RN-152</td><td>OSCAR</td><td>19.55</td><td>52.55</td><td>68.39</td><td>92.38</td><td>39.20</td><td>63.03</td><td>79.49</td></tr><tr><td>Ours</td><td>RN-50</td><td>Transformer</td><td>40.91</td><td>74.53</td><td>84.77</td><td>97.35</td><td>70.22</td><td>87.80</td><td>94.46</td></tr><tr><td>Ours</td><td>RN-50x4</td><td>Transformer</td><td>44.82</td><td>77.04</td><td>86.65</td><td>97.90</td><td>73.16</td><td>88.84</td><td>95.59</td></tr></table></body></html>

Table 10. Comparison between our method and current state-of-the-art models on the CIRR test set. We highlight the best scores in bold and underline the second-best scores.. â€  denotes results cited from [37]. The upper section of the table presents methods that are not directly comparable to our proposed approach, as they either do not utilize a pre-trained textual encoder or do not update its weights. In the lower section, we report methods that are directly comparable to our approach. "RN" stands for ResNet. 
 è¡¨10. æˆ‘ä»¬çš„æ–¹æ³•ä¸CIRRæµ‹è¯•é›†ä¸Šå½“å‰æœ€å…ˆè¿›æ¨¡å‹çš„æ¯”è¾ƒã€‚æˆ‘ä»¬ä»¥ç²—ä½“æ ‡å‡ºæœ€ä½³å¾—åˆ†ï¼Œå¹¶ç”¨ä¸‹åˆ’çº¿æ ‡å‡ºæ¬¡ä½³å¾—åˆ†ã€‚â€ è¡¨ç¤ºå¼•è‡ª[37]çš„ç»“æœã€‚è¡¨æ ¼çš„ä¸ŠåŠéƒ¨åˆ†å±•ç¤ºäº†ä¸æˆ‘ä»¬æè®®æ–¹æ³•ä¸ç›´æ¥å¯æ¯”çš„æ–¹æ³•ï¼Œå› ä¸ºè¿™äº›æ–¹æ³•è¦ä¹ˆæœªä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨ï¼Œè¦ä¹ˆæœªæ›´æ–°å…¶æƒé‡ã€‚åœ¨è¡¨æ ¼çš„ä¸‹åŠéƒ¨åˆ†ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†ä¸æˆ‘ä»¬æ–¹æ³•ç›´æ¥å¯æ¯”çš„æ–¹æ³•ã€‚"RN"ä»£è¡¨ResNetã€‚

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/e73257470695e3393506c14a162d98f7ddc27b9862e3eb09ba251dff2547a73d.jpg)  
Fig. 7. Histograms of cosine similarities between image/text feature pairs. The $\mathbf{x}\cdot\mathbf{\partial}$ -axis represents the cosine similarities. The y-axis represents the (normalized) number of pairs. In the top-line plots, we have used the out-of-the-box CLIP model. In the bottom line, we have used the model fine-tuned during the first stage of training. In the left-side plots, we compare the image features. In the right ones, we compare the text features. The histograms are normalized such that the area under each curve integrates to 1.  
å›¾7. å›¾åƒ/æ–‡æœ¬ç‰¹å¾å¯¹ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ç›´æ–¹å›¾ã€‚$\mathbf{x}\cdot\mathbf{\partial}$è½´è¡¨ç¤ºä½™å¼¦ç›¸ä¼¼åº¦ã€‚yè½´è¡¨ç¤ºï¼ˆå½’ä¸€åŒ–çš„ï¼‰å¯¹æ•°ã€‚åœ¨é¡¶è¡Œå›¾ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¼€ç®±å³ç”¨çš„CLIPæ¨¡å‹ã€‚åœ¨åº•è¡Œå›¾ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†åœ¨è®­ç»ƒç¬¬ä¸€é˜¶æ®µå¾®è°ƒåçš„æ¨¡å‹ã€‚åœ¨å·¦ä¾§å›¾ä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†å›¾åƒç‰¹å¾ã€‚åœ¨å³ä¾§å›¾ä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†æ–‡æœ¬ç‰¹å¾ã€‚ç›´æ–¹å›¾ç»è¿‡å½’ä¸€åŒ–å¤„ç†ï¼Œä½¿å¾—æ¯æ¡æ›²çº¿ä¸‹çš„é¢ç§¯ç§¯åˆ†ç­‰äº1ã€‚

### 4.8 Feature distribution study  

The experiments in this section aim to provide intuition on how the feature distribution in the embedding spaces affects the retrieval performances. All the experiments were carried out on the validation sets using the RN-50 model. We are going to present two different sets of experiments that have slightly different purposes. The first set aims to investigate how the image and text features are distributed in the embedding spaces, while the second one explores how the distribution of the features affects retrieval performance.  
æœ¬èŠ‚çš„å®éªŒæ—¨åœ¨æä¾›å…³äºåµŒå…¥ç©ºé—´ä¸­çš„ç‰¹å¾åˆ†å¸ƒå¦‚ä½•å½±å“æ£€ç´¢æ€§èƒ½çš„ç›´è§‰ã€‚æ‰€æœ‰å®éªŒå‡åœ¨éªŒè¯é›†ä¸Šä½¿ç”¨RN-50æ¨¡å‹è¿›è¡Œã€‚æˆ‘ä»¬å°†ä»‹ç»ä¸¤ç»„ç•¥æœ‰ä¸åŒç›®çš„çš„å®éªŒã€‚ç¬¬ä¸€ç»„å®éªŒæ—¨åœ¨ç ”ç©¶å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾å¦‚ä½•åœ¨åµŒå…¥ç©ºé—´ä¸­åˆ†å¸ƒï¼Œè€Œç¬¬äºŒç»„å®éªŒåˆ™æ¢ç´¢ç‰¹å¾åˆ†å¸ƒå¦‚ä½•å½±å“æ£€ç´¢æ€§èƒ½ã€‚

To investigate how the features distribute in the embedding spaces, we followed [35] and calculate pairwise similarities among them. If the features occupy the embedding space uniformly, their similarities will be lower. Throughout all experiments, due to the quadratic growth of possible pairs, we compute the similarities between 50K randomly sampled pairs. Figure 7 shows the histograms of the features pairwise similarities on both FashionIQ and CIRR datasets. First of all, we can notice that due to the broader domain of CIRR, on such a dataset, both the image and text features similarities are higher when compared to FashionIQ. On both datasets, fine-tuning the image encoder leads to a drastic reduction in the average similarity of the visual features and, thus, to much more efficient use of the embedding space during retrieval. This fact confirms our hypothesis (Section 4.3) that fine-tuning the image encoder adapts the image manifold to the data domain. The fine-tuning of the text encoder leads to a lower reduction in the average pairwise similarity of textual features (almost negligible in FashionIQ) than that observed in visual ones. We suppose that efficient use of the image embedding space is far more critical than efficient use of the textual space since the retrieval is carried out in the image space. In all the experiments, we observe that the fine-tuning of CLIP encoders contributes to reducing the cone effect: â€œthe effective embedding space is restricted to a narrow cone for trained models and models with random weights" [35].  
ä¸ºäº†ç ”ç©¶ç‰¹å¾åœ¨åµŒå…¥ç©ºé—´ä¸­çš„åˆ†å¸ƒæƒ…å†µï¼Œæˆ‘ä»¬éµå¾ª[35]çš„æ–¹æ³•ï¼Œè®¡ç®—å®ƒä»¬ä¹‹é—´çš„æˆå¯¹ç›¸ä¼¼æ€§ã€‚å¦‚æœç‰¹å¾åœ¨åµŒå…¥ç©ºé—´ä¸­å‡åŒ€åˆ†å¸ƒï¼Œå®ƒä»¬çš„ç›¸ä¼¼æ€§å°†è¾ƒä½ã€‚åœ¨æ‰€æœ‰å®éªŒä¸­ï¼Œç”±äºå¯èƒ½å¯¹æ•°çš„å¹³æ–¹å¢é•¿ï¼Œæˆ‘ä»¬è®¡ç®—äº†50Kä¸ªéšæœºé‡‡æ ·å¯¹ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚å›¾7æ˜¾ç¤ºäº†FashionIQå’ŒCIRRæ•°æ®é›†ä¸Šç‰¹å¾æˆå¯¹ç›¸ä¼¼æ€§çš„ç›´æ–¹å›¾ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ°ï¼Œç”±äºCIRRçš„é¢†åŸŸæ›´å¹¿æ³›ï¼Œåœ¨è¯¥æ•°æ®é›†ä¸Šï¼Œå›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„ç›¸ä¼¼æ€§æ¯”FashionIQæ›´é«˜ã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šï¼Œå¾®è°ƒå›¾åƒç¼–ç å™¨æ˜¾è‘—é™ä½äº†è§†è§‰ç‰¹å¾çš„å¹³å‡ç›¸ä¼¼æ€§ï¼Œä»è€Œåœ¨æ£€ç´¢æœŸé—´æ›´é«˜æ•ˆåœ°åˆ©ç”¨äº†åµŒå…¥ç©ºé—´ã€‚è¿™ä¸€äº‹å®è¯å®äº†æˆ‘ä»¬çš„å‡è®¾ï¼ˆç¬¬4.3èŠ‚ï¼‰ï¼Œå³å¾®è°ƒå›¾åƒç¼–ç å™¨ä½¿å›¾åƒæµå½¢é€‚åº”æ•°æ®é¢†åŸŸã€‚æ–‡æœ¬ç¼–ç å™¨çš„å¾®è°ƒå¯¹æ–‡æœ¬ç‰¹å¾å¹³å‡æˆå¯¹ç›¸ä¼¼æ€§çš„é™ä½å¹…åº¦å°äºè§†è§‰ç‰¹å¾ï¼ˆåœ¨FashionIQä¸­å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡ï¼‰ã€‚æˆ‘ä»¬æ¨æµ‹ï¼Œé«˜æ•ˆåˆ©ç”¨å›¾åƒåµŒå…¥ç©ºé—´è¿œæ¯”é«˜æ•ˆåˆ©ç”¨æ–‡æœ¬ç©ºé—´æ›´é‡è¦ï¼Œå› ä¸ºæ£€ç´¢æ˜¯åœ¨å›¾åƒç©ºé—´ä¸­è¿›è¡Œçš„ã€‚åœ¨æ‰€æœ‰å®éªŒä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°CLIPç¼–ç å™¨çš„å¾®è°ƒæœ‰åŠ©äºå‡å°‘é”¥å½¢æ•ˆåº”ï¼šâ€œå¯¹äºè®­ç»ƒè¿‡çš„æ¨¡å‹å’Œå…·æœ‰éšæœºæƒé‡çš„æ¨¡å‹ï¼Œæœ‰æ•ˆåµŒå…¥ç©ºé—´è¢«é™åˆ¶åœ¨ä¸€ä¸ªç‹­çª„çš„é”¥å½¢èŒƒå›´å†…â€[35]ã€‚

The previous experiments demonstrate how the two-stage approach proposed in this study affects the textual and visual CLIP embedding spaces. However, these experiments do not clarify why this increased utilization of embedding space can improve the retrieval process. We conducted additional experiments to investigate the impact of this embedding space reshaping on the image retrieval task. We compute and compare the cosine similarities (the distance function used in the retrieval) between the combined and the index image features.  
ä¹‹å‰çš„å®éªŒå±•ç¤ºäº†æœ¬ç ”ç©¶æå‡ºçš„ä¸¤é˜¶æ®µæ–¹æ³•å¦‚ä½•å½±å“æ–‡æœ¬å’Œè§†è§‰CLIPåµŒå…¥ç©ºé—´ã€‚ç„¶è€Œï¼Œè¿™äº›å®éªŒå¹¶æœªé˜æ˜ä¸ºä½•å¢åŠ åµŒå…¥ç©ºé—´çš„åˆ©ç”¨ç‡å¯ä»¥æ”¹è¿›æ£€ç´¢è¿‡ç¨‹ã€‚æˆ‘ä»¬è¿›è¡Œäº†é¢å¤–çš„å®éªŒï¼Œä»¥ç ”ç©¶è¿™ç§åµŒå…¥ç©ºé—´é‡å¡‘å¯¹å›¾åƒæ£€ç´¢ä»»åŠ¡çš„å½±å“ã€‚æˆ‘ä»¬è®¡ç®—å¹¶æ¯”è¾ƒäº†ç»„åˆç‰¹å¾ä¸ç´¢å¼•å›¾åƒç‰¹å¾ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§ï¼ˆæ£€ç´¢ä¸­ä½¿ç”¨çš„è·ç¦»å‡½æ•°ï¼‰ã€‚

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/d2bd20a5555f12982c579070b1fc1ef0ebe0600a7cdc2e24112c4db10f417215.jpg)  
Fig. 8. Histograms of the cosine similarities between combined and target/non-target feature pairs. The $\mathbf{x}\cdot\mathbf{\partial}$ -axis represents the cosine similarities. The y-axis represents the (normalized) number of pairs. In greenâ€¢: cosine similarities between combined and non-target index features. In violetâ€¢: cosine similarities between combined and target features. In redâ€¢: similarity gap between combined-target and combined-non target features. In dark greenâ€¢: intersection over union area between the two histograms. In the top-line plots, we have used the simple sum as a combining function. In the bottom line ones, we have used the Combiner network. In the left-side plots, we used the out-of-the-box CLIP model. In the right ones, we used the model fine-tuned during the first stage of the training. The histograms are normalized such that the area under each curve integrates to 1.  
å›¾8. ç»„åˆç‰¹å¾ä¸ç›®æ ‡/éç›®æ ‡ç‰¹å¾å¯¹ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ç›´æ–¹å›¾ã€‚$\mathbf{x}\cdot\mathbf{\partial}$è½´è¡¨ç¤ºä½™å¼¦ç›¸ä¼¼åº¦ã€‚yè½´è¡¨ç¤ºï¼ˆå½’ä¸€åŒ–çš„ï¼‰å¯¹æ•°ã€‚ç»¿è‰²â€¢ï¼šç»„åˆç‰¹å¾ä¸éç›®æ ‡ç´¢å¼•ç‰¹å¾ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§ã€‚ç´«è‰²â€¢ï¼šç»„åˆç‰¹å¾ä¸ç›®æ ‡ç‰¹å¾ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§ã€‚çº¢è‰²â€¢ï¼šç»„åˆ-ç›®æ ‡ä¸ç»„åˆ-éç›®æ ‡ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§å·®è·ã€‚æ·±ç»¿è‰²â€¢ï¼šä¸¤ä¸ªç›´æ–¹å›¾ä¹‹é—´çš„äº¤å¹¶æ¯”é¢ç§¯ã€‚åœ¨é¡¶è¡Œå›¾ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ç®€å•æ±‚å’Œä½œä¸ºç»„åˆå‡½æ•°ã€‚åœ¨åº•è¡Œå›¾ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†Combinerç½‘ç»œã€‚åœ¨å·¦ä¾§å›¾ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¼€ç®±å³ç”¨çš„CLIPæ¨¡å‹ã€‚åœ¨å³ä¾§å›¾ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†åœ¨è®­ç»ƒç¬¬ä¸€é˜¶æ®µå¾®è°ƒåçš„æ¨¡å‹ã€‚ç›´æ–¹å›¾ç»è¿‡å½’ä¸€åŒ–å¤„ç†ï¼Œä½¿å¾—æ¯æ¡æ›²çº¿ä¸‹çš„é¢ç§¯ç§¯åˆ†ç­‰äº1ã€‚

Specifically, we perform two distinct computations: in the first one, we compute the similarity between the combined features and the target image features belonging to the same query triplet. In the second one, we compute the similarity between the combined features and random image features that differ from the target ones. Given a query, we will refer to the images that differ from the target as non-target images. We compare each combined feature with ten non-target image features to reduce the variance. 
å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬æ‰§è¡Œäº†ä¸¤ç§ä¸åŒçš„è®¡ç®—ï¼šåœ¨ç¬¬ä¸€ç§ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº†ç»„åˆç‰¹å¾ä¸å±äºåŒä¸€æŸ¥è¯¢ä¸‰å…ƒç»„çš„ç›®æ ‡å›¾åƒç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚åœ¨ç¬¬äºŒç§ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº†ç»„åˆç‰¹å¾ä¸ä¸åŒäºç›®æ ‡çš„éšæœºå›¾åƒç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ç»™å®šä¸€ä¸ªæŸ¥è¯¢ï¼Œæˆ‘ä»¬å°†ä¸åŒäºç›®æ ‡çš„å›¾åƒç§°ä¸ºéç›®æ ‡å›¾åƒã€‚æˆ‘ä»¬å°†æ¯ä¸ªç»„åˆç‰¹å¾ä¸åä¸ªéç›®æ ‡å›¾åƒç‰¹å¾è¿›è¡Œæ¯”è¾ƒï¼Œä»¥å‡å°‘æ–¹å·®ã€‚ 

Figure 8 emphasizes the similarity gaps between the combined and target/non-target features. On both FashionIQ and CIRR datasets, we notice that the element-wise sum of out-of-the-box CLIP features achieves the highest average combined-target features similarity. During both the fine-tuning and the Combiner network training stages, the contrastive training increases the cosine distances between the combined and non-target features instead of increasing their similarity to the target features. By observing both Fig. 8a and Fig. 8b and the corresponding retrieval results in Table 1 and Table 2, we argue that, in these two datasets, the retrieval performances are highly correlated with the similarity gap between combined-target and combined-non target features (displayed as the red arrows in Fig. 8) and with the size of intersection area between the histograms (the smaller the intersection area, the smaller the retrieval errors will be). On the contrary, the absolute value of the combined-target similarity does not seem to be of great importance.  
å›¾8å¼ºè°ƒäº†ç»„åˆç‰¹å¾ä¸ç›®æ ‡/éç›®æ ‡ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§å·®è·ã€‚åœ¨FashionIQå’ŒCIRRæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬æ³¨æ„åˆ°ï¼Œå¼€ç®±å³ç”¨CLIPç‰¹å¾çš„å…ƒç´ çº§æ±‚å’Œå®ç°äº†æœ€é«˜çš„å¹³å‡ç»„åˆ-ç›®æ ‡ç‰¹å¾ç›¸ä¼¼æ€§ã€‚åœ¨å¾®è°ƒå’ŒCombinerç½‘ç»œè®­ç»ƒé˜¶æ®µï¼Œå¯¹æ¯”è®­ç»ƒå¢åŠ äº†ç»„åˆç‰¹å¾ä¸éç›®æ ‡ç‰¹å¾ä¹‹é—´çš„ä½™å¼¦è·ç¦»ï¼Œè€Œä¸æ˜¯å¢åŠ å®ƒä»¬ä¸ç›®æ ‡ç‰¹å¾çš„ç›¸ä¼¼æ€§ã€‚é€šè¿‡è§‚å¯Ÿå›¾8aå’Œå›¾8bä»¥åŠè¡¨1å’Œè¡¨2ä¸­çš„ç›¸åº”æ£€ç´¢ç»“æœï¼Œæˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨è¿™ä¸¤ä¸ªæ•°æ®é›†ä¸­ï¼Œæ£€ç´¢æ€§èƒ½ä¸ç»„åˆ-ç›®æ ‡å’Œç»„åˆ-éç›®æ ‡ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§å·®è·ï¼ˆåœ¨å›¾8ä¸­æ˜¾ç¤ºä¸ºçº¢è‰²ç®­å¤´ï¼‰ä»¥åŠç›´æ–¹å›¾ä¹‹é—´çš„äº¤é›†é¢ç§¯å¤§å°é«˜åº¦ç›¸å…³ï¼ˆäº¤é›†é¢ç§¯è¶Šå°ï¼Œæ£€ç´¢é”™è¯¯è¶Šå°ï¼‰ã€‚ç›¸åï¼Œç»„åˆ-ç›®æ ‡ç›¸ä¼¼æ€§çš„ç»å¯¹å€¼ä¼¼ä¹å¹¶ä¸é‡è¦ã€‚

The two sets of experiments highlight different but strongly related aspects. The first set shows that fine-tuning both CLIP encoders leads to more efficient use of the embedding spaces. In the second set, we prove that the increased occupation of the image space helps to â€œmove away" the combined features from the non-target features.  
è¿™ä¸¤ç»„å®éªŒçªå‡ºäº†ä¸åŒä½†å¯†åˆ‡ç›¸å…³çš„æ–¹é¢ã€‚ç¬¬ä¸€ç»„å®éªŒè¡¨æ˜ï¼Œå¾®è°ƒä¸¤ä¸ªCLIPç¼–ç å™¨å¯ä»¥æ›´é«˜æ•ˆåœ°åˆ©ç”¨åµŒå…¥ç©ºé—´ã€‚åœ¨ç¬¬äºŒç»„å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†å›¾åƒç©ºé—´çš„å¢åŠ å ç”¨æœ‰åŠ©äºå°†ç»„åˆç‰¹å¾â€œè¿œç¦»â€éç›®æ ‡ç‰¹å¾ã€‚

### 4.9 Qualitative results  

To obtain a clearer understanding of which parts of the images the system considers most important during retrieval, we conducted qualitative experiments using the GradCAM technique [44]. Instead of computing gradients versus an output class, we compute gradients with respect to the combined features, which summarize both the visual and textual content of the image and caption, using the GradCAM technique. This approach makes each heat map generated by GradCAM dependent on the reference image and its relative caption, simulating the retrieval process. We use the last convolutional layer of CLIPâ€™s image encoder as the saliency layer.
ä¸ºäº†æ›´æ¸…æ¥šåœ°äº†è§£ç³»ç»Ÿåœ¨æ£€ç´¢è¿‡ç¨‹ä¸­è®¤ä¸ºå›¾åƒçš„å“ªäº›éƒ¨åˆ†æœ€é‡è¦ï¼Œæˆ‘ä»¬ä½¿ç”¨GradCAMæŠ€æœ¯[44]è¿›è¡Œäº†å®šæ€§å®éªŒã€‚æˆ‘ä»¬æ²¡æœ‰é’ˆå¯¹è¾“å‡ºç±»åˆ«è®¡ç®—æ¢¯åº¦ï¼Œè€Œæ˜¯é’ˆå¯¹ç»„åˆç‰¹å¾è®¡ç®—æ¢¯åº¦ï¼Œè¿™äº›ç»„åˆç‰¹å¾æ€»ç»“äº†å›¾åƒå’Œæ ‡é¢˜çš„è§†è§‰å’Œæ–‡æœ¬å†…å®¹ï¼Œä½¿ç”¨GradCAMæŠ€æœ¯ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—GradCAMç”Ÿæˆçš„æ¯ä¸ªçƒ­å›¾ä¾èµ–äºå‚è€ƒå›¾åƒåŠå…¶ç›¸å…³çš„æ ‡é¢˜ï¼Œæ¨¡æ‹Ÿæ£€ç´¢è¿‡ç¨‹ã€‚æˆ‘ä»¬ä½¿ç”¨CLIPå›¾åƒç¼–ç å™¨çš„æœ€åä¸€ä¸ªå·ç§¯å±‚ä½œä¸ºæ˜¾è‘—æ€§å±‚ã€‚  

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/f7f430122795a08f97b1902d9814427e7f6524f337ec6028e2e0166f48d42f5d.jpg)  
Fig. 9. Examples of GradCAM visualization on CIRR dataset computing the gradients with respect to the Combiner output.  
å›¾9. åœ¨CIRRæ•°æ®é›†ä¸Šè®¡ç®—ç›¸å¯¹äºCombinerè¾“å‡ºçš„æ¢¯åº¦çš„GradCAMå¯è§†åŒ–ç¤ºä¾‹ã€‚

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/dd7af23ee11b255c4fbb38a88dd0b028f7c654bbe1c11bc18fdd18b3b07fba96.jpg)  
Fig. 10. Examples of GradCAM visualization on FashionIQ computing the gradients with respect to the Combiner output.  
å›¾10. åœ¨FashionIQæ•°æ®é›†ä¸Šè®¡ç®—ç›¸å¯¹äºCombinerè¾“å‡ºçš„æ¢¯åº¦çš„GradCAMå¯è§†åŒ–ç¤ºä¾‹ã€‚

In Fig. 9 and in Fig. 10 are displayed some examples of the above-described visualization technique. The system is capable of attending to a wide range of concepts, such as style and color changes for the fashion dataset and behavior modification for the CIRR dataset, as we can notice from the experiments with the GradCAM technique. For instance, in Fig. 9, the system attends to the carriage and horse in the first example, the pose of the holding monkey and the baby monkey in the second example, and the pose of the dog in the third example. In Fig. 10, the system attends to the arms and shoulders of the person when the conditioning text referred to the sleeves of the dress and to the logo of the shirt when it was requested to change the Norwegian flag into a Mexican one.  
åœ¨å›¾9å’Œå›¾10ä¸­å±•ç¤ºäº†ä¸€äº›ä¸Šè¿°å¯è§†åŒ–æŠ€æœ¯çš„ç¤ºä¾‹ã€‚ä»ä½¿ç”¨GradCAMæŠ€æœ¯çš„å®éªŒä¸­æˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ°ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå…³æ³¨å¹¿æ³›çš„æ¦‚å¿µï¼Œä¾‹å¦‚æ—¶å°šæ•°æ®é›†ä¸­çš„é£æ ¼å’Œé¢œè‰²å˜åŒ–ï¼Œä»¥åŠCIRRæ•°æ®é›†ä¸­çš„è¡Œä¸ºä¿®æ”¹ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾9ä¸­ï¼Œç³»ç»Ÿå…³æ³¨ç¬¬ä¸€ä¸ªç¤ºä¾‹ä¸­çš„é©¬è½¦å’Œé©¬åŒ¹ï¼Œç¬¬äºŒä¸ªç¤ºä¾‹ä¸­æŠ±çŒ´çš„å§¿åŠ¿å’Œå°çŒ´ï¼Œä»¥åŠç¬¬ä¸‰ä¸ªç¤ºä¾‹ä¸­ç‹—çš„å§¿åŠ¿ã€‚åœ¨å›¾10ä¸­ï¼Œå½“æ¡ä»¶æ–‡æœ¬æåˆ°è£™å­çš„è¢–å­æ—¶ï¼Œç³»ç»Ÿå…³æ³¨äººçš„æ‰‹è‡‚å’Œè‚©è†€ï¼›å½“è¦æ±‚å°†æŒªå¨å›½æ——æ”¹ä¸ºå¢¨è¥¿å“¥å›½æ——æ—¶ï¼Œç³»ç»Ÿå…³æ³¨è¡¬è¡«çš„æ ‡å¿—ã€‚

Finally, we complete the qualitative analysis of our approach by presenting examples of multimodal queries and their corresponding results on both datasets in Fig. 12 and Fig. 11. In FashionIQ, the correct result is returned most of the time in the first three results, while in CIRR, it is returned in the top-5 global and top-3 subset results. Interestingly, the excellent performance of the proposed system let us notice an issue with the FashionIQ dataset: from these examples, we can see that in the FashionIQ dataset, the existence of many false negatives is a real issue that can harm both the results and the training process; examining the first four and the last queries, we can observe that several results returned in the first positions are corresponding to the conditioning text, although only one of them is marked as such. E.g. in the first query, where several dresses have light floral patterns and bright colors, similarly, the first three results for the shirts should be considered correct. We can also see that in the CIRR dataset, the domain of the images is wider compared to FashionIQ, and the problem of the false negatives is a minor issue.  
æœ€åï¼Œæˆ‘ä»¬é€šè¿‡åœ¨å›¾12å’Œå›¾11ä¸­å±•ç¤ºä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å¤šæ¨¡æ€æŸ¥è¯¢åŠå…¶ç›¸åº”ç»“æœï¼Œå®Œæˆäº†å¯¹æˆ‘ä»¬æ–¹æ³•çš„å®šæ€§åˆ†æã€‚åœ¨FashionIQä¸­ï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹æ­£ç¡®ç»“æœåœ¨å‰ä¸‰ä¸ªç»“æœä¸­è¿”å›ï¼Œè€Œåœ¨CIRRä¸­ï¼Œæ­£ç¡®ç»“æœåœ¨å…¨å±€å‰5å’Œå­é›†å‰3çš„ç»“æœä¸­è¿”å›ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæè®®ç³»ç»Ÿçš„å‡ºè‰²æ€§èƒ½è®©æˆ‘ä»¬æ³¨æ„åˆ°FashionIQæ•°æ®é›†çš„ä¸€ä¸ªé—®é¢˜ï¼šä»è¿™äº›ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°åœ¨FashionIQæ•°æ®é›†ä¸­ï¼Œè®¸å¤šå‡é˜´æ€§çš„å­˜åœ¨æ˜¯ä¸€ä¸ªçœŸæ­£çš„é—®é¢˜ï¼Œå¯èƒ½æŸå®³ç»“æœå’Œè®­ç»ƒè¿‡ç¨‹ï¼›æ£€æŸ¥å‰å››ä¸ªå’Œæœ€åä¸€ä¸ªæŸ¥è¯¢ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œè¿”å›åœ¨ç¬¬ä¸€ä½ç½®çš„å‡ ä¸ªç»“æœä¸æ¡ä»¶æ–‡æœ¬ç›¸å¯¹åº”ï¼Œå°½ç®¡åªæœ‰ä¸€ä¸ªè¢«æ ‡è®°ä¸ºå¦‚æ­¤ã€‚ä¾‹å¦‚ï¼Œåœ¨ç¬¬ä¸€ä¸ªæŸ¥è¯¢ä¸­ï¼Œå¤šä»¶è£™å­å…·æœ‰æµ…è‰²èŠ±å‰å›¾æ¡ˆå’Œé²œè‰³çš„é¢œè‰²ï¼ŒåŒæ ·ï¼Œè¡¬è¡«çš„å‰ä¸‰ä¸ªç»“æœä¹Ÿåº”è§†ä¸ºæ­£ç¡®ã€‚æˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°ï¼Œåœ¨CIRRæ•°æ®é›†ä¸­ï¼Œä¸FashionIQç›¸æ¯”ï¼Œå›¾åƒçš„é¢†åŸŸæ›´å¹¿æ³›ï¼Œå‡é˜´æ€§é—®é¢˜æ˜¯ä¸€ä¸ªè¾ƒå°çš„é—®é¢˜ã€‚

!

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/7728fdfee0be6b1327541a6008d7b9a1e3beb74893b353a1dad2d06b31d7b50e.jpg)  
Fig. 11. Qualitative results for the FashionIQ dataset. We highlight with a green border when the retrieved image is labeled as ground truth for the given query.  
å›¾11. FashionIQæ•°æ®é›†çš„å®šæ€§ç»“æœã€‚æˆ‘ä»¬ç”¨ç»¿è‰²è¾¹æ¡†é«˜äº®æ˜¾ç¤ºæ£€ç´¢åˆ°çš„å›¾åƒè¢«æ ‡è®°ä¸ºç»™å®šæŸ¥è¯¢çš„çœŸå®ç»“æœã€‚

# 5 CONCLUSIONS  

In this work, we propose a novel task-oriented fine-tuning scheme to adapt vision-language models for the composed image retrieval task. The primary goal of this fine-tuning is to address the mismatch between the large-scale pre-training of CLIP and the downstream task, thereby enhancing the additivity properties of the embedding spaces. We then propose a two-stage approach that combines fine-tuning with the training of a carefully crafted Combiner network, enabling the meaningful fusion of the fine-tuned multimodal features. To further enhance performance, we introduce a novel pre-processing padding method, which, as demonstrated in the ablation studies, improves performance on datasets with images of varying aspect ratios. We perform experiments on the challenging fashion dataset FashionIQ and the recently presented CIRR dataset. Experiments on both datasets show that our approach outperforms state-of-the-art methods by a significant margin. We also perform qualitative experiments to explain how our approach works. These experiments investigate the impact of the proposed approach on the feature distribution in the embedding spaces and how the reshaping of such embedding spaces influences retrieval performance. Additionally, we conduct visualization experiments using the gradCAM technique.  
åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„é¢å‘ä»»åŠ¡çš„å¾®è°ƒæ–¹æ¡ˆï¼Œä»¥é€‚åº”è§†è§‰-è¯­è¨€æ¨¡å‹ç”¨äºç»„åˆå›¾åƒæ£€ç´¢ä»»åŠ¡ã€‚è¿™ç§å¾®è°ƒçš„ä¸»è¦ç›®æ ‡æ˜¯è§£å†³CLIPå¤§è§„æ¨¡é¢„è®­ç»ƒä¸ä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´çš„ä¸åŒ¹é…ï¼Œä»è€Œå¢å¼ºåµŒå…¥ç©ºé—´çš„å¯åŠ æ€§ã€‚æˆ‘ä»¬éšåæå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µæ–¹æ³•ï¼Œå°†å¾®è°ƒä¸ç²¾å¿ƒè®¾è®¡çš„Combinerç½‘ç»œè®­ç»ƒç›¸ç»“åˆï¼Œå®ç°å¾®è°ƒåçš„å¤šæ¨¡æ€ç‰¹å¾çš„æœ‰æ„ä¹‰èåˆã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„é¢„å¤„ç†å¡«å……æ–¹æ³•ï¼Œæ­£å¦‚æ¶ˆèç ”ç©¶æ‰€ç¤ºï¼Œè¿™ç§æ–¹æ³•åœ¨å…·æœ‰ä¸åŒçºµæ¨ªæ¯”å›¾åƒçš„æ•°æ®é›†ä¸Šæé«˜äº†æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ—¶å°šæ•°æ®é›†FashionIQå’Œæœ€è¿‘æ¨å‡ºçš„CIRRæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»¥æ˜¾è‘—ä¼˜åŠ¿è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å®šæ€§å®éªŒï¼Œä»¥è§£é‡Šæˆ‘ä»¬çš„æ–¹æ³•æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚è¿™äº›å®éªŒç ”ç©¶äº†æè®®æ–¹æ³•å¯¹åµŒå…¥ç©ºé—´ä¸­ç‰¹å¾åˆ†å¸ƒçš„å½±å“ï¼Œä»¥åŠè¿™ç§åµŒå…¥ç©ºé—´çš„é‡å¡‘å¦‚ä½•å½±å“æ£€ç´¢æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨gradCAMæŠ€æœ¯è¿›è¡Œäº†å¯è§†åŒ–å®éªŒã€‚

# ACKNOWLEDGMENTS  

This work was partially supported by the European Commission under European Horizon 2020 Programme, grant number 101004545 - ReInHerit.  

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/03758f19f18bfc016594c3645d58389dd586ad6f31e2bac2d09a8502dceaa7bd.jpg)  
Fig. 12. Qualitative results for the CIRR dataset. We highlight with a green border when the retrieved image is labeled as ground truth for the given query.  

# REFERENCES  

[1] Sandhini Agarwal, Gretchen Krueger, Jack Clark, Alec Radford, Jong Wook Kim, and Miles Brundage. 2021. Evaluating clip: towards characterization of broader capabilities and downstream implications. arXiv preprint arXiv:2108.02818 (2021).   
[2] Jamil Ahmad, Khan Muhammad, Sambit Bakshi, and Sung Wook Baik. 2018. Object-oriented convolutional features for fine-grained image retrieval in large surveillance datasets. Future Generation Computer Systems 81 (2018), 314â€“330.   
[3] Muhammad Umer Anwaar, Egor Labintcev, and Martin Kleinsteuber. 2021. Compositional Learning of Image-Text Query for Image Retrieval. In Proc. of IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). 1140â€“1149.   
[4] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Alberto Del Bimbo. 2022. Exploiting CLIP-Based Multi-modal Approach for Artwork Classification and Retrieval. In The Future of Heritage Science and Technologies: ICT and Digital Heritage: Third International Conference, Florence Heri-Tech 2022, Florence, Italy, May 16â€“18, 2022, Proceedings. Springer, 140â€“149.   
[5] Imon Banerjee, Camille Kurtz, Alon Edward Devorah, Bao Do, Daniel L Rubin, and Christopher F Beaulieu. 2018. Relevance feedback for enhancing content based image retrieval and automatic prediction of semantic image features: Application to bone tumor radiographs. Journal of biomedical informatics 84 (2018), 123â€“135.   
[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877â€“1901.   
[7] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174 (2016).   
[8] Yanbei Chen, Shaogang Gong, and Loris Bazzani. 2020. Image Search With Text Feedback by Visiolinguistic Attention Learning. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR). [9] Ruizhe Cheng, Bichen Wu, Peizhao Zhang, Peter Vajda, and Joseph E. Gonzalez. 2021. Data-Efficient Language-Supervised Zero-Shot Learning With Self-Distillation. In Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. 3119â€“3124.   
[10] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014).   
[11] Mario GCA Cimino, Federico A Galatolo, and Gigliola Vaglini. 2021. Generating Images from Caption and Vice Versa via CLIP-Guided Generative Latent Space Search. In Proceedings of the International Conference on Image Processing and Vision Engineering. 166â€“174.   
[12] Claudia Companioni-Brito, Zygred Mariano-Calibjio, Mohamed Elawady, and Sule Yildirim. 2018. Mobile-Based Painting Photo Retrieval Using Combined Features. In Proc. of International Conference on Image Analysis and Recognition (ICIAR), Vol. 10882. Springer, 278.   
[13] Marcos V Conde and Kerem Turgutlu. 2021. CLIP-Art: Contrastive Pre-Training for Fine-Grained Art Classification. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR). 3956â€“3960.   
[14] Ginger Delmas, Rafael S Rezende, Gabriela Csurka, and Diane Larlus. 2021. ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity. In International Conference on Learning Representations.   
[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 4171â€“4186.   
[16] Eric Dodds, Jack Culpepper, Simao Herdade, Yang Zhang, and Kofi Boakye. 2020. Modality-agnostic attention fusion for visual search with text feedback. arXiv preprint arXiv:2007.00145 (2020).   
[17] Xiao Dong, Xunlin Zhan, Yangxin Wu, Yunchao Wei, Xiaoyong Wei, Minlong Lu, and Xiaodan Liang. 2021. M5product: A multi-modal pretraining benchmark for e-commercial product downstream tasks. arXiv preprint arXiv:2109.04275 (2021).   
[18] Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen. 2021. CLIP2Video: Mastering Video-Text Retrieval via Image CLIP. arXiv preprint arXiv:2106.11097 (2021).   
[19] Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, and Rogerio Feris. 2018. Dialog-based interactive image retrieval. Advances in neural information processing systems 31 (2018).   
[20] Xintong Han, Zuxuan Wu, Phoenix X Huang, Xiao Zhang, Menglong Zhu, Yuan Li, Yang Zhao, and Larry S Davis. 2017. Automatic spatially-aware fashion concept discovery. In Proceedings of the IEEE international conference on computer vision. 1463â€“1471.   
[21] Xiao Han, Licheng Yu, Xiatian Zhu, Li Zhang, Yi-Zhe Song, and Tao Xiang. 2022. Fashionvil: Fashion-focused vision-and-language representation learning. In European Conference on Computer Vision. Springer, 634â€“651.   
[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770â€“778.   
[23] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735â€“1780.   
[24] Bogdan Ionescu, Henning MÃ¼ller, Renaud PÃ©teri, Yashin Dicente Cid, Vitali Liauchuk, Vassili Kovalev, Dzmitri Klimuk, Aleh Tarasau, Asma Ben Abacha, Sadid A Hasan, et al. 2019. ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature. In Proc. of International Conference of the Cross-Language Evaluation Forum for European Languages (CLEF). Springer, 358â€“386.   
[25] Bogdan Ionescu, Henning MÃ¼ller, Renaud PÃ©teri, Duc-Tien Dang-Nguyen, Liting Zhou, Luca Piras, Michael Riegler, PÃ¥l Halvorsen, Minh-Triet Tran, Mathias Lux, et al. 2020. ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications. Advances in Information Retrieval 12036 (2020), 533.   
[26] Surgan Jandial, Pinkesh Badjatiya, Pranit Chawla, Ayush Chopra, Mausoom Sarkar, and Balaji Krishnamurthy. 2022. SAC: Semantic Attention Composition for Text-Conditioned Image Retrieval. In Proc. of IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). 4021â€“4030.   
[27] Surgan Jandial, Ayush Chopra, Pinkesh Badjatiya, Pranit Chawla, Mausoom Sarkar, and Balaji Krishnamurthy. 2020. Trace: Transform aggregate and compose visiolinguistic representations for image search with text feedback. arXiv preprint arXiv:2009.01485 7 (2020), 7.   
[28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In Proc. of International Conference on Machine Learning (ICML).   
[29] Jongseok Kim, Youngjae Yu, Hoeseong Kim, and Gunhee Kim. 2021. Dual Compositional Learning in Interactive Image Retrieval. In Proc. of AAAI Conference on Artificial Intelligence (AAAI), Vol. 35. 1771â€“1779. https://ojs.aaai.org/index.php/AAAI/article/view/16271   
[30] Adriana Kovashka, Devi Parikh, and Kristen Grauman. 2015. WhittleSearch: Interactive Image Search with Relative Attribute Feedback. International Journal of Computer Vision (IJCV) 115, 2 (Apr 2015), 185â€“210. https://doi.org/10.1007/s11263-015-0814-0   
[31] Seungmin Lee, Dongwan Kim, and Bohyung Han. 2021. CoSMo: Content-Style Modulation for Image Retrieval With Text Feedback. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR). 802â€“812.   
[32] Manling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji, and Shih-Fu Chang. 2022. Clip-event: Connecting text and images with event structures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 16420â€“16429.   
[33] Xiaoqing Li, Jiansheng Yang, and Jinwen Ma. 2021. Recent developments of content-based image retrieval (CBIR). Neurocomputing 452 (2021), 675â€“689. https://doi.org/10.1016/j.neucom.2020.07.139   
[34] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. 2020. Oscar: Object-semantics aligned pre-training for vision-language tasks. In Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part XXX 16. Springer, 121â€“137.   
[35] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y Zou. 2022. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. Advances in Neural Information Processing Systems 35 (2022), 17612â€“17625.   
[36] Yating Liu and Yan Lu. 2021. Multi-grained Fusion for Conditional Image Retrieval. In Proc. of International Conference on Multimedia Modeling (MMM). Springer International Publishing, Cham, 315â€“327.   
[37] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. 2021. Image retrieval on real-life images with pre-trained vision-and-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2125â€“2134.   
[38] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In International Conference on Learning Representations. https://openreview.net/forum?id=Bkg6RiCqY7   
[39] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. 2018. Mixed Precision Training. In International Conference on Learning Representations.   
[40] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 1532â€“1543.   
[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 8748â€“8763.   
[42] Yong Rui, T.S. Huang, M. Ortega, and S. Mehrotra. 1998. Relevance feedback: a power tool for interactive content-based image retrieval. IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) 8, 5 (1998), 644â€“655. https://doi.org/10.1109/76.718510   
[43] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. 2015. Imagenet large scale visual recognition challenge. International journal of computer vision 115 (2015), 211â€“252.   
[44] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2019. Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization. International Journal of Computer Vision (IJCV) 128, 2 (Oct 2019), 336â€“359. https://doi.org/10.1007/s11263-019-01228-7   
[45] Minchul Shin, Yoonjae Cho, Byungsoo Ko, and Geonmo Gu. 2021. RTIC: Residual Learning for Text and Image Composition using Graph Convolutional Network. arXiv preprint arXiv:2104.03015 (2021).   
[46] Arnold WM Smeulders, Marcel Worring, Simone Santini, Amarnath Gupta, and Ramesh Jain. 2000. Content-based image retrieval at the end of the early years. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 22, 12 (2000), 1349â€“1380.   
[47] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. 2019. A Corpus for Reasoning about Natural Language Grounded in Photographs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 6418â€“6428.   
[48] Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. 2019. Composing text and image for image retrieval-an empirical odyssey. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 6439â€“6448.   
[49] Zhecan Wang, Noel Codella, Yen-Chun Chen, Luowei Zhou, Jianwei Yang, Xiyang Dai, Bin Xiao, Haoxuan You, Shih-Fu Chang, and Lu Yuan. 2022. CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks. arXiv preprint arXiv:2201.05729 (2022). arXiv:2201.05729 [cs.CV]   
[50] Haokun Wen, Xuemeng Song, Xin Yang, Yibing Zhan, and Liqiang Nie. 2021. Comprehensive Linguistic-Visual Composition Network for Image Retrieval. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR â€™21). Association for Computing Machinery, New York, NY, USA, 1369â€“1378. https://doi.org/10.1145/ 3404835.3462967   
[51] Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogerio Feris. 2021. Fashion iq: A new dataset towards retrieving images by natural language feedback. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition. 11307â€“11317.   
[52] Youngjae Yu, Seunghwan Lee, Yuncheol Choi, and Gunhee Kim. 2020. Curlingnet: Compositional learning between images and text for fashion iq data. arXiv preprint arXiv:2003.12299 (2020).   
[53] Yifei Yuan and Wai Lam. 2021. Conversational Fashion Image Retrieval via Multiturn Natural Language Feedback. In Proc. of International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). ACM. https://doi.org/10.1145/3404835.3462881   
[54] Xunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Minlong Lu, Yichi Zhang, Hang Xu, and Xiaodan Liang. 2021. Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-Modal Pretraining. In Proc. of IEEE/CVF International Conference on Computer Vision (ICCV). 11782â€“11791.   
[55] Bo Zhao, Jiashi Feng, Xiao Wu, and Shuicheng Yan. 2017. Memory-Augmented Attribute Manipulation Networks for Interactive Fashion Search. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR). 6156â€“6164. https://doi.org/10.1109/CVPR.2017.652   
[56] Liang Zheng, Yi Yang, and Qi Tian. 2017. SIFT meets CNN: A decade survey of instance retrieval. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 40, 5 (2017), 1224â€“1244.  