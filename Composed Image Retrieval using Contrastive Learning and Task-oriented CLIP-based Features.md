# Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features  

ALBERTO BALDRATI, Università degli Studi di Firenze - MICC, Italy and Università di Pisa, Italy   
MARCO BERTINI, Università degli Studi di Firenze - MICC, Italy   
TIBERIO URICCHIO, Università degli Studi di Macerata, Italy   
ALBERTO DEL BIMBO, Università degli Studi di Firenze - MICC, Italy  

Given a query composed of a reference image and a relative caption, the Composed Image Retrieval goal is to retrieve images visually similar to the reference one that integrates the modifications expressed by the caption. Given that recent research has demonstrated the efficacy of large-scale vision and language pre-trained (VLP) models in various tasks, we rely on features from the OpenAI CLIP model to tackle the considered task. We initially perform a task-oriented fine-tuning of both CLIP encoders using the element-wise sum of visual and textual features. Then, in the second stage, we train a Combiner network that learns to combine the image-text features integrating the bimodal information and providing combined features used to perform the retrieval. We use contrastive learning in both stages of training. Starting from the bare CLIP features as a baseline, experimental results show that the task-oriented fine-tuning and the carefully crafted Combiner network are highly effective and outperform more complex state-of-the-art approaches on FashionIQ and CIRR, two popular and challenging datasets for composed image retrieval. Code and pre-trained models are available at https://github.com/ABaldrati/CLIP4Cir  

CCS Concepts: $\bullet$ Computing methodologies $\rightarrow$ Image representations; Visual content-based indexing and retrieval.  

Additional Key Words and Phrases: multimodal retrieval, combiner networks, vision language model  

# ACM Reference Format:  

Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Alberto Del Bimbo. 2023. Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features. ACM Trans. Multimedia Comput. Commun. Appl. 1, 1 (August 2023), 23 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn  

# 1 INTRODUCTION  

Content-Based Image Retrieval (CBIR) is a fundamental task in multimedia and computer vision which has undergone a continuous evolution since its early years [46], moving from the use of engineered features like SIFT to CNNs [33, 56]. It has been applied to many different specialized domains like artworks and cultural heritage [4, 12], commerce [17, 54], surveillance [2], nature [24, 25]. In the basic form, the query is composed of only an image, of which features are computed and compared with the ones extracted by a database of images.  

We can extend CBIR systems to improve their effectiveness by adding additional information to the query image. For example, interactive image retrieval systems extend CBIR systems by adding some form of user feedback, e.g. to provide some measure of relevance [5]. In composed image retrieval, the visual query is extended to an image-language pair [37] where a short textual description, typically expressed in natural language, may request constraints and desired changes or add specifications on some attributes of the retrieved results [26]. Figure 1 illustrates two examples of this task. In both queries, a user selects a reference image and then provides additional requests in the form of text, e.g. asking to change details, texture, color, or shape features of the reference image. Composed image retrieval systems find applications in various domains such as web search, e-commerce, and surveillance. However, developing solutions for this task can be challenging due to the need for incorporating feedback and user intent while addressing the semantic gap between image and text content.  

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/1fa5779a2b881daed69777b3b89739fd264cd1d10a840eb93be435af547a68ee.jpg)  
Fig. 1. The left portion of the illustration depicts a specific case of composed image retrieval in the fashion domain, where the user imposes constraints on the character attribute of a t-shirt. Meanwhile, the right part showcases an example where the user asks to alter objects and their cardinality within a real-life image.  

Very recently, researchers proved that deep neural networks combining visual and language modalities like CLIP [41], ALIGN [28], and the more recent method proposed in [9], trained using an image-caption alignment objective on large-scale internet data, can obtain impressive zero-shot transfer on a myriad of downstream tasks like image classification, text-based image retrieval, and object detection.  

In this work, we show that features obtained from vision and language pretrained (VLP) models – we employed CLIP-based features – can be effectively used to implement a composed image retrieval system where user feedback is provided as natural language input to provide additional (or contrasting) requirements concerning those embedded in the visual features of the image used to query the system. Firstly, we apply the system to the fashion domain, performing experiments on the challenging FashionIQ dataset [51]. Then, to study the generalization capabilities to a broader image domain, we perform experiments on the newly introduced CIRR dataset [37]. Experiments show that the proposed approach obtains state-of-the-art results on both datasets.  

To summarize, we highlight our main contributions as follows:  

• We propose a novel task-oriented fine-tuning scheme for adapting vision-language models to the composed image retrieval task. The aim of such a task-oriented adaptation scheme is to reduce the mismatch between the large-scale pre-training and the downstream task.   
• We propose a novel two-stage approach that combines task-oriented fine-tuning with the training of a Combiner network which can perform a fine-grained merging of the multimodal features. This two-stage approach achieves state-of-the-art results on two standard and challenging datasets: FashionIQ and CIRR.   
• We address the issue of using the CLIP model with images having a high aspect ratio since the CLIP visual encoder can input only square pictures. We propose a novel preprocess pipeline suited for image retrieval tasks that helps to reduce content information loss compared to the standard CLIP preprocess pipeline.   
• To provide further insight into the workings of our proposed system, we perform several qualitative experiments. The first experiment aims to demonstrate how our approach affects the feature distribution in the embedding spaces and the impact of pairwise feature distances on retrieval performance. Additionally, we report visualization experiments utilizing the gradCAM technique [44] to gain a deeper understanding of the image portions that are most significant during retrieval.  

# 2 RELATED WORKS  

Traditional CBIR does not use user feedback or its intent to refine results. However, within interactive and composed CBIR, much work has been done to improve retrieval performance by incorporating user’s feedback in terms of relevance to the query [42] or by considering relative [30] and absolute attributes [20, 55]. The limiting expressiveness of attributes was successively addressed in [19, 48] by considering purely textual feedback, allowing richer expressiveness. Nonetheless, the performance of the textual model can limit the system in understanding and reacting appropriately.  

## Visual and language pre-training  

Models like GPT-2, BERT [15] and GPT-3 [6] have shown that large amounts of text combined with recent improvements in attention mechanisms enable learning of powerful features that integrate vast knowledge. Adding images to the learning process, CLIP [41] has very recently shown that it is feasible to perform multimodal zero-shot learning, obtaining remarkable feature generalization of both images and text. CLIP is a deep neural network trained to predict the association between text snippets and paired images. Unlike standard vision models trained on specific datasets that are typically good at only one task, this new class of models learns associations between images and natural language supervision that are widely available on the internet. They are not directly optimized for a benchmark and yet can perform consistently well on different tasks. CLIP effectiveness is still subject of study [1], with first applications to art [13], image generation [11] and zero-shot video retrieval [18], event classification [32], visual commonsense reasoning [49]. Our work builds upon CLIP and further explores its potential in the composed image retrieval task, applying the proposed approach to a specific domain, i.e. fashion, and also to general images. ALIGN [28] uses a dual-encoder architecture to learn the alignment of visual and language representations of image and text pairs using a contrastive loss in a noisy dataset. The extremely large scale of such a dataset, composed of 1 billion pairs, twice the size of the CLIP training dataset, makes up for its noise and leads to state-of-the-art representations even using such a simple learning scheme. Differently from CLIP and ALIGN, the authors in [9] propose a data-efficient contrastive distillation method that learns from a training dataset that is $133\times$ smaller than the one used by CLIP (400 million pairs), using a ResNet50 image encoder and DeCLUTR text encoder.  

## Composed image retrieval  

In the growing area of image retrieval with user feedback that combines images and text, our work relates to two recently introduced datasets that address the composed image retrieval task: i) FashionIQ, a fashion image retrieval with text [51], and with ii) the very recent composed image retrieval of generic images introduced in [37]. In [8], a transformer that can be seamlessly plugged into a CNN to selectively preserve and transform the visual features conditioned on language semantics is presented. Text Image Residual Gating (TIRG) [48] combines image and text features using gating and residual features. The authors of [45] leverage skip connections by combining them with graph neural networks, resulting in improved performance. The authors of [31] employ two different neural network modules to address image style and content. In [29], the authors present a Correction Network which explicitly models the difference between the reference and target image in the embedding space. In [37], a new dataset (CIRR) for composed image retrieval on real-world images is proposed, along with a novel transformer-based model that uses rich pre-trained vision-and-language knowledge, called CIRPLANT, to modify visual features conditioned on natural language. CIRPLANT leverages visual-and-language pre-trained models in composed image retrieval: the OSCAR model [34] is carefully adapted to the task with promising results. In [16], the authors proposed the Modality-Agnostic Attention Fusion (MAAF) model to tackle the composed image retrieval task. The model treats the convolutional spatial image features and learned text embeddings as modality-agnostic tokens and passes them to a Transformer for further processing. In [36], the authors propose a Multi-Grained Fusion (MGF) module which fuses features at different stages. ComposeAE [3] is an autoencoder-based model that learns the composition of image and text features for retrieving images by adopting a deep metric learning (DML) approach instead of fusing them by passing through a few fully connected layers. CurlingNet, proposed in [52], measures the semantic differential relationships between images concerning a conditioning query text. The main components are two networks: the first one, called the Delivery filter, delivers the source image to the candidate cluster according to a given query in embedding space, while the second one, called the Sweeping filter, checks the attributes highlighted in the query and learns the path from the center of valid target candidates to the target image. In [53], the composed image retrieval task is extended to a multi-turn conversation. The authors proposed a system that utilizes ComposeAE [3] to combine image and text at each turn. The combined representation is then fed into a recurrent network, following the turn order, for further processing. In [26], the authors present the SAC (Semantic Attention Composition) framework, which consists of two modules: the Semantic Feature Attention (SFA) module finds the salient regions of the image w.r.t. the text, and then the Semantic Feature Modification (SFM) module determines how to change the relevant parts of the image compositing coarse and fine salient image features computed by SFA with text embeddings.  

The proposed method starts with the hypothesis of having a unified embedding space for images and text achieved through the Vision-Language model CLIP. In the first stage, we fine-tune both CLIP encoders to adapt them to the composed image retrieval task. Next, using the task-adapted embedding spaces, we train a Combiner network to merge the multimodal features. In contrast to fashion-oriented approaches like [8, 31], our method does not rely on spatial features. Instead, we argue that when considering images of a broader domain, the semantics hold greater significance than local visual aspects.  

## 3 THE PROPOSED METHOD  

The proposed approach addresses the multimodal task of composed image retrieval. The input query consists of a reference image $I_{q}$ (e.g., an image of a black shirt with a cartoon lion) and a relative caption $T_{q}$ that includes a descriptive request from the user about the image (e.g., "has dog print and is dark grey color"). The goal is to retrieve target images that satisfy similarity constraints imposed by both the input components (e.g., an image of a dark grey shirt with a dog print, as shown in Fig. 3). For a successful retrieval, the system should understand the semantics of the image and the meaning of the text, integrate the multi-domain information, and then use the fused representation to retrieve the relevant images.  
所提出的方法解决了组合图像检索的多模态任务。输入查询由参考图像$I_{q}$（例如，一件带有卡通狮子的黑色衬衫的图像）和包含用户关于图像的描述性请求的相对说明$T_{q}$（例如，"有狗图案且是深灰色"）组成。目标是检索满足两个输入组件所施加的相似性约束的目标图像（例如，一件带有狗图案的深灰色衬衫，如图3所示）。为了成功检索，系统应理解图像的语义和文本的含义，整合多领域信息，然后使用融合表示来检索相关图像。

In contrast to previous works like [8, 29, 31, 45] that build from different image and textual models, we start from the hypothesis of having a unified embedding of images and text, obtained through using the CLIP model [41]. CLIP is a vision-language model trained to align images and their corresponding text captions in a unified embedding space. It consists of an image encoder $\psi_{I}$ and a text encoder $\psi_{T}$ . Given an image $I_{i}$ , the image encoder extracts a feature representation $\psi_{I}(I)\in\mathbb{R}^{d}$ , where $d$ is the size of the CLIP embedding space. Similarly, for a given text caption $T$ , the text encoder extracts a feature representation $\psi_{T}(T)\in\mathbb{R}^{d}$ . CLIP learns to map similar concepts expressed in images and text to similar feature representations. For instance, given the image of a cat $I_{c}$ and the text $T_{c}$ “a photo of a cat", the way CLIP is trained should guarantee that $\psi_{I}(I_{c})\approx\psi_{T}(T_{c})$ .  
与像[8, 29, 31, 45]这样从不同图像和文本模型构建的先前工作不同，我们从假设拥有通过使用CLIP模型[41]获得的图像和文本的统一嵌入开始。CLIP是一种视觉-语言模型，经过训练可以在统一的嵌入空间中对齐图像及其相应的文本标题。它由图像编码器$\psi_{I}$和文本编码器$\psi_{T}$组成。给定图像$I_{i}$，图像编码器提取特征表示$\psi_{I}(I)\in\mathbb{R}^{d}$，其中$d$是CLIP嵌入空间的大小。类似地，对于给定的文本标题$T$，文本编码器提取特征表示$\psi_{T}(T)\in\mathbb{R}^{d}$。CLIP学习将图像和文本中表达的相似概念映射到相似的特征表示。例如，给定猫的图像$I_{c}$和文本$T_{c}$"猫的照片"，CLIP的训练方式应该保证$\psi_{I}(I_{c})\approx\psi_{T}(T_{c})$。

We argue that, even though having a unified embedding space is a good starting point, it is not exactly what we need in the task we are considering. In composed image retrieval, the goal is to move from the reference to the target image point in the image embedding space with the aid of textual information. Hence, instead of utilizing a unified image-text embedding space, our approach involves creating two separate embedding spaces that can be combined through a sum operation. Formally, given an image of a black dress $I_{x}$ and the corresponding text $T_{y}$ ("is blue"). Let $I_{z}$ represent the image of a blue dress. Our aim is to shape the embedding spaces such that $\psi_{I}(I_{x})+\psi_{T}(T_{y})\approx\psi_{I}(I_{z})$ . When this equation is satisfied, we can affirm that the textual embedding space exhibits strong "additivity properties" in relation to the image space, or equivalently, the embedding spaces are additive.  
我们认为，尽管拥有统一的嵌入空间是一个良好的起点，但这并不完全符合我们所考虑任务的需求。在组合图像检索中，目标是在文本信息的帮助下，在图像嵌入空间中从参考图像点移动到目标图像点。因此，我们的方法不是利用统一的图像-文本嵌入空间，而是创建两个可以通过求和操作组合的独立嵌入空间。形式上，给定一张黑色连衣裙的图像$I_{x}$和相应的文本$T_{y}$（"是蓝色的"）。让$I_{z}$表示蓝色连衣裙的图像。我们的目标是塑造嵌入空间，使得$\psi_{I}(I_{x})+\psi_{T}(T_{y})\approx\psi_{I}(I_{z})$。当这个等式成立时，我们可以确认文本嵌入空间相对于图像空间表现出强烈的"加性属性"，或者等效地说，嵌入空间是可加的。

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/46dca4d6af1dea2eef6c5c64f6fb139f75a8787e5e1988525686d61ff3c03c53.jpg)  

Fig. 2. First stage of training. In this stage, we perform a task-oriented fine-tuning of CLIP encoders to reduce the mismatch between the large-scale pre-training and the downstream task. We start by extracting the image-text query features and combining them through an element-wise sum. We then employ a contrastive loss to minimize the distance between combined features and target image features in the same triplet and maximize the distance from the other images in the batch. We update the weights of both CLIP encoders.  
图2. 训练的第一阶段。在这个阶段，我们对CLIP编码器进行面向任务的微调，以减少大规模预训练与下游任务之间的不匹配。我们首先提取图像-文本查询特征，并通过逐元素求和将它们组合起来。然后，我们采用对比损失来最小化组合特征与同一三元组中目标图像特征之间的距离，并最大化与批次中其他图像的距离。我们更新两个CLIP编码器的权重。

Ideally, the embeddings of the relative caption should correspond to the displacement vector from the query image to the target image features, i.e. $\psi_{T}(T_{y})\approx\psi_{I}(I_{z})-\psi_{I}(I_{x})$ .  

We propose a two-stage approach to address the task of composed image retrieval by taking full advantage of the capabilities of CLIP’s features. In the first stage, we tackle the objective mismatch between the large-scale pretraining of CLIP and the downstream task: we propose a novel fine-tuning scheme tailored to improving the additivity properties of the embedding spaces. In the second stage, starting from the task-oriented features, we train from scratch a Combiner neural network that learns to perform a fine-grained combination of image-text features. Although we train the Combiner network from scratch, we design its structure to take full advantage of the first stage of training (see Section 3.2 for more details). During both stages the training is performed using triplets $(I_{q},T_{q},I_{t})$ , where $q=(I_{q},T_{q})$ is the query and $I_{t}$ is the target image that we aim to retrieve given $q$ .  
理想情况下，相对说明的嵌入应该对应于从查询图像到目标图像特征的位移向量，即$\psi_{T}(T_{y})\approx\psi_{I}(I_{z})-\psi_{I}(I_{x})$。

我们提出了一种两阶段方法来解决组合图像检索任务，充分利用CLIP特征的能力。在第一阶段，我们解决CLIP大规模预训练与下游任务之间的目标不匹配问题：我们提出了一种新颖的微调方案，旨在改善嵌入空间的加性属性。在第二阶段，从面向任务的特征开始，我们从头开始训练一个Combiner神经网络，学习执行图像-文本特征的精细组合。尽管我们从头开始训练Combiner网络，但我们设计其结构以充分利用第一阶段的训练（详见3.2节）。在两个阶段中，训练都使用三元组$(I_{q},T_{q},I_{t})$进行，其中$q=(I_{q},T_{q})$是查询，$I_{t}$是给定$q$我们希望检索的目标图像。

At inference time, given a query $(I_{q},T_{q})$ , we utilize the fine-tuned CLIP encoders and the trained Combiner network to generate the combined features. Subsequently, following the standard image-to-image retrieval approach, we compute the cosine distances between the combined features and the database of index image features. The results are then sorted based on their similarity.  
在推理阶段，给定查询$(I_{q},T_{q})$，我们利用微调后的CLIP编码器和训练好的Combiner网络生成组合特征。随后，按照标准的图像到图像检索方法，我们计算组合特征与索引图像特征数据库之间的余弦距离。结果然后根据相似度进行排序。

## 3.1 Task-oriented fine-tuning  

In this stage, we adapt both CLIP’s encoders to composed image retrieval reducing the mismatch between the large-scale pre-training and the downstream task. Given a query consisting of a reference image $I_{q}$ and a relative caption $T_{q}$ , we extract their feature representations using the CLIP image encoder $\psi_{I}$ and text encoder $\psi_{T}$ respectively. This results in $\psi_{I}(I_{q})\in\mathbb{R}^{d}$ and $\psi_{T}(T_{q})\in\mathbb R^{d}$ , where $d$ denotes the size of the CLIP embedding space. To combine the query features, we perform an element-wise sum, resulting in $\phi_{q}=\psi_{I}(I_{q})+\psi_{T}(T_{q})$ .  
在这个阶段，我们将两个CLIP编码器适配到组合图像检索任务，减少大规模预训练与下游任务之间的不匹配。给定一个由参考图像$I_{q}$和相对说明$T_{q}$组成的查询，我们分别使用CLIP图像编码器$\psi_{I}$和文本编码器$\psi_{T}$提取它们的特征表示。这将得到$\psi_{I}(I_{q})\in\mathbb{R}^{d}$和$\psi_{T}(T_{q})\in\mathbb R^{d}$，其中$d$表示CLIP嵌入空间的大小。为了组合查询特征，我们执行逐元素求和，得到$\phi_{q}=\psi_{I}(I_{q})+\psi_{T}(T_{q})$。

Our objective is to minimize the distance between the query combined features $\phi_{q}$ and the target image features $\phi_{t}=\psi_{I}(I_{t})$ belonging to the same triplet and, at the same time, maximize the distance from the other target images in the same batch. To this end, following [31, 45, 48], we employ a batch-based contrastive loss:  
我们的目标是最小化查询组合特征$\phi_{q}$与属于同一三元组的目标图像特征$\phi_{t}=\psi_{I}(I_{t})$之间的距离，同时最大化与同一批次中其他目标图像的距离。为此，参考[31, 45, 48]，我们采用基于批次的对比损失：

$$
\mathcal{L}_{c o n t r}=\frac{1}{B}\sum_{i=1}^{B}-\log\frac{\exp\{\tau*\kappa(\phi_{q}^{i},\phi_{t}^{i})\}}{\sum_{j=1}^{B}\exp\{\tau*\kappa(\phi_{q}^{i},\phi_{t}^{j})\}}
$$  

Here, $\kappa(\cdot)$ denotes the cosine similarity, $\tau$ is a temperature parameter that controls the range of the logits, and $B$ is the number of images in a batch. We update the weights of both CLIP encoders. We use this loss because, being a batch-wise contrastive loss, it does not require the definition of a sampling strategy: it considers all negative samples in a mini-batch. Figure 2 shows an overview of the task-oriented fine-tuning stage.  
这里，$\kappa(\cdot)$表示余弦相似度，$\tau$是控制logits范围的温度参数，$B$是批次中的图像数量。我们更新两个CLIP编码器的权重。我们使用这种损失函数是因为，作为一种基于批次的对比损失，它不需要定义采样策略：它考虑了小批次中的所有负样本。图2展示了面向任务的微调阶段的概览。

Using the element-wise sum as the combination of query features goes in the direction of making CLIP’s embedding spaces more additive. Consequently, similar concepts expressed in text and images no longer share similar features. Instead, the textual features serve as displacement vectors from the query to the target in the image space. From a high-level perspective, we notice that, in composed image retrieval, the image and the text do not play the same role. The task is not symmetric with respect to the input: we start from an image, and we would like to retrieve another image using textual guidance. For this reason, the break up of the unified embedding space is not an undesirable side-effect.  
使用逐元素求和作为查询特征的组合方式有助于使CLIP的嵌入空间更具加性。因此，文本和图像中表达的相似概念不再共享相似特征。相反，文本特征作为图像空间中从查询到目标的位移向量。从高层次的角度来看，我们注意到，在组合图像检索中，图像和文本不扮演相同的角色。该任务相对于输入不是对称的：我们从一个图像开始，希望使用文本指导检索另一个图像。因此，统一嵌入空间的分解并不是一个不良的副作用。

我们将微调后的图像编码器和文本编码器分别表示为 $\overline{{\psi_{I}}}$和$\overline{{\psi_{T}}}$。
We will denote the fine-tuned image encoder and text encoder as $\overline{{\psi_{I}}}$ and $\overline{{\psi_{T}}}$ , respectively.  

## 3.2 Combiner training  

During the training of the Combiner network, we follow the same general framework as in the previous stage. However, this time we train from scratch the Combiner network instead of updating the weights of the CLIP encoders. In contrast to the first stage, we use the Combiner network $C_{\theta}$ to combine the query features. Specifically, the combined features are obtained as $\overline{{\phi_{q}}}=C_{\theta}(\overline{{\psi_{I}}}(I_{q}),\overline{{\psi_{T}}}(T_{q}))$ . We optimize the Combiner network by utilizing the $\mathcal{L}_{c o n t r}$ loss described in Eq. (1) with $\overline{{\phi_{q}}}$ and $\overline{{\phi_{t}}}=\overline{{\psi_{T}}}(I_{t})$ as inputs. Figure 3 depicts a visual overview of the Combiner network training stage. By employing the contrastive loss, we train the Combiner $C_{\theta}$ to produce features as close as possible to the target features and as far away as possible from all other image features.  
在Combiner网络的训练过程中，我们遵循与前一阶段相同的总体框架。然而，这次我们从头开始训练Combiner网络，而不是更新CLIP编码器的权重。与第一阶段不同，我们使用Combiner网络$C_{\theta}$来组合查询特征。具体来说，组合特征通过$\overline{{\phi_{q}}}=C_{\theta}(\overline{{\psi_{I}}}(I_{q}),\overline{{\psi_{T}}}(T_{q}))$获得。我们利用公式(1)中描述的$\mathcal{L}_{contr}$损失来优化Combiner网络，将$\overline{{\phi_{q}}}$和$\overline{{\phi_{t}}}=\overline{{\psi_{T}}}(I_{t})$作为输入。图3展示了Combiner网络训练阶段的整体视图。通过采用对比损失，我们训练Combiner $C_{\theta}$生成尽可能接近目标特征、同时远离其他图像特征的特征表示。

The Combiner network, depicted in Fig. 4, is designed to take full advantage of the first stage of training and the increased additivity properties of the adapted embedding spaces. The idea is to learn the residual of a convex combination of the image-text query features. We begin by projecting the text and image features through a linear transformation followed by a ReLU function. The resulting projected features are then concatenated and passed to two separate branches. The first branch, labeled as (1) in Fig. 4, is responsible for computing the coefficients of a convex combination between the image and text features. To compute these coefficients, we feed the concatenated features into a linear layer, followed by the ReLU function, another linear layer, and the sigmoid function. The sigmoid output provides the coefficients needed for the query image-text convex combination. The second branch, labeled as (2), outputs the mixture contribution of the image and text features. The structure of this branch is the same as the first branch, except it does not include the final sigmoid function. Finally, we sum the convex combination of the query features and the learned image-text mixture. To reduce overfitting, we apply dropout to each layer.  
如图4所示的Combiner网络，其设计旨在充分利用第一阶段的训练成果和适配后嵌入空间增强的加性属性。核心思想是学习图像-文本查询特征凸组合的残差。我们首先通过线性变换和ReLU函数对文本和图像特征进行投影。将投影后的特征拼接后，传递至两个独立的分支。图4中标记为(1)的第一个分支负责计算图像与文本特征之间凸组合的系数。为了计算这些系数，我们将拼接特征输入线性层，随后经过ReLU函数、另一个线性层和sigmoid函数。sigmoid输出提供查询图像-文本凸组合所需的系数。标记为(2)的第二个分支输出图像和文本特征的混合贡献。该分支结构与第一个分支相同，但不包含最终的sigmoid函数。最后，我们对查询特征的凸组合与学习到的图像-文本混合结果进行求和。为减少过拟合，我们在每个层应用dropout。

By denoting the outputs of the first branch (1) as $\lambda$ and $1-\lambda$ , and the output of the second branch (2) as $v$ , we can express the combined features as $\overline{{{\phi_{q}}}}=\left(1-\lambda\right)*\overline{{{\psi_{I}}}}(I_{q})+\lambda*\overline{{{\psi_{T}}}}(T_{q})+v$ . Notably, the convex combination $\left(1-\lambda\right)*\overline{{\psi_{I}}}(I_{q})+\lambda*\overline{{\psi_{T}}}(T_{q})$ is a generalization of the element-wise sum of the query features. Consequently, as the embedding spaces exhibit stronger additivity properties, the Combiner’s effectiveness in its task is enhanced. We intentionally design the Combiner to capitalize on the task adaptation achieved during the first stage of training.  
通过将第一个分支(1)的输出表示为$\lambda$和$1-\lambda$，第二个分支(2)的输出表示为$v$，我们可以将组合特征表达为$\overline{{{\phi_{q}}}}=\left(1-\lambda\right)*\overline{{{\psi_{I}}}}(I_{q})+\lambda*\overline{{{\psi_{T}}}}(T_{q})+v$。值得注意的是，凸组合$\left(1-\lambda\right)*\overline{{\psi_{I}}}(I_{q})+\lambda*\overline{{\psi_{T}}}(T_{q})$是查询特征逐元素求和的一般化形式。因此，随着嵌入空间展现出更强的加性属性，Combiner在任务中的有效性也随之增强。我们特意设计Combiner网络以充分利用第一阶段训练中获得的任务适配能力。

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/ccbff7eb3d754da86de3e4bd9e07ac9394da7b6f06ced2c0872998476772acdd.jpg)  
Fig. 3. Second stage of training. In this stage, we train from scratch a Combiner network that learns to fuse the multimodal features extracted with CLIP encoders. We start by extracting the image-text query features using the fine-tuned encoders, and we combine them using the Combiner network. We then employ a contrastive loss to minimize the distance between combined features and target image features in the same triplet and maximize the distance from the other images in the batch. We keep both CLIP encoders frozen while we only update the weights of the Combiner network. At inference time the fine-tuned encoders and the trained Combiner are used to produce an effective representation used to query the database.  
训练的第二阶段。在这个阶段，我们从头开始训练Combiner网络，学习融合由CLIP编码器提取的多模态特征。我们首先使用微调后的编码器提取图像-文本查询特征，并通过Combiner网络进行组合。然后采用对比损失来最小化组合特征与同一三元组中目标图像特征的距离，同时最大化与批次中其他图像的距离。我们保持两个CLIP编码器的参数冻结，仅更新Combiner网络的权重。在推理阶段，微调后的编码器与训练好的Combiner网络共同生成用于数据库查询的有效特征表示。

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/bd805296f3cc380c6d9485a28658955c6a8af8bf3b3ca10eec6abaacafc8f0d2.jpg)  
Fig. 4. Architecture of the Combiner network $C_{\theta}$ . It takes as input the multimodal query features and outputs a unified representation. $\sigma$ represents the sigmoid function. We denote the outputs of the first branch (1) as $\lambda$ and $1-\lambda$ , while the output of the second branch (2) as 𝑣. The combined features are $\overline{{{\phi_{q}}}}=\left(1-\lambda\right)*\overline{{{\psi_{I}}}}(I_{q})+\lambda*\overline{{{\psi_{T}}}}(T_{q})+v$  
图4. Combiner网络$C_{\theta}$的架构。该网络以多模态查询特征为输入，输出统一的特征表示。$\sigma$表示sigmoid函数。第一个分支(1)的输出记为$\lambda$和$1-\lambda$，第二个分支(2)的输出记为𝑣。组合特征为$\overline{{{\phi_{q}}}}=\left(1-\lambda\right)*\overline{{{\psi_{I}}}}(I_{q})+\lambda*\overline{{{\psi_{T}}}}(T_{q})+v$。

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/71686ea0c1a3a745747f481ec3e14f0f99f7773428cf16cf0142b61a91fe3a50.jpg)  
Fig. 5. Histogram of image aspect ratios in FashionIQ and CIRR datasets (a) and the three categories of FashionIQ (b). The $\mathbf{x}\cdot\mathbf{\partial}$ -axis represents the aspect ratio defined as 𝑚𝑎𝑥 (𝑤𝑖𝑑𝑡ℎ, ℎ𝑒𝑖𝑔ℎ𝑡)/𝑚𝑖𝑛(𝑤𝑖𝑑𝑡ℎ, ℎ𝑒𝑖𝑔ℎ𝑡) while the y-axis represents the number of images (in logarithmic scale). The width of each bin is 0.5, and the first bin starts at 1. More than half of the dataset’s images are skewed and have at least a 1.5 aspect ratio. In the FashionIQ dataset, the issue is evident in the Dress category.  
图5. FashionIQ和CIRR数据集(a)及FashionIQ三个子类(b)的图像纵横比分布直方图。$\mathbf{x}\cdot\mathbf{\partial}$轴表示由𝑚𝑎𝑥(𝑤𝑖𝑑𝑡ℎ, ℎ𝑒𝑖𝑔ℎ𝑡)/𝑚𝑖𝑛(𝑤𝑖𝑑𝑡ℎ, ℎ𝑒𝑖𝑔ℎ𝑡)定义的纵横比，y轴表示图像数量（对数尺度）。每个数据条的宽度为0.5，第一个数据条起始于1。数据集中超过半数的图像存在比例失衡，其纵横比至少为1.5。在FashionIQ数据集中，这一现象在Dress（连衣裙）类别中尤为显著。

## 3.3 Preprocess Pipeline预处理流程  

The standard preprocess pipeline of CLIP is mainly composed of two steps: a resize operation where the smaller side of the image matches the CLIP input dimension 𝑖𝑛𝑝𝑢𝑡_𝑑𝑖𝑚 followed by a center crop operation which results in a square patch 𝑖𝑛𝑝𝑢𝑡_𝑑𝑖𝑚 $\times$ 𝑖𝑛𝑝𝑢𝑡_𝑑𝑖𝑚 output. Subsequently, as the ratio between the largest and the smaller side increases, the area of the image lost after the preprocess increases. From now on, we will say that an image has a high aspect ratio when it is far from having a square shape. In Fig. 5 is shown how, in the datasets we consider (detailed in Section 4), the number of images with a high aspect ratio is not negligible. As can be seen, this is especially true for the FashionIQ dress category and the CIRR dataset.  
CLIP的标准预处理流程主要包含两个步骤：首先进行尺寸调整（使图像的较短边匹配CLIP输入维度$input\_dim$），随后进行中心裁剪操作，最终得到$input\_dim \times input\_dim$的正方形图像块。随着图像长宽比例的增加，预处理过程中丢失的图像区域也会相应增加。当图像形状明显偏离正方形时，我们称其具有高纵横比。如图5所示，在我们研究的数据集（详见第4节）中，高纵横比图像的数量不可忽视，这一现象在FashionIQ的Dress类别和CIRR数据集中尤为显著。

One way to address the loss of information due to the center crop operation is to apply zero-padding to match the smaller side to the larger side, effectively squaring the image. Although this approach eliminates the loss of content information, it also reduces the resolution of the useful portion of the image since the CLIP image encoder input dimension cannot change. Thus, we develop a preprocessing pipeline that seeks to balance the two approaches discussed above. Specifically, we apply padding to an image only if its aspect ratio exceeds a predefined target ratio. Additionally, instead of squaring the image, we adjust its aspect ratio to match the target ratio when padding is applied. The pseudocode for the proposed preprocess pipeline is shown in Algorithm 1.  
为解决中心裁剪导致的信息丢失问题，可采用零值填充方法使较短边与较长边等长，从而将图像调整为正方形。虽然这种方法能保留完整内容信息，但由于CLIP图像编码器的输入维度固定，会导致有效区域分辨率降低。为此，我们开发了一种平衡两种方法的预处理流程：仅在图像纵横比超过预设目标比例时添加填充，且填充时调整图像纵横比使其匹配目标比例而非强制转为正方形。具体预处理流程的伪代码如算法1所示。

Figure 6 presents the preprocess pipelines as mentioned earlier. It is evident that when the ratio between the larger and smaller sides deviates significantly from one, the standard CLIP preprocess removes a substantial portion of the image, which considerably hampers the retrieval process. Although the visual disparities between the square pad and the proposed pad (with a target ratio of 1.25) approaches are not substantial, we will demonstrate that the model benefits from having such an increased usable portion in the images during retrieval.  
图6. 不同预处理流程的对比。可以明显看出，当图像长宽比显著偏离1时，标准CLIP预处理流程会移除图像的大部分区域，这将显著阻碍检索过程。尽管正方形填充与建议填充（目标比例为1.25）方法产生的视觉差异并不显著，但我们将证明模型在检索过程中会因图像可用区域的增加而受益。

# 4 EXPERIMENTAL RESULTS  

## 4.1 Implementation details  

We perform the experiments using two CLIP models of different sizes. The smallest one relies on a modified ResNet50 (RN-50) [22] architecture. It takes input images of $224\times224$ , and the size of its embedding space is $d=1024$ . The biggest one, denoted as $\mathrm{RN}{-}50\mathrm{x}4$ , follows the EfficientNet-style model scaling and uses approximately $4\times$ the computation of RN-50. It takes input images of $288\times288$ , and the size of its embedding space is $d=640$ .  
我们使用两种不同规模的CLIP模型进行实验。较小模型基于改进的ResNet50（RN-50）[22]架构，接收$224\times224$尺寸的输入图像，其嵌入空间维度为$d=1024$。较大模型记为$\mathrm{RN}{-}50\mathrm{x}4$，采用EfficientNet风格的模型缩放方法，计算量约为RN-50的4倍。该模型接收$288\times288$尺寸的输入图像，其嵌入空间维度为$d=640$。

# in_image : input image to be preprocessed   
# target_ratio : target aspect ratio   
# dim: CLIP image encoder input dimension   
# in_image : 待预处理的输入图像
# target_ratio : 目标纵横比 
# dim: CLIP图像编码器输入维度
def preprocess ( in_image , target_ratio , dim ):
    w, h $=$ in_image.size
    aspect_ratio $=$ max(w, h) / min(w, h)
    # 仅当纵横比超过目标值时进行填充
    if aspect_ratio $<$ target_ratio :
        out_image $=$ in_image
    else :
        # 零值填充使图像纵横比匹配目标值
        scaled_max_wh $=$ max(w, h) / target_ratio
        hp $=$ max((scaled_max_wh - w) // 2, 0)
        vp $=$ max((scaled_max_wh - h) // 2, 0)
        padding $=$ (hp, vp, hp, vp)
        out_image $=$ pad(in_image, padding, 0)
    # 调整尺寸并进行中心裁剪
    out_image $=$ resize(out_image, dim)
    out_image $=$ center_crop(out_image, dim)
    return out_image

Algorithm 1: Python-style pseudocode of the proposed preprocess pipeline.  
算法1: 基于Python风格的预处理流程伪代码。 

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/182bb20c593f18854c475729ca748efda767763b17879b555bf0119b82e05746.jpg)  
Fig. 6. Comparison among different preprocesses pipelines. The proposed padding method results in images that contain more details than square padding and provides a better overview than the standard CLIP padding.  
图6. 不同预处理流程的对比。建议的填充方法相较于正方形填充能保留更多图像细节，相比标准CLIP填充提供更优的全局视角。

In the Combiner network (Figure 4), the first two linear layers before the concatenation have input-output dimensionality equal to $(d,4d)$ . After the concatenation in both branches, we have two linear layers. In the first branch (1), the first linear layer has input-output dimensionality of $(4d,8d)$ , and the second one (8𝑑, 1). In the other branch, the first linear layer has input-output dimensionality of $(4d,8d)$ while the second one $(8d,d)$ . Following the standard practice, we set the dropout rate to 0.5. During retrieval, we normalize both the combined and index set features to have a unit $L_{2}$ -norm.  
在Combiner网络（图4）中，拼接操作前的两个线性层具有$(d,4d)$的输入-输出维度。两个分支在拼接后各包含两个线性层：在第一个分支(1)中，第一个线性层的输入-输出维度为$(4d,8d)$，第二个层为$(8d,1)$；在另一个分支中，第一个线性层维度为$(4d,8d)$，第二个层为$(8d,d)$。遵循常规做法，我们将丢弃率设置为0.5。在检索阶段，我们对组合特征和索引集特征进行单位$L_{2}$范数归一化。

Following the original CLIP training strategy, in the fine-tuning stage, we employed AdamW optimizer [38] with a learning rate of $2e-6$ and a weight decay coefficient of $1e-2$ . Due to GPU memory constraints, we set the batch size to 512 for fine-tuning the RN-50-based CLIP model and 192 for fine-tuning the RN-50x4-based model. We kept the batch normalization layer frozen. We fine-tuned the CLIP encoders for a maximum of 150 epochs. During the training of the Combiner network, we keep both fine-tuned CLIP encoders frozen and only train the Combiner function. We set the learning rate to $2e-5$ and train the model for a maximum of 300 epochs. We set the batch size to 4096 when using both backbones. We used the PyTorch library throughout the experiments. We set the target ratio in the preprocessing pipeline to 1.25. Following the approach described in [41], we set the parameter $\tau$ in Eq. (1) to 100. This value ensures that the logits have a sufficient dynamic range. To mitigate overfitting, we adopt an early stopping strategy. We use mixed-precision training [39] to save memory and speed up the training in both stages. We employ gradient checkpointing [7] to further reduce memory usage.  
遵循原始CLIP训练策略，在微调阶段我们采用AdamW优化器[38]，学习率设置为$2e-6$，权重衰减系数为$1e-2$。受限于GPU显存，基于RN-50的CLIP模型微调批量大小设为512，RN-50x4模型设为192。我们保持批量归一化层冻结状态，CLIP编码器最大微调150个训练周期。在Combiner网络训练阶段，我们冻结两个已微调的CLIP编码器，仅训练Combiner函数。学习率设为$2e-5$，最大训练300个周期。当使用双骨干网络时，批量大小设为4096。整个实验基于PyTorch库实现，预处理流程中目标比例设为1.25。根据[41]的方法，我们将公式(1)中的参数$\tau$设为100，该值确保逻辑值具有足够的动态范围。为缓解过拟合，采用早停策略。在双阶段训练中均使用混合精度训练[39]来节省内存并加速训练，同时应用梯度检查点技术[7]进一步降低显存消耗。

We conduct all experiments on a single NVIDIA Titan RTX (24GB) GPU. The first stage of training requires approximately 4 hours for the RN-50x4 model and 2 hours for the RN-50 model. The training of the Combiner network takes less than an hour for both models.  
所有实验均在单块NVIDIA Titan RTX（24GB）GPU上进行。RN-50x4模型的第一阶段训练耗时约4小时，RN-50模型约2小时。Combiner网络训练对两种模型均可在1小时内完成。

## 4.2 Datasets and metrics  

4.2.1 FashionIQ. FashionIQ [51] is composed of 77,684 fashion images crawled from the web and split into the train, validation, and test sets, divided into three different categories: Dress, Toptee and Shirt. Among the 46,609 training images, there are 18,000 training triplets made of a reference image, a pair of relative captions, and a target image. The captions describe properties to modify in the reference image to match the target image. The validation and test sets consist of 15,537 and 15,538 images, respectively, with 6,017 and 6,119 triplets.  
4.2.1 FashionIQ. FashionIQ [51]数据集由77,684张网络爬取的时尚图像构成，划分为训练集、验证集和测试集，包含Dress（连衣裙）、Toptee（短袖上衣）和Shirt（衬衫）三个子类。其中46,609张训练图像包含18,000个训练三元组，每个三元组由参考图像、一对关联文本描述和目标图像组成。文本描述指导如何修改参考图像以匹配目标图像。验证集和测试集分别包含15,537和15,538张图像，对应6,017和6,119个三元组。

We follow the standard experimental setting as in [29, 31]. We employ the average recall at rank K (Recall $\boldsymbol{@}\mathrm{K})$ as an evaluation metric, namely Recall@10 $(\mathrm{R}@10)$ and Recall $\ @50$ $(\mathrm{R}@50)$ . Note that for each triplet, there is only a positive index image. Hence, each query has ${\mathrm{R@K}}$ zero or one. All results are on the validation set since, at the time of writing, test set ground-truth labels have not been released yet.  
我们遵循[29,31]的标准实验设置，采用平均召回率@K（Recall $\boldsymbol{@}\mathrm{K}$）作为评估指标，即R@10（Recall@10）和R@50（Recall@50）。需注意每个三元组仅包含一个正样本索引图像，因此每个查询的${\mathrm{R@K}}$取值为0或1。由于测试集标注尚未公开，所有结果均基于验证集得出。

4.2.2 CIRR. The authors of [37] designed the CIRR dataset to address two common problems encountered in composed image retrieval datasets, such as FashionIQ. These problems are the lack of sufficient visual complexity caused by the restricted image domain and the numerous false negatives due to the unfeasibility of extensively labeling target images for each (reference, text) pair. As a result, some images in the dataset that correspond to valid matches for a query are not labeled as valid targets. CIRR (Compose Image Retrieval on Real-life images) dataset consists of 21,552 real-life images taken from the popular natural language reasoning $N L V R^{2}$ dataset [47]. It has the same structure as the FashionIQ dataset and contains 36,554 triplets randomly assigned in $80\%$ for training, $10\%$ for validation, and $10\%$ for the test. The dataset images are grouped in multiple subsets of six semantically and visually similar images. To have negative images with high visual similarity the relative captions are collected describing the differences between two images in the same subset.  
4.2.2 CIRR. 文献[37]提出的CIRR数据集旨在解决组合图像检索数据集（如FashionIQ）中常见的两个问题：因图像领域受限导致的视觉复杂性不足，以及难以对每个（参考图像，文本）对进行大规模目标图像标注而产生的假阴性样本问题。该数据集包含21,552张来自自然语言推理数据集$NLVR^{2}$[47]的真实场景图像，遵循与FashionIQ相同的三元组结构，共含36,554个三元组，按80%训练、10%验证、10%测试的比例随机划分。数据集图像被组织为多个包含六张语义和视觉相似图像的子集，通过收集描述同一子集内两图像差异的相对文本来构建高视觉相似度的负样本。

The standard evaluation protocol proposed by the authors of the dataset is to report the recall at rank K (Recall@K) at four different ranks (1, 5, 10, 50). Moreover, thanks to the unique design of the CIRR dataset, it is also possible to report the RecallSubset metric that considers only the images in the subset of the query. This subset metric has two main benefits: it is not affected by false-negative samples and, thanks to negative samples with high visual similarity, it captures fine-grained image-text modifications. The reference metrics are the $\mathrm{R}@5$ which accounts for possible false negatives in the entire corpus, and the $\mathrm{R}_{\mathrm{Subset}}@1$ , which better illustrates the fine-grained reasoning abilities.  
该数据集的标准评估协议要求报告四个不同排名K（1、5、10、50）的Recall@K指标。得益于独特的数据集设计，还可计算仅考虑查询子集图像的RecallSubset指标。该子集指标具有双重优势：不受假阴性样本影响，且通过高相似度负样本捕捉细粒度图文修改。核心参考指标为考虑全库假阴性的$\mathrm{R}@5$，以及更好体现细粒度推理能力的$\mathrm{R}_{\mathrm{Subset}}@1$。

### 4.3 Task-oriented fine-tuning effects  

In this section, we present a set of experiments that illustrate how the task-oriented fine-tuning of CLIP encoders and their increased additivity properties contribute to easing the task of the Combiner network and help to improve retrieval performance. For each dataset, we compare the performance varying the combining function and the modality of the CLIP fine-tuning. Throughout all the experiments, we use the RN-50 CLIP model. For each fine-tuning modality, we train from scratch a different Combiner network. We report the results in Table 1 for the FashionIQ dataset and in Table 2 for the CIRR dataset.  
本节通过系列实验阐明CLIP编码器的任务导向微调及其增强的可加性特性如何简化Combiner网络的任务，并有助于提升检索性能。针对各数据集，我们通过改变组合函数和CLIP微调模式来比较性能表现。在全部实验中均使用RN-50 CLIP模型，针对每种微调模式从头开始训练不同的Combiner网络。在FashionIQ数据集上的实验结果记录于表1，CIRR数据集的结果见Table 2。

Notably, the element-wise sum of out-of-the-box CLIP features achieves impressive results without domain or task-specific training on both datasets. This performance is intriguing as it demonstrates that the CLIP imagetext common embedding space exhibits good additivity properties, even though its training objective does not explicitly optimize for this aspect. Fine-tuning only the CLIP image encoder brings an interesting performance boost compared to the out-of-the-box CLIP features. This improvement is expected when employing the elementwise sum as the combining function, given that the out-of-the-box CLIP features lack domain or task-specific training. However, the most promising improvement occurs when utilizing the trained Combiner network. The text encoder fine-tuning achieves slightly better performance than image encoder fine-tuning. We can notice that on the FashionIQ dataset, the improvement over the image encoder fine-tuning remains constant when using either the element-wise sum or the Combiner network as a combining function. However, on the CIRR dataset, the situation differs. When comparing with the performance of the image encoder fine-tuning, using the element-wise sum to combine the query features results in comparable global metrics, but significantly improved fine-grained subset metrics. In contrast, when utilizing the Combiner network, we observe a reduction in the gaps within the subset metrics, while achieving a greater improvement in the global metrics. We achieve the best results on both datasets when we fine-tune both encoders. The element-wise sum of the fine-tuned features outperforms the performance of the out-of-the-box features combined with the trained Combiner network by a significant margin. Moreover, when we combine the query features with the Combiner network, the performances further improve. It is worth highlighting that when utilizing the Combiner as a combining function, the improvement achieved by fine-tuning both encoders over the out-of-the-box CLIP features is the arithmetic sum of the improvements obtained by fine-tuning either the image or the text encoder.  
值得注意的是，未经领域或任务特定训练的CLIP特征元素求和，在两个数据集上均取得了令人印象深刻的结果。这一表现引人注目，因为它表明CLIP图像-文本共享嵌入空间具有良好的可加性，尽管其训练目标并未明确优化这一方面。与未调整的CLIP特征相比，仅微调CLIP图像编码器带来了有趣的性能提升。当采用元素求和作为组合函数时，这一提升是可以预见的，因为未经调整的CLIP特征缺乏领域或任务特定的训练。然而，最显著的改进出现在使用训练过的Combiner网络时。文本编码器的微调比图像编码器的微调略胜一筹。我们注意到，在FashionIQ数据集上，无论是使用元素求和还是Combiner网络作为组合函数，相对于图像编码器微调的改进保持稳定。但在CIRR数据集上，情况有所不同。与图像编码器微调的表现相比，使用元素求和组合查询特征时，全局指标相当，但细分子集指标显著提升。相反，使用Combiner网络时，我们观察到子集指标间的差距缩小，同时全局指标获得更大提升。当同时微调两个编码器时，我们在两个数据集上都达到了最佳结果。微调后特征的元素求和表现远超未调整特征与训练过的Combiner网络结合的表现。此外，当使用Combiner网络组合查询特征时，性能进一步提升。值得强调的是，使用Combiner作为组合函数时，同时微调两个编码器相对于未调整CLIP特征的改进，是单独微调图像或文本编码器所获改进的算术和。

Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features • 11   


<html><body><table><tr><td colspan="3"></td><td colspan="2">Shirt</td><td colspan="2">Dress</td><td colspan="2">Toptee</td><td colspan="2">Average</td></tr><tr><td>CF</td><td>IFT</td><td>TFT</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td></tr><tr><td rowspan="4">Sum</td><td>×</td><td>×</td><td>19.53</td><td>35.57</td><td>17.70</td><td>36.29</td><td>21.88</td><td>42.93</td><td>19.70</td><td>38.26</td></tr><tr><td></td><td>×</td><td>30.08</td><td>52.94</td><td>29.10</td><td>52.01</td><td>34.42</td><td>57.62</td><td>31.20</td><td>54.19</td></tr><tr><td>×</td><td></td><td>32.29</td><td>53.73</td><td>27.76</td><td>52.31</td><td>35.14</td><td>60.12</td><td>31.73</td><td>55.39</td></tr><tr><td></td><td>√</td><td>38.67</td><td>59.42</td><td>35.99</td><td>62.22</td><td>43.35</td><td>67.52</td><td>39.34</td><td>63.05</td></tr><tr><td rowspan="4">Combiner</td><td>×</td><td>×</td><td>31.85</td><td>52.50</td><td>27.22</td><td>50.62</td><td>33.81</td><td>57.57</td><td>30.96</td><td>53.56</td></tr><tr><td></td><td></td><td>34.30</td><td>55.79</td><td>32.47</td><td>55.18</td><td>38.45</td><td>62.36</td><td>35.07</td><td>57.78</td></tr><tr><td>×</td><td>←</td><td>35.87</td><td>57.21</td><td>31.43</td><td>54.98</td><td>38.20</td><td>63.22</td><td>35.16</td><td>58.47</td></tr><tr><td></td><td></td><td>39.87</td><td>60.84</td><td>37.67</td><td>63.16</td><td>44.88</td><td>68.59</td><td>40.80</td><td>64.20</td></tr></table></body></html>  

Table 1. Recall at K on the FashionIQ validation set while varying the combining function and the modality of CLIP finetuning. We denote IFT (image encoder fine-tuning) and TFT (text encoder fine-tuning) to represent whether the image encoder or the text encoder is fine-tuned in the first stage. CF (combining function) indicates the function used to combine the query features. We highlight the best scores in bold and underline the second-best scores.   
表1. 在FashionIQ验证集上，不同组合函数和CLIP微调模态下的K值召回率。我们使用IFT（图像编码器微调）和TFT（文本编码器微调）来表示在第一阶段是否对图像编码器或文本编码器进行了微调。CF（组合函数）指的是用于组合查询特征的函数。最佳得分以粗体标注，次佳得分则以下划线标出。


<html><body><table><tr><td rowspan="2"></td><td></td><td></td><td colspan="4">Recall@K</td><td colspan="3">Rsubset @K</td></tr><tr><td>IFT</td><td>TFT</td><td>K： 二</td><td>K = 5</td><td>K： = 10</td><td>K： 二 50</td><td>K 1</td><td>K = ：2</td><td>K： =3</td></tr><tr><td rowspan="4">Sum</td><td></td><td>×</td><td>21.38</td><td>50.85</td><td>64.00</td><td>87.23</td><td>54.48</td><td>76.01</td><td>87.16</td></tr><tr><td></td><td>×</td><td>31.67</td><td>66.08</td><td>79.36</td><td>95.38</td><td>58.12</td><td>78.42</td><td>89.78</td></tr><tr><td></td><td></td><td>32.72</td><td>66.63</td><td>79.22</td><td>94.86</td><td>67.21</td><td>86.00</td><td>93.81</td></tr><tr><td>√</td><td>√</td><td>40.97</td><td>74.70</td><td>85.51</td><td>96.94</td><td>68.81</td><td>86.96</td><td>93.90</td></tr><tr><td rowspan="4">Combiner</td><td>×</td><td></td><td>31.26</td><td>64.79</td><td>77.71</td><td>95.31</td><td>61.56</td><td>81.08</td><td>91.12</td></tr><tr><td></td><td>×</td><td>34.01</td><td>69.07</td><td>81.77</td><td>95.72</td><td>62.78</td><td>81.80</td><td>91.41</td></tr><tr><td>×</td><td></td><td>36.86</td><td>71.32</td><td>82.32</td><td>96.24</td><td>68.28</td><td>86.51</td><td>94.14</td></tr><tr><td><</td><td></td><td>42.05</td><td>76.13</td><td>86.51</td><td>97.49</td><td>70.15</td><td>87.18</td><td>94.40</td></tr></table></body></html>

Table 2. Recall at K on the CIRR validation set while varying the combining function and the modality of CLIP fine-tuning. We denote IFT (image encoder fine-tuning) and TFT (text encoder fine-tuning) to represent whether the image encoder or the text encoder is fine-tuned in the first stage. CF (combining function) indicates the function used to combine the query features. We highlight the best scores in bold and underline the second-best scores.  
表2展示了在CIRR验证集上，通过改变结合函数和CLIP微调模态，K值召回率的变化情况。我们用IFT（图像编码器微调）和TFT（文本编码器微调）来表示第一阶段是否对图像编码器或文本编码器进行了微调。CF（结合函数）指的是用于组合查询特征的功能。我们将最佳得分加粗显示，并将次佳得分加下划线。

Given this last observation and all the other results, we formulate the hypothesis that the fine-tuning of the image and the text encoder learn different and complementary information that improves performances differently. We conjecture that the fine-tuning of the image encoder adapts the image manifold to the domain of the data (e.g., the fashion domain for the FashionIQ dataset). On the contrary, the fine-tuning of the text-encoder adapts the text embedding space to the task of composed image retrieval by transforming textual features into displacement vectors within the image embedding space. In support of this conjecture, we highlight the difference in performances between the global metrics and subset metrics on the CIRR dataset when comparing the image and the text encoder fine-tuning using the element-wise sum as a combining function (second and third row in Table 2). We note that in the global metrics, where the domain of the images is diverse, the performance differences between the two experiments approach zero. Conversely, in the subset metrics, where the visual differences among the images are low, the image fine-tuning is not capable of capturing the fine-grained differences making the textual information more discriminative and thus making the fine-tuning of the text encoder perform better. The experiments described in Section 4.8 provide additional confirmation of our intuition.  
基于这一最后观察及所有其他结果，我们提出假设：图像和文本编码器的微调学习到了不同且互补的信息，从而以不同的方式提升了性能。我们推测，图像编码器的微调使图像流形适应数据的领域（例如，FashionIQ数据集的时尚领域）。相反，文本编码器的微调则通过将文本特征转换为图像嵌入空间内的位移向量，使文本嵌入空间适应组合图像检索的任务。为支持这一推测，我们强调了在CIRR数据集上，使用元素和作为结合函数时，图像与文本编码器微调在全球指标和子集指标上的性能差异（见表2第二和第三行）。我们注意到，在全球指标中，图像领域多样化，两项实验间的性能差异趋近于零。相反，在子集指标中，图像间视觉差异较小，图像微调无法捕捉到细粒度的差异，这使得文本信息更具区分性，从而让文本编码器的微调表现更佳。第4.8节所述的实验进一步证实了我们的直觉。

<html><body><table><tr><td></td><td colspan="2">Shirt</td><td colspan="2">Dress</td><td colspan="2">Toptee</td><td colspan="2">Average</td></tr><tr><td>Model</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td></tr><tr><td>Element-wisesum</td><td>38.67</td><td>59.42</td><td>35.99</td><td>62.22</td><td>43.35</td><td>67.52</td><td>39.34</td><td>63.05</td></tr><tr><td>Convexcombination</td><td>39.45</td><td>60.16</td><td>36.44</td><td>62.57</td><td>44.05</td><td>67.87</td><td>39.98</td><td>63.53</td></tr><tr><td>W/oconvexcombination</td><td>31.40</td><td>55.64</td><td>35.94</td><td>61.03</td><td>40.29</td><td>64.97</td><td>35.87</td><td>60.55</td></tr><tr><td>Static skip</td><td>39.00</td><td>60.54</td><td>36.99</td><td>63.11</td><td>44.26</td><td>68.23</td><td>40.08</td><td>63.96</td></tr><tr><td>Proposed Combiner</td><td>39.87</td><td>60.84</td><td>37.67</td><td>63.16</td><td>44.88</td><td>68.59</td><td>40.80</td><td>64.20</td></tr></table></body></html>

Table 3. Recall at K on the FashionIQ validation set, with variations on the Combiner architecture. We highlight the best scores in bold and underline the second-best scores.  
表3. 在FashionIQ验证集上，针对不同Combiner架构变体的K召回率。最佳得分以粗体标出，次佳得分则加下划线显示。

<html><body><table><tr><td></td><td colspan="4">Recall@K</td><td colspan="4">Rsubset @K</td></tr><tr><td>Model</td><td>K=1</td><td>K=5</td><td>K =10</td><td>K=50</td><td>K=1</td><td></td><td>K=2</td><td>K=3</td></tr><tr><td>Element-wisesum</td><td>40.97</td><td>74.70</td><td>85.51</td><td>96.94</td><td>68.81</td><td></td><td>86.96</td><td>93.90</td></tr><tr><td>Convexcombination</td><td>41.11</td><td>75.56</td><td>85.55</td><td>97.44</td><td></td><td>70.46</td><td>87.08</td><td>94.33</td></tr><tr><td>W/oconvexcombination</td><td>36.98</td><td>72.06</td><td>82.83</td><td>96.67</td><td></td><td>65.53</td><td>84.74</td><td>93.06</td></tr><tr><td>Static skip</td><td>41.88</td><td>75.87</td><td>86.20</td><td>97.46</td><td></td><td>69.89</td><td>87.35</td><td>94.21</td></tr><tr><td>Proposed Combiner</td><td>42.05</td><td>76.13</td><td>86.51</td><td></td><td>97.49</td><td>70.15</td><td>87.18</td><td>94.40</td></tr></table></body></html>

Table 4. Recall at K on the CIRR validation set, with variations on the Combiner architecture. We highlight the best scores in bold and underline the second-best scores.  
表4. 在CIRR验证集上K值召回率，展示了Combiner架构的不同变体。我们用粗体标出最佳分数，并用下划线标出次佳分数。

### 4.4 Combiner ablation study  

In this section, we present a set of experiments with ablations and variations of the proposed Combiner network. We perform all the experiments using the fine-tuned RN-50 CLIP model. We train all the Combiner networks using a batch size of 4096 and a learning rate of $2e-5$ .  
在本节中，我们展示了一系列针对所提出的Combiner网络进行的消融实验及变体研究。所有实验均采用微调后的RN-50 CLIP模型执行。我们训练所有Combiner网络时，使用的批量大小为4096，学习率设定为$2e-5$。

Given the proposed Combiner network illustrated in Fig. 4, we denote the outputs of the first branch (1) as $\lambda$ and $1-\lambda$ , while the output of the second branch (2) as $v$ . The output features of the proposed Combiner are: $\overline{{{\phi_{q}}}}=\left(1-\lambda\right)*\overline{{{\psi_{I}}}}(I_{q})+\lambda*\overline{{{\psi_{T}}}}(T_{q})+v$ .  

To evaluate each component of the proposed design, we tested the following variations:  

• Element-wise sum: fine-tuned image and text features are summed: $\overline{{\phi_{q}}}=\overline{{\psi_{I}}}(I_{q})+\overline{{\psi_{T}}}(T_{q})$   
• Convex combination: only convex combination of image and text features, i.e. the model without the mixture contribution of text and image: $\overline{{\phi_{q}}}=(1-\lambda)*\overline{{\psi_{I}}}(I_{q})+\lambda*\overline{{\psi_{T}}}(T_{q})$   
• W/o convex combination: only the mixture contribution of text and image, i.e the model without the convex combination of text and image features: $\overline{{\phi_{q}}}=v$   
• Static skip: the convex coefficients are statically set to $0.5\colon\overline{{\phi_{q}}}=0.5*\overline{{\psi_{I}}}(I_{q})+0.5*\overline{{\psi_{T}}}(T_{q})+v$   
• Proposed Combiner: the Combiner architecture illustrated in Fig. 4.  
根据图4所示的Combiner网络设计，我们将第一个分支（1）的输出表示为$\lambda$和$1-\lambda$，而第二个分支（2）的输出表示为$v$。所提出的Combiner的输出特征为：$\overline{{{\phi_{q}}}}=(1-\lambda)*\overline{{{\psi_{I}}}}(I_{q})+\lambda*\overline{{{\psi_{T}}}}(T_{q})+v$。

为了评估设计中的每个组成部分，我们测试了以下变体：

- 元素级求和：微调后的图像和文本特征直接相加：$\overline{{\phi_{q}}}=\overline{{\psi_{I}}}(I_{q})+\overline{{\psi_{T}}}(T_{q})$
- 凸组合：仅对图像和文本特征进行凸组合，即不考虑文本和图像混合贡献的模型：$\overline{{\phi_{q}}}=(1-\lambda)*\overline{{\psi_{I}}}(I_{q})+\lambda*\overline{{\psi_{T}}}(T_{q})$
- 无凸组合：仅考虑文本和图像的混合贡献，即不考虑图像和文本特征凸组合的模型：$\overline{{\phi_{q}}}=v$
- 静态跳跃：凸系数静态设置为0.5：$\overline{{\phi_{q}}}=0.5*\overline{{\psi_{I}}}(I_{q})+0.5*\overline{{\psi_{T}}}(T_{q}})+v$
- 提出的Combiner：如图4所示的Combiner架构。


<html><body><table><tr><td></td><td></td><td></td><td colspan="2">Shirt</td><td colspan="2">Dress</td><td colspan="2">Toptee</td><td colspan="2">Average</td></tr><tr><td>Approach</td><td>IFT</td><td>TFT</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td></tr><tr><td rowspan="3">End-to-end</td><td></td><td>x</td><td>31.79</td><td>53.14</td><td>30.29</td><td>53.49</td><td>33.55</td><td>59.15</td><td>31.87</td><td>55.26</td></tr><tr><td>×</td><td></td><td>33.02</td><td>54.41</td><td>30.19</td><td>53.64</td><td>35.90</td><td>61.60</td><td>33.03</td><td>56.55</td></tr><tr><td></td><td></td><td>37.29</td><td>59.02</td><td>34.65</td><td>60.83</td><td>41.20</td><td>65.99</td><td>37.71</td><td>61.95</td></tr><tr><td rowspan="3">Two-stages</td><td></td><td>x</td><td>34.30</td><td>55.79</td><td>32.47</td><td>55.18</td><td>38.45</td><td>62.36</td><td>35.07</td><td>57.78</td></tr><tr><td>×</td><td></td><td>35.87</td><td>57.21</td><td>31.43</td><td>54.98</td><td>38.20</td><td>63.22</td><td>35.16</td><td>58.47</td></tr><tr><td></td><td>√</td><td>39.87</td><td>60.84</td><td>37.67</td><td>63.16</td><td>44.88</td><td>68.59</td><td>40.80</td><td>64.20</td></tr></table></body></html>

Table 5. Recall at K on the FashionIQ validation set employing either the two-stage or the end-to-end approach. We denote IFT (image encoder fine-tuning) and TFT (text encoder fine-tuning) to represent whether the image encoder or the text encoder is fine-tuned in the first stage. We highlight the best scores in bold and underline the second-best scores. 
表5展示了在FashionIQ验证集上采用两阶段或端到端方法时的K值召回率。我们用IFT（图像编码器微调）和TFT（文本编码器微调）来表示在第一阶段是否对图像编码器或文本编码器进行了微调。我们将最佳得分加粗显示，并将次优得分加下划线。 

We report the results for each variation in Table 3 for the FashionIQ dataset and in Table 4 for the CIRR dataset. The element-wise sum of the fine-tuned features serves as a solid starting point. As shown in section 4.3, the task-oriented fine-tuning process is highly effective and results in significant improvements over the out-of-thebox features on both datasets. The convex combination baseline, which dynamically computes text and image convex coefficients for greater adaptability to the query, achieves a slight improvement over the element-wise sum of the features. Notably, when we remove the text and image convex combination, we observe a significant drop in performance compared to the proposed Combiner. This emphasizes the importance of the text and image convex combination in achieving good performance. This result demonstrates that allowing the Combiner network to learn the residual from the element-wise sum (or its generalization, the convex combination) leads to a considerable improvement in performance. This outcome is expected because without the contribution of the image-text convex combination, the effectiveness of the first-stage training, which aims to enhance the additivity properties of the embedding spaces, is compromised. It is worth noting that setting the convex coefficients statically to 0.5 leads to a slight decrease in performance, which is attributed to the greater adaptability of the dynamically computed coefficients.  
我们在表3中报告了FashionIQ数据集的每种变体结果，在表4中报告了CIRR数据集的结果。微调特征的元素和作为一个坚实的起点。如第4.3节所示，面向任务的微调过程非常有效，并且在两个数据集上都显著优于开箱即用的特征。凸组合基线动态计算文本和图像的凸系数，以提高对查询的适应性，相比特征的元素和略有提升。值得注意的是，当我们移除文本和图像的凸组合时，与提出的Combiner相比，性能显著下降。这强调了文本和图像凸组合在实现良好性能中的重要性。这一结果表明，允许Combiner网络从元素和（或其泛化，凸组合）中学习残差，可以显著提高性能。这一结果是预期的，因为如果没有图像-文本凸组合的贡献，旨在增强嵌入空间可加性的第一阶段训练的有效性就会受到影响。值得注意的是，将凸系数静态设置为0.5会导致性能略有下降，这归因于动态计算系数的更大适应性。

Our experiments demonstrate the crucial role of the Combiner architecture in effectively exploiting the full potential of the additive embedding spaces constructed during the first stage of training. By enabling the network to learn the residual from the dynamically computed convex combination, we observe significant performance improvements.  
我们的实验证明了Combiner架构在有效利用第一阶段训练中构建的加性嵌入空间全部潜力中的关键作用。通过使网络能够从动态计算的凸组合中学习残差，我们观察到了显著的性能提升。

### 4.5 Analysis of Two-Stage vs. End-to-End approach  

In order to explain why a two-stage training method, where the CLIP encoder and Combiner are trained separately, in contrast to an end-to-end approach, we perform an experiment where we compare the two settings on both CIRR and FashionIQ datasets. First, we train end-to-end by fine-tuning CLIP encoders while training the Combiner network simultaneously. Then, we followed the proposed two-stage approach. In both settings, we also enable fine-tuning of the textual or image encoders separately and jointly. In all the experiments in this section, we use the RN-50 CLIP model.  
为了解释为何采用两阶段训练方法——即分别训练CLIP编码器和组合器，而非端到端方式，我们进行了一项实验，在CIRR和FashionIQ两个数据集上对比了这两种设置。首先，我们采用端到端方式，在微调CLIP编码器的同时训练组合器网络。随后，遵循所提出的两阶段方法进行训练。在这两种设置中，我们还分别及联合启用了文本或图像编码器的微调。本节所有实验均采用RN-50 CLIP模型。

We present the results in Table 5 for the FashionIQ dataset and in Table 6 for the CIRR dataset. Remarkably, the two-stage approach consistently outperforms the end-to-end one on both datasets. These superior results remain consistent even when varying the fine-tuning modality.  
我们在表5中展示了FashionIQ数据集的结果，表6中则是CIRR数据集的结果。值得注意的是，两阶段方法在两个数据集上均持续优于端到端方法。即便在微调模式变化的情况下，这一优势结果依然保持一致。

The results validate the effectiveness of constructing an embedding space with robust additivity properties before combining the features using a non-linear function. We hypothesize that when training the Combiner network simultaneously with the CLIP encoders, the entire system struggles to effectively learn the additive embedding spaces and the non-linear combining function in a cohesive manner. As a result, this limitation negatively impacts the overall performance, leading to suboptimal outcomes.  
这些结果验证了在利用非线性函数组合特征之前，构建具有强加法性质的嵌入空间的有效性。我们推测，当组合器网络与CLIP编码器同时训练时，整个系统难以有效地以一体化方式学习加法嵌入空间和非线性组合函数。因此，这种限制对整体性能产生了负面影响，导致了不尽人意的结果。

<html><body><table><tr><td rowspan="2"></td><td rowspan="2">TFT</td><td rowspan="2"></td><td colspan="4">Recall@K</td><td colspan="3">Rsubset @K</td></tr><tr><td>K = 1</td><td>K=5</td><td>K = 10</td><td>K = 50</td><td>K=</td><td>K： =2</td><td>K： =3</td></tr><tr><td rowspan="3">End-to-end</td><td></td><td>x</td><td>33.19</td><td>67.01</td><td>79.83</td><td>95.52</td><td>58.90</td><td>79.33</td><td>90.28</td></tr><tr><td>×</td><td></td><td>33.58</td><td>67.59</td><td>79.57</td><td>95.28</td><td>67.37</td><td>85.58</td><td>93.44</td></tr><tr><td></td><td>√</td><td>40.03</td><td>74.09</td><td>85.14</td><td>97.12</td><td>68.14</td><td>86.06</td><td>93.64</td></tr><tr><td rowspan="3">Two-stages</td><td></td><td>x</td><td>34.01</td><td>69.07</td><td>81.77</td><td>95.72</td><td>62.78</td><td>81.80</td><td>91.41</td></tr><tr><td></td><td></td><td>36.86</td><td>71.32</td><td>82.32</td><td>96.24</td><td>68.28</td><td>86.51</td><td>94.14</td></tr><tr><td></td><td></td><td>42.05</td><td>76.13</td><td>86.51</td><td>97.49</td><td>70.15</td><td>87.18</td><td>94.40</td></tr></table></body></html>

Table 6. Recall at K on the CIRR validation set employing either the two-stage or the end-to-end approach. We denote IFT (image encoder fine-tuning) and TFT (text encoder fine-tuning) to represent whether the image encoder or the text encoder is fine-tuned in the first stage. We highlight the best scores in bold and underline the second-best scores.  
表6展示了在CIRR验证集上采用两阶段或端到端方法时的K值召回率。我们用IFT（图像编码器微调）和TFT（文本编码器微调）来表示第一阶段是否对图像编码器或文本编码器进行了微调。最佳得分以粗体标出，次佳得分则以下划线表示。

### 4.6 Preprocess upshot  

In this section, we show how the proposed preprocess pipeline, described in Section 3.3, contributes to further improving performance. We compare the proposed preprocess with two other methods: the standard CLIP preprocess pipeline, primarily consisting of resize and center crop operations, and the Square preprocess, which involves applying a square zero-pad to the image before resizing and center cropping. The comparison among the different preprocess techniques is presented in Table 7 for the FashionIQ dataset and Table 8 for the CIRR dataset. 
在本节中，我们展示了第3.3节中描述的提议预处理流程如何进一步提升性能。我们将提议的预处理与另外两种方法进行了比较：标准的CLIP预处理流程，主要包括调整大小和中心裁剪操作；以及Square预处理，即在调整大小和中心裁剪之前对图像应用方形零填充。不同预处理技术的比较结果在FashionIQ数据集的表7和CIRR数据集的表8中呈现。

On the FashionIQ dataset, the improvement obtained using the proposed preprocess pipeline over the standard one is substantial in the Dress category and noticeable in the Toptee category. Conversely, the square pad preprocess technique achieves comparable performance to the proposed one in the Dress and Toptee categories while suffering a performance deficit in the Shirt category. Overall, we observe a correlation between the difference in performance among the methods and the number of images with a high aspect ratio, as depicted in Figure 5. In other words, when dealing with images with a high aspect ratio, it is preferable to pad them to avoid losing crucial portions of the image during the center crop operation. On the other hand, when images have a low aspect ratio, it is more effective not to reduce the usable portion of the image with padding. The proposed preprocess pipeline achieves the best performance by effectively adapting to the aspect ratio of each image. On the CIRR dataset, we observe that the proposed preprocess significantly improves performance compared to the standard CLIP and the square preprocess. The performance gain is particularly significant in low-rank recall measures, where the importance of every lost detail is crucial.  
在FashionIQ数据集上，使用提议的预处理流程相比标准流程在Dress类别中获得了显著的改进，在Toptee类别中也有明显的提升。相反，方形填充预处理技术在Dress和Toptee类别中取得了与提议预处理相当的性能，但在Shirt类别中表现不佳。总体而言，我们观察到不同方法之间的性能差异与高纵横比图像的数量之间存在相关性，如图5所示。换句话说，在处理高纵横比的图像时，宜对其进行填充，以避免在中心裁剪操作中丢失图像的关键部分。另一方面，当图像的纵横比较低时，不通过填充减少图像可用部分更为有效。提议的预处理流程通过有效适应每张图像的纵横比，实现了最佳性能。在CIRR数据集上，我们观察到提议的预处理相比标准CLIP和方形预处理显著提高了性能。性能提升在低排名召回率指标中尤为明显，在这些指标中，每一个丢失的细节都至关重要。

### 4.7 Comparison with SotA  

We compare the proposed method with state-of-the-art approaches on two standard and challenging datasets. To ensure a fair comparison, we follow the standard experimental settings of the two datasets [37, 51]. Unless specifically mentioned, we report the metrics for each method as documented in the official papers, and we refer to those papers for more comprehensive details about the individual approaches.  
我们将提议的方法与两个标准且具有挑战性的数据集上的最新方法进行了比较。为了确保公平比较，我们遵循这两个数据集的标准实验设置[37, 51]。除非特别说明，我们报告的方法指标均来自官方论文中的记录，并请读者参考这些论文以获取各个方法的更全面细节。

Table 9 reports the comparison between the proposed method and other state-of-the-art approaches. We divide the table into two sections: the upper section includes methods that are not directly comparable to our approach. These approaches either do not utilize a pre-trained textual encoder [8, 29, 31, 45, 50, 52] or, in the case of TRACE [27], they use BERT [15] as a pre-trained textual encoder but do not update its weights. It is important to note that even when a competitor [8, 29, 45] utilizes the GloVe word embedding [40], we do not consider their textual encoder as pre-trained. All the methods in this section rely on a ResNet model pre-trained on the ImageNet dataset [43] and fine-tuned during training. We include the results of these methods to provide a more comprehensive discussion. The lower section of Table 9 reports methods that are directly comparable to ours: they rely on both pre-trained visual and language models updating all the weights of both backbones during training. CIRRPLANT [37] relies on the OSCAR pretrained model as a textual backbone, while [16, 21, 26] rely on the pre-trained BERT model. It is worth mentioning that FashionViL is a fashion-oriented approach that carries out a large-scale pre-training for learning $\mathrm{V+L}$ representation in the fashion domain. For this reason, it is not surprising that it exhibits strong performances in a fashion dataset such as FashionIQ. When considering the RN50-based method, the proposed approach outperforms the competitors by improving up to $9\%$ in average $\mathrm{R}\ @10$ and $7\%$ in average $\mathrm{R}@50$ compared to the best-performing competitor, FashionViL, when using the same visual backbone architecture. Our method demonstrates the highest recall across all categories, with a particularly significant margin observed in the Shirt category. When considering the larger $\mathrm{RN}{-}50\mathrm{x}4$ -based model, we observe an improvement ranging from $2\%$ to $4\%$ in all categories compared to the smaller backbone. This result demonstrates that our approach scales well when using larger and heavier VL models.  
表9报告了提议方法与其他最新方法的比较。我们将表格分为两部分：上半部分包括与我们方法不直接可比的方法。这些方法要么不使用预训练的文本编码器[8, 29, 31, 45, 50, 52]，要么如TRACE[27]使用BERT[15]作为预训练文本编码器但不更新其权重。需要注意的是，即使某个竞争方法[8, 29, 45]使用了GloVe词嵌入[40]，我们也不认为其文本编码器是预训练的。本部分中的所有方法都依赖于在ImageNet数据集[43]上预训练并在训练期间进行微调的ResNet模型。我们纳入这些方法的结果以提供更全面的讨论。表9的下半部分报告了与我们方法直接可比的方法：这些方法依赖于预训练的视觉和语言模型，并在训练期间更新两个骨干网络的所有权重。CIRRPLANT[37]依赖OSCAR预训练模型作为文本骨干，而[16, 21, 26]依赖预训练的BERT模型。值得一提的是，FashionViL是一种面向时尚的方法，它在大规模预训练中学习时尚领域的$\mathrm{V+L}$表示。因此，它在FashionIQ这样的时尚数据集上表现出色并不令人意外。在考虑基于RN50的方法时，与使用相同视觉骨干架构的最佳竞争者FashionViL相比，提议方法在平均$\mathrm{R}\ @10$上提高了高达$9%$，在平均$\mathrm{R}@50$上提高了$7%$，超过了竞争对手。我们的方法在所有类别中显示出最高的召回率，特别是在Shirt类别中差距尤为显著。在考虑更大的$\mathrm{RN}{-}50\mathrm{x}4$模型时，与较小的骨干相比，我们观察到所有类别的改进范围在$2%$到$4%$之间。这一结果表明，我们的方法在使用更大、更重的VL模型时具有良好的扩展性。

<html><body><table><tr><td></td><td></td><td colspan="2">Shirt</td><td colspan="2">Dress</td><td colspan="2">Toptee</td><td colspan="2">Average</td></tr><tr><td>CF</td><td>Preprocess</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td></tr><tr><td rowspan="3">Sum</td><td>Standard</td><td>37.64</td><td>59.76</td><td>33.42</td><td>59.84</td><td>40.90</td><td>66.80</td><td>37.32</td><td>62.13</td></tr><tr><td>Square</td><td>37.09</td><td>58.52</td><td>35.94</td><td>62.03</td><td>42.53</td><td>66.29</td><td>38.52</td><td>62.28</td></tr><tr><td>Proposed</td><td>38.67</td><td>59.42</td><td>35.99</td><td>62.22</td><td>43.35</td><td>67.52</td><td>39.34</td><td>63.05</td></tr><tr><td rowspan="3">Combiner</td><td>Standard</td><td>39.40</td><td>61.33</td><td>35.25</td><td>60.44</td><td>43.95</td><td>67.72</td><td>39.53</td><td>63.16</td></tr><tr><td>Square</td><td>38.71</td><td>60.21</td><td>37.97</td><td>62.86</td><td>44.12</td><td>68.03</td><td>40.26</td><td>63.70</td></tr><tr><td>Proposed</td><td>39.87</td><td>60.84</td><td>37.67</td><td>63.16</td><td>44.88</td><td>68.59</td><td>40.80</td><td>64.20</td></tr></table></body></html>

Table 7. Recall at K on FashionIQ validation set varying the combining function and the preprocessing pipeline used. CF (combining function) indicates the function used to combine the query features. We highlight the best scores in bold and underline the second-best scores.  
表7. 在FashionIQ验证集上，根据使用的组合函数和预处理流程变化的K值召回率。CF（组合函数）表示用于组合查询特征的函数。我们以粗体标出最佳得分，并用下划线标出次佳得分。

<html><body><table><tr><td rowspan="2">CF</td><td rowspan="2"></td><td colspan="4">Recall@K</td><td colspan="3">Rsubset @K</td></tr><tr><td>Preprocess K =1</td><td>K=5</td><td>K = 10</td><td>K =50</td><td>K= ：1</td><td>K=2</td><td>K=3</td></tr><tr><td rowspan="3">Sum</td><td>Standard</td><td>39.51</td><td>74.00</td><td>84.72</td><td>97.20</td><td>68.36</td><td>86.15</td><td>94.26</td></tr><tr><td>Square</td><td>41.26</td><td>74.34</td><td>85.00</td><td>96.84</td><td>69.15</td><td>85.89</td><td>93.90</td></tr><tr><td>Proposed</td><td>40.97</td><td>74.70</td><td>85.51</td><td>96.94</td><td>68.81</td><td>86.96</td><td>93.90</td></tr><tr><td rowspan="3">Combiner</td><td>Standard</td><td>40.08</td><td>74.15</td><td>84.67</td><td>97.20</td><td>69.53</td><td>86.27</td><td>94.45</td></tr><tr><td>Square</td><td>41.95</td><td>74.96</td><td>85.24</td><td>96.58</td><td>70.65</td><td>86.67</td><td>94.24</td></tr><tr><td>Proposed</td><td>42.05</td><td>76.13</td><td>86.51</td><td>97.49</td><td>70.15</td><td>87.18</td><td>94.40</td></tr></table></body></html>

Table 8. Recall at K on CIRR validation set varying the combining function and the preprocessing pipeline used. CF (combining function) indicates the function used to combine the query features. We highlight the best scores in bold and underline the second-best scores.  
表8. 在CIRR验证集上，根据使用的组合函数和预处理流程变化的K值召回率。CF（组合函数）表示用于组合查询特征的函数。我们以粗体标出最佳得分，并用下划线标出次佳得分。

In Table 10, we report a comparison between the proposed method and other state-of-the-art approaches. As for FashionIQ, the upper section of the table reports methods that are not directly comparable with the proposed one: they do not utilize a pre-trained textual encoder. As the visual backbone, they employ a ResNet-based model, which is pre-trained on ImageNet and fine-tuned during training. The lower section of the table includes directly comparable methods, such as MAAF, which utilizes BERT as a text encoder, and CIRPLANT, which relies on the pre-trained Vision-Language model OSCAR. The results presented in Table 10 are obtained through the official evaluation server. Our approach consistently outperforms the competitors by a significant margin, particularly in low-rank recall measures, where we notice an improvement of approximately $20\%$ in $\mathrm{R}(\varpi1$ when using the RN50 visual backbone. When considering the larger RN- $\it{\cdot50x4}$ model, we observe improvements ranging from $3\%$ in low-rank recall metrics to $1\%$ as the recall rank increases.  
在表10中，我们报告了提议方法与其他最先进方法的比较。与FashionIQ类似，表格的上半部分报告了与提议方法不直接可比的方法：这些方法未使用预训练的文本编码器。作为视觉骨干，它们采用了基于ResNet的模型，该模型在ImageNet上预训练并在训练期间进行微调。表格的下半部分包括直接可比的方法，例如使用BERT作为文本编码器的MAAF，以及依赖预训练视觉-语言模型OSCAR的CIRPLANT。表10中呈现的结果是通过官方评估服务器获得的。我们的方法始终以显著优势超越竞争对手，特别是在低排名召回率指标中，我们在使用RN50视觉骨干时注意到$\mathrm{R}(\varpi1$提高了大约$20\%$。在考虑更大的RN-$\it{\cdot50x4}$模型时，我们观察到低排名召回率指标的改进幅度为$3\%$，随着召回排名的增加，改进幅度逐渐减小至$1\%$。

<html><body><table><tr><td rowspan="2">Method</td><td colspan="2">Encoder</td><td colspan="2">Shirt</td><td colspan="2">Dress</td><td colspan="2">Toptee</td><td colspan="2">Average</td></tr><tr><td>Visual</td><td>Textual</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td><td>R@10</td><td>R@50</td></tr><tr><td>TRACE [27]</td><td>RN-50</td><td>BERT [15]</td><td>20.80</td><td>40.80</td><td>22.70</td><td>44.91</td><td>24.22</td><td>49.80</td><td>22.57</td><td>46.19</td></tr><tr><td>VAL [8]</td><td>RN-50</td><td>LSTM(GloVe) [23]</td><td>22.38</td><td>44.15</td><td>22.53</td><td>44.00</td><td>27.53</td><td>51.68</td><td>24.15</td><td>46.61</td></tr><tr><td>CurlingNet [52]</td><td>RN-152</td><td>biGRU [10]</td><td>21.45</td><td>44.56</td><td>26.15</td><td>53.24</td><td>30.12</td><td>55.23</td><td>25.90</td><td>51.01</td></tr><tr><td>RTIC-GCN[45]</td><td>RN-50</td><td>LSTM(GloVe)</td><td>23.79</td><td>47.25</td><td>29.15</td><td>54.04</td><td>31.61</td><td>57.98</td><td>28.18</td><td>53.09</td></tr><tr><td>CoSMo [31]</td><td>RN-50</td><td>LSTM</td><td>24.90</td><td>49.18</td><td>25.64</td><td>50.30</td><td>29.21</td><td>57.46</td><td>26.58</td><td>52.31</td></tr><tr><td>DCNet [29]</td><td>RN-50</td><td>Conv1D(GloVe)</td><td>23.95</td><td>47.30</td><td>28.95</td><td>56.07</td><td>30.44</td><td>58.29</td><td>27.78</td><td>53.89</td></tr><tr><td>CLVC-Net [50]</td><td>RN-50</td><td>LSTM</td><td>28.75</td><td>54.76</td><td>29.85</td><td>56.47</td><td>33.50</td><td>64.00</td><td>30.70</td><td>58.41</td></tr><tr><td>CIRPLANT [37]</td><td>RN-152</td><td>OSCAR [34]</td><td>17.53</td><td>38.81</td><td>17.45</td><td>40.41</td><td>21.64</td><td>45.38</td><td>18.87</td><td>41.53</td></tr><tr><td>MAAF [16]</td><td>RN-50</td><td>BERT</td><td>18.55</td><td>37.63</td><td>18.59</td><td>39.66</td><td>23.05</td><td>45.95</td><td>20.06</td><td>41.08</td></tr><tr><td>SAC [26]</td><td>RN-50</td><td>BERT</td><td>28.02</td><td>51.86</td><td>26.52</td><td>51.01</td><td>32.70</td><td>61.23</td><td>29.08</td><td>54.70</td></tr><tr><td>FashionViL[21]</td><td>RN-50</td><td>BERT</td><td>25.17</td><td>50.39</td><td>33.47</td><td>59.94</td><td>34.98</td><td>60.79</td><td>31.20</td><td>57.04</td></tr><tr><td>Ours</td><td>RN-50</td><td>Transformer</td><td>39.87</td><td>60.84</td><td>37.67</td><td>63.16</td><td>44.88</td><td>68.59</td><td>40.80</td><td>64.20</td></tr><tr><td>Ours</td><td>RN-50x4</td><td>Transformer</td><td>44.41</td><td>65.26</td><td>39.46</td><td>64.55</td><td>47.48</td><td>70.98</td><td>43.78</td><td>66.93</td></tr></table></body></html>

Table 9. Comparison between our method and current state-of-the-art models on the Fashion-IQ validation set. We highlight the best scores in bold and underline the second-best scores. The upper section of the table presents methods that are not directly comparable to our proposed approach, as they either do not utilize a pre-trained textual encoder or do not update its weights. "RN" stands for ResNet.  
表9. 我们的方法与Fashion-IQ验证集上当前最先进模型的比较。我们以粗体标出最佳得分，并用下划线标出次佳得分。表格的上半部分展示了与我们提议方法不直接可比的方法，因为这些方法要么未使用预训练的文本编码器，要么未更新其权重。"RN"代表ResNet。

<html><body><table><tr><td></td><td colspan="2">Encoder</td><td colspan="4">Recall@K</td><td colspan="3">Rsubset @K</td></tr><tr><td>Method</td><td>Visual</td><td>Textual</td><td>K=1</td><td>K=5</td><td>K =10</td><td>K =50</td><td>K=1</td><td>K： =2</td><td>K=3</td></tr><tr><td>TIRG [48]</td><td>RN-18</td><td>LSTM</td><td>14.61</td><td>48.37</td><td>64.08</td><td>90.03</td><td>22.67</td><td>44.97</td><td>65.14</td></tr><tr><td>TIRG+LastConv+ [48]</td><td>RN-18</td><td>LSTM</td><td>11.04</td><td>35.68</td><td>51.27</td><td>83.29</td><td>23.82</td><td>45.65</td><td>64.55</td></tr><tr><td>MAAFt [16]</td><td>RN-50</td><td>LSTM</td><td>10.31</td><td>33.03</td><td>48.30</td><td>80.06</td><td>21.05</td><td>41.81</td><td>61.60</td></tr><tr><td>MAAF-IT [16]</td><td>RN-50</td><td>LSTM</td><td>9.90</td><td>32.86</td><td>48.83</td><td>80.27</td><td>21.17</td><td>42.04</td><td>60.91</td></tr><tr><td>MAAF-RP+ [16]</td><td>RN-50</td><td>LSTM</td><td>10.22</td><td>33.32</td><td>48.68</td><td>81.84</td><td>21.41</td><td>42.17</td><td>61.60</td></tr><tr><td>ARTEMIS [14]</td><td>RN-152</td><td>biGRU</td><td>16.96</td><td>46.10</td><td>61.31</td><td>87.73</td><td>39.99</td><td>62.20</td><td>75.67</td></tr><tr><td>MAAFt [16]</td><td>RN-50</td><td>BERT</td><td>10.12</td><td>33.10</td><td>48.01</td><td>80.57</td><td>22.04</td><td>42.41</td><td>62.14</td></tr><tr><td>CIRPLANTt [37]</td><td>RN-152</td><td>OSCAR</td><td>19.55</td><td>52.55</td><td>68.39</td><td>92.38</td><td>39.20</td><td>63.03</td><td>79.49</td></tr><tr><td>Ours</td><td>RN-50</td><td>Transformer</td><td>40.91</td><td>74.53</td><td>84.77</td><td>97.35</td><td>70.22</td><td>87.80</td><td>94.46</td></tr><tr><td>Ours</td><td>RN-50x4</td><td>Transformer</td><td>44.82</td><td>77.04</td><td>86.65</td><td>97.90</td><td>73.16</td><td>88.84</td><td>95.59</td></tr></table></body></html>

Table 10. Comparison between our method and current state-of-the-art models on the CIRR test set. We highlight the best scores in bold and underline the second-best scores.. † denotes results cited from [37]. The upper section of the table presents methods that are not directly comparable to our proposed approach, as they either do not utilize a pre-trained textual encoder or do not update its weights. In the lower section, we report methods that are directly comparable to our approach. "RN" stands for ResNet. 
 表10. 我们的方法与CIRR测试集上当前最先进模型的比较。我们以粗体标出最佳得分，并用下划线标出次佳得分。†表示引自[37]的结果。表格的上半部分展示了与我们提议方法不直接可比的方法，因为这些方法要么未使用预训练的文本编码器，要么未更新其权重。在表格的下半部分，我们报告了与我们方法直接可比的方法。"RN"代表ResNet。

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/e73257470695e3393506c14a162d98f7ddc27b9862e3eb09ba251dff2547a73d.jpg)  
Fig. 7. Histograms of cosine similarities between image/text feature pairs. The $\mathbf{x}\cdot\mathbf{\partial}$ -axis represents the cosine similarities. The y-axis represents the (normalized) number of pairs. In the top-line plots, we have used the out-of-the-box CLIP model. In the bottom line, we have used the model fine-tuned during the first stage of training. In the left-side plots, we compare the image features. In the right ones, we compare the text features. The histograms are normalized such that the area under each curve integrates to 1.  
图7. 图像/文本特征对之间的余弦相似度直方图。$\mathbf{x}\cdot\mathbf{\partial}$轴表示余弦相似度。y轴表示（归一化的）对数。在顶行图中，我们使用了开箱即用的CLIP模型。在底行图中，我们使用了在训练第一阶段微调后的模型。在左侧图中，我们比较了图像特征。在右侧图中，我们比较了文本特征。直方图经过归一化处理，使得每条曲线下的面积积分等于1。

### 4.8 Feature distribution study  

The experiments in this section aim to provide intuition on how the feature distribution in the embedding spaces affects the retrieval performances. All the experiments were carried out on the validation sets using the RN-50 model. We are going to present two different sets of experiments that have slightly different purposes. The first set aims to investigate how the image and text features are distributed in the embedding spaces, while the second one explores how the distribution of the features affects retrieval performance.  
本节的实验旨在提供关于嵌入空间中的特征分布如何影响检索性能的直觉。所有实验均在验证集上使用RN-50模型进行。我们将介绍两组略有不同目的的实验。第一组实验旨在研究图像和文本特征如何在嵌入空间中分布，而第二组实验则探索特征分布如何影响检索性能。

To investigate how the features distribute in the embedding spaces, we followed [35] and calculate pairwise similarities among them. If the features occupy the embedding space uniformly, their similarities will be lower. Throughout all experiments, due to the quadratic growth of possible pairs, we compute the similarities between 50K randomly sampled pairs. Figure 7 shows the histograms of the features pairwise similarities on both FashionIQ and CIRR datasets. First of all, we can notice that due to the broader domain of CIRR, on such a dataset, both the image and text features similarities are higher when compared to FashionIQ. On both datasets, fine-tuning the image encoder leads to a drastic reduction in the average similarity of the visual features and, thus, to much more efficient use of the embedding space during retrieval. This fact confirms our hypothesis (Section 4.3) that fine-tuning the image encoder adapts the image manifold to the data domain. The fine-tuning of the text encoder leads to a lower reduction in the average pairwise similarity of textual features (almost negligible in FashionIQ) than that observed in visual ones. We suppose that efficient use of the image embedding space is far more critical than efficient use of the textual space since the retrieval is carried out in the image space. In all the experiments, we observe that the fine-tuning of CLIP encoders contributes to reducing the cone effect: “the effective embedding space is restricted to a narrow cone for trained models and models with random weights" [35].  
为了研究特征在嵌入空间中的分布情况，我们遵循[35]的方法，计算它们之间的成对相似性。如果特征在嵌入空间中均匀分布，它们的相似性将较低。在所有实验中，由于可能对数的平方增长，我们计算了50K个随机采样对之间的相似性。图7显示了FashionIQ和CIRR数据集上特征成对相似性的直方图。首先，我们可以注意到，由于CIRR的领域更广泛，在该数据集上，图像和文本特征的相似性比FashionIQ更高。在两个数据集上，微调图像编码器显著降低了视觉特征的平均相似性，从而在检索期间更高效地利用了嵌入空间。这一事实证实了我们的假设（第4.3节），即微调图像编码器使图像流形适应数据领域。文本编码器的微调对文本特征平均成对相似性的降低幅度小于视觉特征（在FashionIQ中几乎可以忽略不计）。我们推测，高效利用图像嵌入空间远比高效利用文本空间更重要，因为检索是在图像空间中进行的。在所有实验中，我们观察到CLIP编码器的微调有助于减少锥形效应：“对于训练过的模型和具有随机权重的模型，有效嵌入空间被限制在一个狭窄的锥形范围内”[35]。

The previous experiments demonstrate how the two-stage approach proposed in this study affects the textual and visual CLIP embedding spaces. However, these experiments do not clarify why this increased utilization of embedding space can improve the retrieval process. We conducted additional experiments to investigate the impact of this embedding space reshaping on the image retrieval task. We compute and compare the cosine similarities (the distance function used in the retrieval) between the combined and the index image features.  
之前的实验展示了本研究提出的两阶段方法如何影响文本和视觉CLIP嵌入空间。然而，这些实验并未阐明为何增加嵌入空间的利用率可以改进检索过程。我们进行了额外的实验，以研究这种嵌入空间重塑对图像检索任务的影响。我们计算并比较了组合特征与索引图像特征之间的余弦相似性（检索中使用的距离函数）。

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/d2bd20a5555f12982c579070b1fc1ef0ebe0600a7cdc2e24112c4db10f417215.jpg)  
Fig. 8. Histograms of the cosine similarities between combined and target/non-target feature pairs. The $\mathbf{x}\cdot\mathbf{\partial}$ -axis represents the cosine similarities. The y-axis represents the (normalized) number of pairs. In green•: cosine similarities between combined and non-target index features. In violet•: cosine similarities between combined and target features. In red•: similarity gap between combined-target and combined-non target features. In dark green•: intersection over union area between the two histograms. In the top-line plots, we have used the simple sum as a combining function. In the bottom line ones, we have used the Combiner network. In the left-side plots, we used the out-of-the-box CLIP model. In the right ones, we used the model fine-tuned during the first stage of the training. The histograms are normalized such that the area under each curve integrates to 1.  
图8. 组合特征与目标/非目标特征对之间的余弦相似度直方图。$\mathbf{x}\cdot\mathbf{\partial}$轴表示余弦相似度。y轴表示（归一化的）对数。绿色•：组合特征与非目标索引特征之间的余弦相似性。紫色•：组合特征与目标特征之间的余弦相似性。红色•：组合-目标与组合-非目标特征之间的相似性差距。深绿色•：两个直方图之间的交并比面积。在顶行图中，我们使用了简单求和作为组合函数。在底行图中，我们使用了Combiner网络。在左侧图中，我们使用了开箱即用的CLIP模型。在右侧图中，我们使用了在训练第一阶段微调后的模型。直方图经过归一化处理，使得每条曲线下的面积积分等于1。

Specifically, we perform two distinct computations: in the first one, we compute the similarity between the combined features and the target image features belonging to the same query triplet. In the second one, we compute the similarity between the combined features and random image features that differ from the target ones. Given a query, we will refer to the images that differ from the target as non-target images. We compare each combined feature with ten non-target image features to reduce the variance. 
具体而言，我们执行了两种不同的计算：在第一种中，我们计算了组合特征与属于同一查询三元组的目标图像特征之间的相似性。在第二种中，我们计算了组合特征与不同于目标的随机图像特征之间的相似性。给定一个查询，我们将不同于目标的图像称为非目标图像。我们将每个组合特征与十个非目标图像特征进行比较，以减少方差。 

Figure 8 emphasizes the similarity gaps between the combined and target/non-target features. On both FashionIQ and CIRR datasets, we notice that the element-wise sum of out-of-the-box CLIP features achieves the highest average combined-target features similarity. During both the fine-tuning and the Combiner network training stages, the contrastive training increases the cosine distances between the combined and non-target features instead of increasing their similarity to the target features. By observing both Fig. 8a and Fig. 8b and the corresponding retrieval results in Table 1 and Table 2, we argue that, in these two datasets, the retrieval performances are highly correlated with the similarity gap between combined-target and combined-non target features (displayed as the red arrows in Fig. 8) and with the size of intersection area between the histograms (the smaller the intersection area, the smaller the retrieval errors will be). On the contrary, the absolute value of the combined-target similarity does not seem to be of great importance.  
图8强调了组合特征与目标/非目标特征之间的相似性差距。在FashionIQ和CIRR数据集上，我们注意到，开箱即用CLIP特征的元素级求和实现了最高的平均组合-目标特征相似性。在微调和Combiner网络训练阶段，对比训练增加了组合特征与非目标特征之间的余弦距离，而不是增加它们与目标特征的相似性。通过观察图8a和图8b以及表1和表2中的相应检索结果，我们认为，在这两个数据集中，检索性能与组合-目标和组合-非目标特征之间的相似性差距（在图8中显示为红色箭头）以及直方图之间的交集面积大小高度相关（交集面积越小，检索错误越小）。相反，组合-目标相似性的绝对值似乎并不重要。

The two sets of experiments highlight different but strongly related aspects. The first set shows that fine-tuning both CLIP encoders leads to more efficient use of the embedding spaces. In the second set, we prove that the increased occupation of the image space helps to “move away" the combined features from the non-target features.  
这两组实验突出了不同但密切相关的方面。第一组实验表明，微调两个CLIP编码器可以更高效地利用嵌入空间。在第二组实验中，我们证明了图像空间的增加占用有助于将组合特征“远离”非目标特征。

### 4.9 Qualitative results  

To obtain a clearer understanding of which parts of the images the system considers most important during retrieval, we conducted qualitative experiments using the GradCAM technique [44]. Instead of computing gradients versus an output class, we compute gradients with respect to the combined features, which summarize both the visual and textual content of the image and caption, using the GradCAM technique. This approach makes each heat map generated by GradCAM dependent on the reference image and its relative caption, simulating the retrieval process. We use the last convolutional layer of CLIP’s image encoder as the saliency layer.
为了更清楚地了解系统在检索过程中认为图像的哪些部分最重要，我们使用GradCAM技术[44]进行了定性实验。我们没有针对输出类别计算梯度，而是针对组合特征计算梯度，这些组合特征总结了图像和标题的视觉和文本内容，使用GradCAM技术。这种方法使得GradCAM生成的每个热图依赖于参考图像及其相关的标题，模拟检索过程。我们使用CLIP图像编码器的最后一个卷积层作为显著性层。  

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/f7f430122795a08f97b1902d9814427e7f6524f337ec6028e2e0166f48d42f5d.jpg)  
Fig. 9. Examples of GradCAM visualization on CIRR dataset computing the gradients with respect to the Combiner output.  
图9. 在CIRR数据集上计算相对于Combiner输出的梯度的GradCAM可视化示例。

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/dd7af23ee11b255c4fbb38a88dd0b028f7c654bbe1c11bc18fdd18b3b07fba96.jpg)  
Fig. 10. Examples of GradCAM visualization on FashionIQ computing the gradients with respect to the Combiner output.  
图10. 在FashionIQ数据集上计算相对于Combiner输出的梯度的GradCAM可视化示例。

In Fig. 9 and in Fig. 10 are displayed some examples of the above-described visualization technique. The system is capable of attending to a wide range of concepts, such as style and color changes for the fashion dataset and behavior modification for the CIRR dataset, as we can notice from the experiments with the GradCAM technique. For instance, in Fig. 9, the system attends to the carriage and horse in the first example, the pose of the holding monkey and the baby monkey in the second example, and the pose of the dog in the third example. In Fig. 10, the system attends to the arms and shoulders of the person when the conditioning text referred to the sleeves of the dress and to the logo of the shirt when it was requested to change the Norwegian flag into a Mexican one.  
在图9和图10中展示了一些上述可视化技术的示例。从使用GradCAM技术的实验中我们可以注意到，该系统能够关注广泛的概念，例如时尚数据集中的风格和颜色变化，以及CIRR数据集中的行为修改。例如，在图9中，系统关注第一个示例中的马车和马匹，第二个示例中抱猴的姿势和小猴，以及第三个示例中狗的姿势。在图10中，当条件文本提到裙子的袖子时，系统关注人的手臂和肩膀；当要求将挪威国旗改为墨西哥国旗时，系统关注衬衫的标志。

Finally, we complete the qualitative analysis of our approach by presenting examples of multimodal queries and their corresponding results on both datasets in Fig. 12 and Fig. 11. In FashionIQ, the correct result is returned most of the time in the first three results, while in CIRR, it is returned in the top-5 global and top-3 subset results. Interestingly, the excellent performance of the proposed system let us notice an issue with the FashionIQ dataset: from these examples, we can see that in the FashionIQ dataset, the existence of many false negatives is a real issue that can harm both the results and the training process; examining the first four and the last queries, we can observe that several results returned in the first positions are corresponding to the conditioning text, although only one of them is marked as such. E.g. in the first query, where several dresses have light floral patterns and bright colors, similarly, the first three results for the shirts should be considered correct. We can also see that in the CIRR dataset, the domain of the images is wider compared to FashionIQ, and the problem of the false negatives is a minor issue.  
最后，我们通过在图12和图11中展示两个数据集上的多模态查询及其相应结果，完成了对我们方法的定性分析。在FashionIQ中，大多数情况下正确结果在前三个结果中返回，而在CIRR中，正确结果在全局前5和子集前3的结果中返回。有趣的是，提议系统的出色性能让我们注意到FashionIQ数据集的一个问题：从这些示例中，我们可以看到在FashionIQ数据集中，许多假阴性的存在是一个真正的问题，可能损害结果和训练过程；检查前四个和最后一个查询，我们可以观察到，返回在第一位置的几个结果与条件文本相对应，尽管只有一个被标记为如此。例如，在第一个查询中，多件裙子具有浅色花卉图案和鲜艳的颜色，同样，衬衫的前三个结果也应视为正确。我们还可以看到，在CIRR数据集中，与FashionIQ相比，图像的领域更广泛，假阴性问题是一个较小的问题。

!

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/7728fdfee0be6b1327541a6008d7b9a1e3beb74893b353a1dad2d06b31d7b50e.jpg)  
Fig. 11. Qualitative results for the FashionIQ dataset. We highlight with a green border when the retrieved image is labeled as ground truth for the given query.  
图11. FashionIQ数据集的定性结果。我们用绿色边框高亮显示检索到的图像被标记为给定查询的真实结果。

# 5 CONCLUSIONS  

In this work, we propose a novel task-oriented fine-tuning scheme to adapt vision-language models for the composed image retrieval task. The primary goal of this fine-tuning is to address the mismatch between the large-scale pre-training of CLIP and the downstream task, thereby enhancing the additivity properties of the embedding spaces. We then propose a two-stage approach that combines fine-tuning with the training of a carefully crafted Combiner network, enabling the meaningful fusion of the fine-tuned multimodal features. To further enhance performance, we introduce a novel pre-processing padding method, which, as demonstrated in the ablation studies, improves performance on datasets with images of varying aspect ratios. We perform experiments on the challenging fashion dataset FashionIQ and the recently presented CIRR dataset. Experiments on both datasets show that our approach outperforms state-of-the-art methods by a significant margin. We also perform qualitative experiments to explain how our approach works. These experiments investigate the impact of the proposed approach on the feature distribution in the embedding spaces and how the reshaping of such embedding spaces influences retrieval performance. Additionally, we conduct visualization experiments using the gradCAM technique.  
在这项工作中，我们提出了一种新颖的面向任务的微调方案，以适应视觉-语言模型用于组合图像检索任务。这种微调的主要目标是解决CLIP大规模预训练与下游任务之间的不匹配，从而增强嵌入空间的可加性。我们随后提出了一个两阶段方法，将微调与精心设计的Combiner网络训练相结合，实现微调后的多模态特征的有意义融合。为了进一步提升性能，我们引入了一种新颖的预处理填充方法，正如消融研究所示，这种方法在具有不同纵横比图像的数据集上提高了性能。我们在具有挑战性的时尚数据集FashionIQ和最近推出的CIRR数据集上进行了实验。两个数据集上的实验表明，我们的方法以显著优势超过了最先进的方法。我们还进行了定性实验，以解释我们的方法是如何工作的。这些实验研究了提议方法对嵌入空间中特征分布的影响，以及这种嵌入空间的重塑如何影响检索性能。此外，我们使用gradCAM技术进行了可视化实验。

# ACKNOWLEDGMENTS  

This work was partially supported by the European Commission under European Horizon 2020 Programme, grant number 101004545 - ReInHerit.  

![](https://cdn-mineru.openxlab.org.cn/extract/724422a4-453f-453b-a159-f4ec560b71af/03758f19f18bfc016594c3645d58389dd586ad6f31e2bac2d09a8502dceaa7bd.jpg)  
Fig. 12. Qualitative results for the CIRR dataset. We highlight with a green border when the retrieved image is labeled as ground truth for the given query.  

# REFERENCES  

[1] Sandhini Agarwal, Gretchen Krueger, Jack Clark, Alec Radford, Jong Wook Kim, and Miles Brundage. 2021. Evaluating clip: towards characterization of broader capabilities and downstream implications. arXiv preprint arXiv:2108.02818 (2021).   
[2] Jamil Ahmad, Khan Muhammad, Sambit Bakshi, and Sung Wook Baik. 2018. Object-oriented convolutional features for fine-grained image retrieval in large surveillance datasets. Future Generation Computer Systems 81 (2018), 314–330.   
[3] Muhammad Umer Anwaar, Egor Labintcev, and Martin Kleinsteuber. 2021. Compositional Learning of Image-Text Query for Image Retrieval. In Proc. of IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). 1140–1149.   
[4] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Alberto Del Bimbo. 2022. Exploiting CLIP-Based Multi-modal Approach for Artwork Classification and Retrieval. In The Future of Heritage Science and Technologies: ICT and Digital Heritage: Third International Conference, Florence Heri-Tech 2022, Florence, Italy, May 16–18, 2022, Proceedings. Springer, 140–149.   
[5] Imon Banerjee, Camille Kurtz, Alon Edward Devorah, Bao Do, Daniel L Rubin, and Christopher F Beaulieu. 2018. Relevance feedback for enhancing content based image retrieval and automatic prediction of semantic image features: Application to bone tumor radiographs. Journal of biomedical informatics 84 (2018), 123–135.   
[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.   
[7] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174 (2016).   
[8] Yanbei Chen, Shaogang Gong, and Loris Bazzani. 2020. Image Search With Text Feedback by Visiolinguistic Attention Learning. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR). [9] Ruizhe Cheng, Bichen Wu, Peizhao Zhang, Peter Vajda, and Joseph E. Gonzalez. 2021. Data-Efficient Language-Supervised Zero-Shot Learning With Self-Distillation. In Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. 3119–3124.   
[10] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014).   
[11] Mario GCA Cimino, Federico A Galatolo, and Gigliola Vaglini. 2021. Generating Images from Caption and Vice Versa via CLIP-Guided Generative Latent Space Search. In Proceedings of the International Conference on Image Processing and Vision Engineering. 166–174.   
[12] Claudia Companioni-Brito, Zygred Mariano-Calibjio, Mohamed Elawady, and Sule Yildirim. 2018. Mobile-Based Painting Photo Retrieval Using Combined Features. In Proc. of International Conference on Image Analysis and Recognition (ICIAR), Vol. 10882. Springer, 278.   
[13] Marcos V Conde and Kerem Turgutlu. 2021. CLIP-Art: Contrastive Pre-Training for Fine-Grained Art Classification. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR). 3956–3960.   
[14] Ginger Delmas, Rafael S Rezende, Gabriela Csurka, and Diane Larlus. 2021. ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity. In International Conference on Learning Representations.   
[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 4171–4186.   
[16] Eric Dodds, Jack Culpepper, Simao Herdade, Yang Zhang, and Kofi Boakye. 2020. Modality-agnostic attention fusion for visual search with text feedback. arXiv preprint arXiv:2007.00145 (2020).   
[17] Xiao Dong, Xunlin Zhan, Yangxin Wu, Yunchao Wei, Xiaoyong Wei, Minlong Lu, and Xiaodan Liang. 2021. M5product: A multi-modal pretraining benchmark for e-commercial product downstream tasks. arXiv preprint arXiv:2109.04275 (2021).   
[18] Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen. 2021. CLIP2Video: Mastering Video-Text Retrieval via Image CLIP. arXiv preprint arXiv:2106.11097 (2021).   
[19] Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, and Rogerio Feris. 2018. Dialog-based interactive image retrieval. Advances in neural information processing systems 31 (2018).   
[20] Xintong Han, Zuxuan Wu, Phoenix X Huang, Xiao Zhang, Menglong Zhu, Yuan Li, Yang Zhao, and Larry S Davis. 2017. Automatic spatially-aware fashion concept discovery. In Proceedings of the IEEE international conference on computer vision. 1463–1471.   
[21] Xiao Han, Licheng Yu, Xiatian Zhu, Li Zhang, Yi-Zhe Song, and Tao Xiang. 2022. Fashionvil: Fashion-focused vision-and-language representation learning. In European Conference on Computer Vision. Springer, 634–651.   
[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770–778.   
[23] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735–1780.   
[24] Bogdan Ionescu, Henning Müller, Renaud Péteri, Yashin Dicente Cid, Vitali Liauchuk, Vassili Kovalev, Dzmitri Klimuk, Aleh Tarasau, Asma Ben Abacha, Sadid A Hasan, et al. 2019. ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature. In Proc. of International Conference of the Cross-Language Evaluation Forum for European Languages (CLEF). Springer, 358–386.   
[25] Bogdan Ionescu, Henning Müller, Renaud Péteri, Duc-Tien Dang-Nguyen, Liting Zhou, Luca Piras, Michael Riegler, Pål Halvorsen, Minh-Triet Tran, Mathias Lux, et al. 2020. ImageCLEF 2020: Multimedia retrieval in lifelogging, medical, nature, and internet applications. Advances in Information Retrieval 12036 (2020), 533.   
[26] Surgan Jandial, Pinkesh Badjatiya, Pranit Chawla, Ayush Chopra, Mausoom Sarkar, and Balaji Krishnamurthy. 2022. SAC: Semantic Attention Composition for Text-Conditioned Image Retrieval. In Proc. of IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). 4021–4030.   
[27] Surgan Jandial, Ayush Chopra, Pinkesh Badjatiya, Pranit Chawla, Mausoom Sarkar, and Balaji Krishnamurthy. 2020. Trace: Transform aggregate and compose visiolinguistic representations for image search with text feedback. arXiv preprint arXiv:2009.01485 7 (2020), 7.   
[28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In Proc. of International Conference on Machine Learning (ICML).   
[29] Jongseok Kim, Youngjae Yu, Hoeseong Kim, and Gunhee Kim. 2021. Dual Compositional Learning in Interactive Image Retrieval. In Proc. of AAAI Conference on Artificial Intelligence (AAAI), Vol. 35. 1771–1779. https://ojs.aaai.org/index.php/AAAI/article/view/16271   
[30] Adriana Kovashka, Devi Parikh, and Kristen Grauman. 2015. WhittleSearch: Interactive Image Search with Relative Attribute Feedback. International Journal of Computer Vision (IJCV) 115, 2 (Apr 2015), 185–210. https://doi.org/10.1007/s11263-015-0814-0   
[31] Seungmin Lee, Dongwan Kim, and Bohyung Han. 2021. CoSMo: Content-Style Modulation for Image Retrieval With Text Feedback. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR). 802–812.   
[32] Manling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji, and Shih-Fu Chang. 2022. Clip-event: Connecting text and images with event structures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 16420–16429.   
[33] Xiaoqing Li, Jiansheng Yang, and Jinwen Ma. 2021. Recent developments of content-based image retrieval (CBIR). Neurocomputing 452 (2021), 675–689. https://doi.org/10.1016/j.neucom.2020.07.139   
[34] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. 2020. Oscar: Object-semantics aligned pre-training for vision-language tasks. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX 16. Springer, 121–137.   
[35] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y Zou. 2022. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. Advances in Neural Information Processing Systems 35 (2022), 17612–17625.   
[36] Yating Liu and Yan Lu. 2021. Multi-grained Fusion for Conditional Image Retrieval. In Proc. of International Conference on Multimedia Modeling (MMM). Springer International Publishing, Cham, 315–327.   
[37] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. 2021. Image retrieval on real-life images with pre-trained vision-and-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2125–2134.   
[38] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In International Conference on Learning Representations. https://openreview.net/forum?id=Bkg6RiCqY7   
[39] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. 2018. Mixed Precision Training. In International Conference on Learning Representations.   
[40] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 1532–1543.   
[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 8748–8763.   
[42] Yong Rui, T.S. Huang, M. Ortega, and S. Mehrotra. 1998. Relevance feedback: a power tool for interactive content-based image retrieval. IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) 8, 5 (1998), 644–655. https://doi.org/10.1109/76.718510   
[43] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. 2015. Imagenet large scale visual recognition challenge. International journal of computer vision 115 (2015), 211–252.   
[44] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2019. Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization. International Journal of Computer Vision (IJCV) 128, 2 (Oct 2019), 336–359. https://doi.org/10.1007/s11263-019-01228-7   
[45] Minchul Shin, Yoonjae Cho, Byungsoo Ko, and Geonmo Gu. 2021. RTIC: Residual Learning for Text and Image Composition using Graph Convolutional Network. arXiv preprint arXiv:2104.03015 (2021).   
[46] Arnold WM Smeulders, Marcel Worring, Simone Santini, Amarnath Gupta, and Ramesh Jain. 2000. Content-based image retrieval at the end of the early years. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 22, 12 (2000), 1349–1380.   
[47] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. 2019. A Corpus for Reasoning about Natural Language Grounded in Photographs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 6418–6428.   
[48] Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. 2019. Composing text and image for image retrieval-an empirical odyssey. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 6439–6448.   
[49] Zhecan Wang, Noel Codella, Yen-Chun Chen, Luowei Zhou, Jianwei Yang, Xiyang Dai, Bin Xiao, Haoxuan You, Shih-Fu Chang, and Lu Yuan. 2022. CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks. arXiv preprint arXiv:2201.05729 (2022). arXiv:2201.05729 [cs.CV]   
[50] Haokun Wen, Xuemeng Song, Xin Yang, Yibing Zhan, and Liqiang Nie. 2021. Comprehensive Linguistic-Visual Composition Network for Image Retrieval. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR ’21). Association for Computing Machinery, New York, NY, USA, 1369–1378. https://doi.org/10.1145/ 3404835.3462967   
[51] Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogerio Feris. 2021. Fashion iq: A new dataset towards retrieving images by natural language feedback. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition. 11307–11317.   
[52] Youngjae Yu, Seunghwan Lee, Yuncheol Choi, and Gunhee Kim. 2020. Curlingnet: Compositional learning between images and text for fashion iq data. arXiv preprint arXiv:2003.12299 (2020).   
[53] Yifei Yuan and Wai Lam. 2021. Conversational Fashion Image Retrieval via Multiturn Natural Language Feedback. In Proc. of International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). ACM. https://doi.org/10.1145/3404835.3462881   
[54] Xunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Minlong Lu, Yichi Zhang, Hang Xu, and Xiaodan Liang. 2021. Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-Modal Pretraining. In Proc. of IEEE/CVF International Conference on Computer Vision (ICCV). 11782–11791.   
[55] Bo Zhao, Jiashi Feng, Xiao Wu, and Shuicheng Yan. 2017. Memory-Augmented Attribute Manipulation Networks for Interactive Fashion Search. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR). 6156–6164. https://doi.org/10.1109/CVPR.2017.652   
[56] Liang Zheng, Yi Yang, and Qi Tian. 2017. SIFT meets CNN: A decade survey of instance retrieval. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 40, 5 (2017), 1224–1244.  